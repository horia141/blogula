Sometimes we have more variables than we need. You can choose a subset of these by domain-specific knowledge (you know that some variables are important), exploratory analysis (plots that try to identify interesting variables) or statistical selection. Statistical selection: step-wise addition, AIC, BIC, LASSO, Ridge-Regression. You should do it on a hold out sample, because selection biases your inference. What are error measures: \f{R^2} (more variables means a bigger \f{R^2} so this isn't as good), adjusted \f{R^2} (takes into account the number variables), AIC, BIC. You can have forward/backward/both selection which means adding/removing/doing both variables in order to improve error measures. You could also try all possible subsets of variables. But there are \f{2^n} of these, not that good.

The most complicated model is not always the best. Sometimes adding extra features can hurt the model. The complete model, with all the features is called the full model.

Measuring the goodness of a model:
* Linear models often have a test for each variable performed of the form \f{H_0: \beta_i = 0 when all the other variables are included} and \f{H_A : \beta_i \neq 0 when all the other variables are included}. We can use the resulting p-values to see which features are important and which not. Those for which we reject the null are.
* We could also use the adjusted \f{R^2} - this is a unbiased estimator of \f{R^2} and it depends on the number of features - too many and this drops. Keeps more features than using the p-values though.

Methods for selecting features:
* Stepwise meathods: methods which look at subsets of features in order to select the best one.

= Backwards elimination using p-values =

Start with the set of all features and fit the model, computing p-values for the hypotheses that the coefficients are non-zero. Remove the feature with the highest p-value (most likely to be zero). Refit the model and repeat, until all features have coefficients different from zero in a statistically significant way. The p-values will tend to fall after each elimination, so a feature which was not significant initially, but close to the edge, might become significant after several other variables have been removed. Colinearity / dependence basically. Important to refit. In general, every feature contributes *something*, but that something might be too little for the extra complexity involved.

= Backwards elimination using adjusted \f{R^2} =

Start with the set of all features. Fit \f{p} models, each of \f{p-1} features - each model has dropped one feature from the set. Select the model of highest adjusted \f{R^2} and keep its feature set as the new canonical one. Repeat the process for the new canonical set. Continue until there is no increase in adjusted \f{R^2} by dropping a feature.

= Forwards selection using p-values =

Start with the empty model. Fit \f{p} models, each with a single feature included. Select the one with the lowest p-value as the winner. Include it's feature in the canonical set. Refit \f{p-1} models, using the canonical feature and the rest of the models. Stop when no variable added is statistically significant. It is not necessary to read We reach the same model as with backwards elimination.

= Forwards selection using adjusted \f{R^2} =

Start with the empty model. Fit \f{p} models, each with a single feature included. Select the one with the largest adjusted \f{R^2} as the winner. Include it's feature in the canonical set. Refit \f{p-1} models, using the canonical feature and the rest of the models. Stop when adding variables does not result in an increase in the adjusted \f{R^2}. It is not necessary to read We reach the same model as with backwards elimination.

= best subset selection =

generate all models with \f{I} variables and select the best according to RSS/r^2/deviance etc. Well have \f{p+1} models. choose the best using a model selection criterion (\f{C_p}, BIC, AIC, adjuster \f{r^2}). intractable for large p, even though you might use branch and bound techniques for example. may suffer from variance problems: by chance, when testing so many models, we'll find a good one on the training set.

= forward stepwise selection =

start with a model containing no predictors, then, for a number of steps, add the input variable which, when added to the current model, will produce the lowest RSS/r^2/deviance etc. at the end we'll have \f{p+1} models, from each step. choose the best using a model selection criterion (\f{C_p}, BIC, AIC, adjuster \f{r^2}). has at most \f{1+p(p-1)} model building & evaluation steps. can work in cases where \f{p > n}, but can the models must be buildable past \f{n} descriptors, otherwise it stops at \f{n} models. Linear regression via least squares usually is not. not guaranteed to get the best model though.

= backward stepwise selection =

start with a model containing all predictors, then remove, at each step one variable from the model at the previous step, and hold the one with highest RSS/r^2/deviance etc. at the end we'll have \f{p+1} models, for each step. choose the best using a model selection criterion (\f{C_p}, BIC, AIC, adjuster \f{r^2}). has at most \f{1+p(p-1)} model building & evaluation steps. can work in cases where \f{p > n}, but the model must be buildable past \f{n} descriptors, otherwise it can't really do anything. not guaranteed to get the best model though.

= model assesment =

when we have models which include multiple predictors, using RSS,\f{r^2} etc. as gauges of model fit isn't very good, since these suffer from the typical problems (being smaller for more variables etc.) which mean they don't estimate the test error well. we can either make adjustments to them in order to indirectly estimate the training error or use CV to select the models. this important for model selection methods as well.

== Maslow \f{C_p} ==

\f{C_p = 1/N (RSS+2d\hat{\sigma}^2)} \f{\hat{sigma}^2} is an estimate of the noise variance while \f{d} is the number of variables included. if \f{\hat{sigma}^2} is an unbiased estimate of \f{\sigma^2} then \f{C_p} is an unbiased estimate of the test MSE. only for least squares models.

== Akaike Information Criterion (AIC) ==

\f{AIC = 1/N\hat{sigma}^2 (RSS+2d\hat{\sigma}^2) + O(1)}. for all MaxLikelihood fitted models, which turn into least squares for gaussian errors (proportional to \f{C_p}).

== Baesyan Information Criterion (BIC) ==

\f{BIC = 1/N(RSS + \log{N}d\hat{\sigma}^2)} for least squares models. big penalty for mediumish \f{N}, thus favoring very small models.

== Adjusted \f{R^2} ==

\f{R_a^2 = 1 - RSS/(N-d-1)/TSS/(N-1)}. after some number of variables, the increase in RSS will be small, and be offset by the division.

== Crossvalidation ==

direct estimation of the test error. no assumptions about the model, good for when its hard to estimate \f{d} or \f{\hat{\sigma}}. previous models used for computationally tight situations.

one standard error rule: compute standard error at each model / model size, and select the smallest model within one standard deviation of the smallest error. When performance curves become flat, the procedure becomes succeptible to variance/noise. This eliminates some of that error, by making the criterion for "best" more challanging.
