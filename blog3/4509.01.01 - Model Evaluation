First, pick a baseline - maybe "random choosing" or beat probability of seeing stuff or beat a known good system.

Measure what you care: money, +1s. You can't always measure it. We can create metrics for goodness and badness.

[standard discussion of binary classification performance eval: tp, fp, fn, tn, accuracy, precision, recall]

For clustering: Dunn Index (higher is better)

For regression: MSE.

These are proxies for what we want - do not optimize the metric instead of the actual thing we want to optimize.

[standard discussion of train/test and train/validation/test system, k-fold validation]

Sometimes people use staggared test sets - use a part of the test set in 1mo, another in 3mo and the last before launch.

Confusion matrix.

Splitting data : random (not necessarily good as statistics would say) vs deliberate (choose splits so we have equal classes as in test set/wild).

Bweare of over-fitting.

Model checking can lead to some problems: overfitting, overtesting (doing a lot of tests \f{\Rightarrow} P-values and confidence intervals don't have the power you claim they do) and biased inference. Still, good to do to not miss something very obvious.

* variance is constant: Heteroskedastic situation. confidence intervals and P-values are not right if they're computed with the constant variance assumption. You can see if another variable explains the increased variance or use sandwich variance estimators for big \f{N}.
* you are summarizing a linear trend: Maybe the trend is not linear. You see a pattern in the residuals and you also see variable variance. One could use Poisson regression, use a data transformation, smooth the data or fit a nonlinear tred, use regression anyway but interptet just the linear trend between the variables, but abstain from predicting, and use sandwich estimators if \f{N} is big.
* you have all the right terms in the model: Use exploratory analysis to identify other variables to include (maybe if they're really OK to see),  use sandwich estimators if \f{N} is big and report unexplained patterns in the data.
* there are no big outliers: Try fitting the model with the outliers. Then try fitting the model without the outliers. If there is a big difference, be careful - something interesting is happening. Is it OK to remove them? If they are experimental mistakes - remove and document them. If they are real - consider reporting how sensitive your estimate is to the outliers. Consider using a robust linear model fit.

A tool for seeing if our assumptions hold is plotting the residuals and plotting a QQ-plot of the residuals with the \f{\mathcal{N}[0,1]} distribution on the \f{x} axis and the standardized quantiles on the \f{y} axis - they should be on the \f{f(x) = x} line, since we assume they're normal.

For GLMs (Logistic, Poisson etc.), the \def{deviance} is also used. Compares your model with a perfect fit model and sees if the variance your model explains is still good. It doesn't say what's wrong. It may be big even for conservative models and it also depends on sample size (like P-values).

\f{R^2} - (variance of residuals) is also a used summary, but it might be bad.

Any number we use is necesarily limited.

You simulate data from your model to see if it matches the data you already have. You should try to violate your assumptions to see what breaks. This simulation should be repeated so you can observe different datasets and see how robuts your answers are. Your variables should be generated from distributions similar to the ones infered from the data.

= Resampling methods for model evaluation =

validation set: split set randomly in two. train on one and test on the other. higher verifiability in rest error estimated as well as worse fit. overestimate test error.

leave one out cross validation: split set into validation and test set \f{N} time. each time one observation is in the test at and the others in the training set. the error is the mean of all the errors for all the folds  - lower bias and an better estimate of the test error. same result every time since you don't have randomness. more computationally expensive. linear models have \f{CV = 1/N \sum_{i=1}^{N} ((y_i - \hat{y}_I)/(1-h_I))^2} where \f{h_I} is the leverage of that observation.

k-gold cross validation: for k times perform a split into k buckets randomly. eat on one and train on the others. average error is the reported result. easier to compute. one usually uses k-fold cross validation with \f{k = 5} or \f{k = 10}. there is a bias variance trade off occurring. using high k we get fairly unbiased estimates of the test error but the values are highly correlated since the models are trained on roughly the same data. hence higher variance. lower k means higher bias but lower variance. same approach for classification. can be used along side hypothesis testing for features and derived features to aid model selection.