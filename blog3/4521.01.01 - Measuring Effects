In some cases, we are interested in the magnitude of something, not only its significance. We then have to say something about the effect size and its direction. Usually this comes in the context of comparing two means, because with just one mean we know what we have.

= Effects Of Proportions =

For proportions (estimates of probabilities) there are several measures:
* Absolute Risk Reduction: \f{\p_1 - p_2}.
* Relative Risk Reduction: \f{(\p_1 - p_2) / p_1}. Can exagerate the importance of an effect, esp when asbolute risk are low. 
* Odds Ratio: \f{p_1 / p_2}.

= Differences In Means =

When the values are measured in ratio scales, and the means are useful by themselves, then we can simply use the difference. We can also raport the difference in means to the control mean, and get a percentage increase/decrease in what we're looking for. When the scale is not useful, as with interval scales, it is better to standardize - measure the difference relative to the standard deviation.

We have:
* Hedge's \f{g} is defined as \f{(m_1 - m_2) / SD}.
* Cohen's \f{d} is defined as \f{g\sqrt{N / (N-1)}} (SD standardized by \f{N} instead of \f{N-1}).

Effect sizes in \f{[0.2,0.5)} are small effects, in \f{[0.5,0.8)} are medium effects and above are large effects.

When the effect size is measured in standard deviation units as it is for Hedges's g and Cohen's d, it is important to recognize that the variability in the subjects has a large influence on the effect size measure. Therefore, if two experiments both compared the same treatment to a control but the subjects were much more homogeneous in Experiment 1 than in Experiment 2, then a standardized effect size measure would be much larger in the former experiment than in the latter. 
