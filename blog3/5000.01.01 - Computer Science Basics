An \def{algorithm} is any well-defined computational procedure that takes some value or set of values as \def{input} and produces some value or set of values as \def{output}. It is a sequence of computational steps for turning inputs into outputs. A \def{computational problem} specifies in general terms the desired input/output relationship. A particular input, in the proper formar for the problem, is called an \def{instance} of the problame. An algorithm is \def{correct} is, for every input sequence, it halts with the correct output. The algorithm then \def{solves} the given problem. Incorrect algorithms are sometimes useful, if we can control how they error. A \def{data structure} is a way to store and organize data in order to facilitate access and modifications.

\def{Analyzing an algorithm} means predicting the resources that the algorithm requires. The types of resources tracked are:
* \def{Time}: How much time does the algorithm need in order to process an input of a certain size.
* \def{Space}: How much space does the algorithm need in order to process an input of a certain size.
* \def{Communication}: How many messages does the algorithm need in order to process an input of a certain size.

We must have a model of the implementation technology before starting the analysis. The standard is to use the \def{random-access machine}, which is a one processor, one memory, simple arithmetic and branching, uniform memory access machine. Our algorithms will be implemented as programs for this machine. We limit the size of the \def{words} on the machine to \f{c\log{n}} bits where \f{c \geq 1}, such that we can at least index the data we have, but can't store arbitrarily large amounts of data in a word. No model of caches.

The best notion of \def{input size} depends on the problem. Examples of inputs:
* The number of items in the input, the length of a list etc.
* The number of bits used to represent two numbers.
* The number of nodes and edges in a graph.

The \def{running time} of an algorithm on an input is the number of primitive operations, according to the model of the implementation technology used, necessary to compute the output on that input. We usually express this just as a function of \f{n}, not of the actual input, because that would be too messy. More precisely, this is the \def{worst-case running time}, \f{T(n)}, which describes the maximum time it takes to solve an instance of size \f{n}. Knowing this provides an upper bound on the running time. The worst-case is often pretty common. There is an \def{average-case running time}, \f{T_a(n)}, which describes the average time it takes to solve an instance of size \f{n}, assuming an uniform input distribution, and the \def{best-case running time}, \f{T_b(n)}, which describes the minimum time it takes to solve an instance of size \f{n}. The former is most interesting. The average case is often as "good" as the worst case. For the average time, if the input does not follow a nice distribution, we might use a \def{randomized algorithm}, that is, one which makes random choices, in order to fix some of the issues. Then we will deal with an \def{expected running time}.

We are interested in the \def{order of growth} of time functions. These functions are usually represented as sums of terms, and we are only interested in the term with highest growth, which will dominate the others. This means that the behaviour we describe will be true asymptotically, and at lower size inputs, things might be different, due to how all the terms interact.

A \def{loop invariant} is a statement which helps with proving an algorithm is correct. It is an abstraction over a more formal induction, with a bound on only \f{1} to \f{n}. Must have:
* Initialization - The invariant must be true prior to the first iteration of the loop.
* Maintenance -  If the invariant is true before an iteration, it must be true after it as well.
* Termination - When the loop terminates, the invariant should give a useful property to show that the algorithm is correct.

The \def{searching problem} is: given a sequence of \f{n} numbers \f{A = [a_1,\dots,a_n]} and a value \f{v}, find the index \f{i \in \overline{1:n}} such that \f{v = A[i]} or the special value \f{null} if there is no such element in \f{A}.

When working with recursive definitions, we can reduce them from structural inductions to simple inductions, by using a construction such as: "\f{S(n)}: the object \f{o} constructed by \f{n} applications of the recursive construction rules has some property \f{A(o)}".

A very useful induction form is the gated induction: \f{S(n)} is \f{\text{if} P(n) \text{then} Q(n)}. We can disregard any situation where \f{P(n)} is false on the fact that \f{F \Rightarrow T} is true. This is useful to restrict our induction to a certain region \f{\hcrange{1}{n}} or only for \f{n} powers of \f{2} etc.

\def{Structural induction} is induction on trees. If a statement \f{S(T)} about trees is true for the case of a single node, and if, for a given node \f{n} with children \f{c_1,\dots,c_k} the fact that it is true for the trees rooted at \f{c_1,\dots,c_k} means that it is true for the whole tree rooted at \f{n}, then \f{S(T)} is true for all trees. Intuition for why structural induction works: we start with all possible single node trees. For this \f{S(T)} is true. Then we add parents to these, and again, \f{S(T)} is true, by induction step. We continually add parents and other children at every step having the conditions for either the base case or the induction case, therefore \f{S} is true for whatever trees we build. But, do we build all trees? More formally, assume there is some tree \f{T_0} for which \f{S(T_0)} is false, and \f{T_0} is the smallest of such trees. \f{T_0} cannot be a single node, because otherwise the base case would apply. Then \f{T_0} has \f{m \leq 1} nodes. But, the subtrees of the children of \f{T_0}, \f{T_1,\dots,T_k} have at most \f{m-1} nodes each (otherwise we would get more than \f{m} nodes in \f{T_0}). But by our assumption about \f{T_0}, \f{S(T_i)} is true. Then, by the induction hypothesis, \f{S(T_0)} is true as well. Contradiction. This can also be reduced to strong induction, by building \f{S'(i): for all trees of }T\f{ nodes, }S(T)\f{ is true}. Error in argumentation: assume \f{S(T)} true for all \f{n}-node trees and add a node somewhere and say that it is true for all \f{n+1}-node trees. We haven't proved it for all \f{n+1} node trees, though.

= Algorithm design principles =

== Incremental approach ==

Build the solution one piece at a time. Usually will maintain some sort of invariant on each step, for the subset of things already processed.

== Divide And Conquer ==

Divide-and-Conquer: a general approach to solving problems in which a problem is partitioned into smaller components, the smaller components are solved (usually in the same manner -- recursive) and then the results are combined in some way.

Break the problem into several subproblems which are similar to the original problem recursively, and then combine these solutions to create a solution to the original problem. The \def{divide-and-conquer} approach.

Dividing a problem into smaller subproblems is a general approach, however divide-and-conquer stands out by having the subproblems be similar to the original one.

Essential steps:
* Divide - Split the problem into a number of subproblems that are smaller instances of the same problem.
* Conquer - Solve the smaller problems. If the problems are small, do it straigh-forward, otherwise apply the procedure again.
* Combine - Form the solution to the original problem by combining the solutions to the subproblems.

A case which is solved recursively is called an \def{recursive case} while one which is solved directly is called an \def{base case}.

= Big-Oh / Asymptotic notation =

Given \f{g(n)} we denote by \f{\Theta(g(n))} the set of functions \f{\Theta(g(n)) = \{f(n) \colon \exists c_1, c_2, n_0 \colon \forall n \geq n_0 \colon 0 \leq c_1g(n) \leq f(n) \leq c_2g(n) \}}. If a function \f{f} belongs to this set we write \f{f(n) = \Theta(g(n))} instead of the more familiar \f{f \in \Theta(g(n))}. We say that \f{g(n)} is an \def{asymptotically tight bound} for \f{f(n)}.

We use \f{\Theta(1)} to refer to a constant function.

Given \f{g(n)} we denote by \f{O(g(n))} the set of functions \f{O(g(n)) = \{f(n) \colon \exists c, n_0 \colon \forall n \geq n_0 \colon 0 \leq f(n) \leq c g(n) \}}. If a function \f{f} belongs to this set we write \f{f(n) = O(g(n))} instead of the more familiar \f{f \in O(g(n))}. We say that \f{g(n)} is an \def{asymptotic upper bound} for \f{f(n)}.

We have \f{\Theta(g(n)) \subseteq O(g(n))}, or, more informally, if \f{f(n) = \Theta(g(n))} then \f{(n) = O(g(n))}.

When we use \f{O}-notation to bound the running time on a worst case input, we have a bound on every input. A \f{\Theta}-notation bound of the running time of the worst input, however, does not apply to the full input set, since the best case might escape this bound.

Given \f{g(n)} we denote by \f{\Omega(g(n))} the set of functions \f{\Omega(g(n)) = \{f(n) \colon \exists c, n_0 \colon \forall n \geq n_0 \colon 0 \leq cg(n) \leq f(n) \}}. If a function \f{f} belongs to this set we write \f{f(n) = \Omega(g(n))} instead of the more familiar \f{f \in \Omega(g(n))}. We say that \f{g(n)} is an \def{asymptotic lower bound} for \f{f(n)}.

Given \f{g(n)} we denote by \f{o(g(n))} the set of functions \f{\omega(g(n)) = \{f(n) \colon \forall c \exists n_0 \colon \forall n \geq n_0 \colon 0 \leq cg(n) \leq f(n) \}}. If a function \f{f} belongs to this set we write \f{f(n) = o(g(n))} instead of the more familiar \f{f \in o(g(n))}. This says that the upper bound must be such that no matter how large we make \f{c}, we can find an \f{n_0} where \f{f} is smaller. The function \f{f} becomes insignificant relative to \f{g}, or \f{\lim_{n} f(n) / g(n) = 0}. It is as \f{<} is to \f{\leq} to \f{O}. We say \def{asymptotically smaller}.

Given \f{g(n)} we denote by \f{\omega(g(n))} the set of functions \f{\omega(g(n)) = \{f(n) \colon \forall c \exists n_0 \colon \forall n \geq n_0 \colon 0 \leq f(n) \leq cg(n) \}}. If a function \f{f} belongs to this set we write \f{f(n) = \omega(g(n))} instead of the more familiar \f{f \in \omega(g(n))}. This says that the upper bound must be such that no matter how large we make \f{c}, we can find an \f{n_0} where \f{f} is larger. The function \f{g} becomes insignificant relative to \f{f}, or \f{\lim_{n} f(n) / g(n) = +\infty}. It is as \f{<} is to \f{\leq} to \f{\Omega}. We say \def{asymptotically larger}.

We say that \f{f} is a tight bound on \f{T(n)} if \f{T(n) = O(f(n))} and for any other \f{g(n)} for which \f{T(n) = O(g(n))} then \f{f(n) = O(g(n))} - can't find a function which grows faster than \f{T} but slower than \f{f}.

We have \f{\Theta(g(n)) \subseteq \Omega(g(n))}, or, more informally, if \f{f(n) = \Theta(g(n))} then \f{(n) = \Omega(g(n))}.

\f{90-10} rule: \f{90\%} of the running time of a program can be attributed to \f{10\%} of the code.

[Similarity between notations and inequality operations]

== Big-Oh rules ==

Theorem: We also have \f{f(n) = \Theta(g(n))} if and only if \f{f(n) = O(g(n))} and \f{f(n) = \Omega(g(n))}.

Theorem: \f{f(n) = O(g(n))} if and only if \f{g(n) = \Omega(f(n))}. Just choose a \f{c_g = 1/c_f} and \f{n^g_0 = n^f_0} for the second inequality.

Theorem: \f{f(n) = o(g(n))} if and only if \f{g(n) = \omega(f(n))}. Just choose a \f{c_g = 1/c_f} and \f{n^g_0 = n^f_0} for the second inequality.

[Transitivity for all notations]
[Reflexivity for big notations]
[Symmetry for \f{Theta}]

[Trichotomy does not hold]

Theorem: an algorithm has running time \f{\Theta(g(n))} if the worst case running time is \f{O(g(n))} and the best case running time is \f{\Omega(g(n))}.

Theorem: \f{o(g(n)) \cap \omega(g(n)) = \emptyset}.

Constant factors don't matter. \f{T(n) = O(dT(n))} with \f{d > 0}.

Polynomial discarding \f{T(n) = O(a_kn^k + ... + a_1n + a_0) = O(n^k)}.

Slower growth discarding \f{T(n) = O(f(n) + g(n)) = O(f(n))} if \f{g(n) / f(n)} approaches \f{0} as \f{n} approaches infinity. alternatively \f{T(n) = O(f(n) + g(n)) = O(f(n))} if \f{f(n) \geq g(n)}.

Typical program time complexities and how they relate \f{O(1) = O(\log\log{n}) = O(\log{n}) = O(\log^k{n}) = O(\sqrt{n}) = O(n) = O(n\log{n}) = O(n^2) = O(n^k) = O(2^n) = O(k^n) = O(n^n)}. We also have \f{n^b = o(a^n)} for \f{a,b > 0} and \f{\log^b{n} = o(n^a)} for \f{a,b > 0}.

Transitive rule of Big-Oh relationshipts: if \f{f(n) = O(h(n))} and \f{h(n) = O(g(n))} then \f{f(n) = O(g(n))}.

If \f{f'(n) = O(g'(n))} then \f{f(n) = O(g(n))}.

The summation rule: \f{f(n) + g(n) = O(\max(f(n),g(n)))} - choose \f{n_0 = 1}, \f{c = 2}.

If \f{g(n) \geq 0} but otherwise arbitrary and if \f{T(n) = O(f(n))} then \f{g(n)T(n) = O(g(n)f(n))}. Use same \f{n_0} and \f{c}.

If \f{S(n) = O(f(n))} and \f{T(n) = O(g(n))} then \f{S(n)T(n) = O(f(n)g(n))}. Select \f{n_0 = \max(n_0^f, n_0^g)} and \f{c = c^fc^g}.

If \f{f(n) = O(g(n))} then \f{\max(f(n),g(n)) = O(g(n))}. Same \f{n_0} and \f{c = max(c^f,1)}.

If \f{f_1} and \f{f_2} are both tight bounds for \f{T} then \f{f_1} is a tight bound for \f{f_2} and \f{f_2} is a tight bound for \f{f_2} (by definition).

== Running times of basic program constructs ==

This is done inductively: every program can be represented as a tree - the structure tree associated, whose form is determined by the programming language used as well as by the program itself. For most C-derived imperative languages, this can be expressed in terms of simple statements, sequences of statements, ifs, fors, whiles and function calls. We start from the leaves and use Big-Oh rules as well as the following rules for structures, going up towards the root, as we build the full time.

Simple statements (assumed): \f{O(1)}. The base case of the induction.

Sequence of statements \f{s_1 s_2 \dots s_k}: \f{O(f_1(n) + f_2(n) + \dots + f_k(n))} or \f{O(\max(f_1,f_2,\dots,f_k)(n))}, by the summation rule..

If-statement: \f{if \text{exp} s_1 else s_2}: \f{O(\max(f_{\text{exp}}, f_1, f_2)(n))}.

For-loop: \f{for (i = \alpha; i < \beta; i += \gamma) s}: \f{O(\ceil{(\beta - \alpha)/\gamma} f(n))}. More precisely, the loop init is executed once, the loop test is executed \f{\ceil{(\beta - \alpha)/\gamma} + 1} times (\f{+1} for the last and failing test) and the increment is executed \f{\ceil{(\beta - \alpha)/\gamma}} times (might be \f{0} if \f{\beta < \alpha}).

While-loop: \f{while exp s}: \f{O(f_{\text{exp}} f(n))}, where \f{T_{\text{exp}}} must be proved to be of some form. More precisely, the loop test is executed once at least (first test), and at least twice if executes at least once.

Function call: the complexity of the function itself and some constant overhead for calling etc.

Dealing with recursive functions is a whole field in itself. See later.

== Dealing with recursive functions ==

Let \f{T_F(n)} be the running time of a recursive function (assume, for the sake of argument, it is direct recurtion). We must build a so-called recurrance relation between \f{T_F(n)} and \f{T_F(n')} where \f{n' < n} and it is the size of the argument we call \f{f} recursively with. Both the form of \f{n'} and the recurrence relation is determined from the structure of the program.

We then have a basis form of \f{T_F} corresponding to the base case(s) of the recursion and a recurrance form of \f{T_F} for the other cases, which is obtained by trying to apply the same rules as in the previous non-recursive cases, but replacing any call to \f{f} with \f{T_F(n')} instead of the Big-Oh form for it (because we don't know what the form for it is). We then replace all expression \f{O(g(n))} with \f{cg(n)} (one constant for each function) to allow for analysis. There are tools for solving recurrance relations in this form.

Tool 1 - Repeated substitution. Step (1) write \f{T_F(n)} in terms of \f{T_F(n')}. Step (2) write \f{T_F(n')} in terms of \f{T_F(n'')} etc until we reach the base cases. Assume constants as in previous point. Replace results from later steps into earlier steps until we reach \f{T_F(n)} and have, hopefully, a good form for it. Formally prove by induction of the form \f{S(i) \colon \text{if} 1 \leq i \leq n \text{then} T(n) = f(i)a + T(n - f(i))}. Don't have to worry about outside of the range. This makes sense for both linear structures \f{T_F(n) = T_F(g(n)) + f(n)} and tree-like structures \f{T_F(n) = kT_F(g(n)) + f(n)}, where \f{g(n) = n-1} or \f{g(n) = n/2} etc.

Tool 2 - guess a form for \f{f(n)} such that it is an upper bound for \f{T(n)} and it is easier to solve for. Much like differntial equations. A general principle to use: if \f{A \leq B + E} and we can show \f{E \leq 0} then \f{A \leq B} follows. Another general principle: bound by \f{f(n) = g(n) + d}, where \f{g(n)} is the hinted form and \f{d} is a slack term we use to cover for lower order terms in \f{T(n)}. For merge-sort we guess \f{f(n) = cn\log_2{n} + d} for example. From our exploration there will be some additional constraints imposed on all variables involved. [See exercises in Foundations of Computer Science p161 for applications].

Tool 3 - when we have a recurrance of the form \f{T(1) = a} and \f{T(n) = T(n-1) + g(n)} then the overall is \f{T(n) = a + \sum_{i=2}^n g(i)}. Proof by induction/common sense.

Tool 4 - when we have a recurrance of the form \f{T(1) = a} and \f{T(n) = 2T(n-1) + g(n)} then the overall is \f{T(n) = an + \sum_{i=0}^{\floor{\log_2{n}}} 2^i g(n/2^i)}.

== Some other results ==

Assume \f{T(1) = a}, \f{b > 0}, \f{k \geq 0}. For each use the boxes/tree diagram to see more intuitively how the problem is decomposed and where all the sums appear.

\f{T(n) = T(n-1) + bn^k} means \f{T(n) = O(n^{k+1})}.

\f{T(n) = cT(n-1) + p(n)}, \f{c > 1} and \f{p(n)} is a polynomial means \f{T(n) = O(c^n)}. We first prove, by induction that \f{T(n) = ac^{n-1} + \sum_{i=0}^{n-2}c^i p(n-i)}. Let \f{A = \max_{i}p(i)} we get \f{T(n) \leq ac^{n-1} A \sum_{i=0}^{n-1} c^i}. Let \f{B = \max(A,a)} and we get that \f{T(n) \leq B/(c-1) c^n = O(c^n)}. This bound is tight, \f{T(n) = O(d^n)} does not hold for \f{d < c}. Prove by induction that \f{T(n) \geq \beta c^{n+1} > \beta d^n > d^n} where \f{\beta = \max(0, \min(a, \min_{i} p(i)))}.

\f{T(n) = T(n/d) + g(n)} means \f{T(n) = O(g(n))}. For \f{d = 2} imagine that the amount of work for \f{n} - \f{g(n)} is done by all other steps combined (draw a box-diagram). Therefore we get \f{O(2g(n))}.

\f{T(n) = cT(n/d) + bn^k} and \f{c > d^k} means \f{T(n) = O(n^{\log_d{c}}) = O(n^{k + \epsilon})} where \f{\epsilon} is the factor larger than \f{k} added by the fact that \f{c > d^k} and \f{\log_d{c} > \log_d{d^k} = k}. Using expansion and tree view of computationsee that \f{T(n) \leq \alpha n^k \sum_{i=0}^{\log{n}} (c/d^k)^i} where \f{\alpha = \max(a,b)}. Use sum of increasing powers formula to get a form in terms of the logarithm and we have \f{T(n) \leq c^{\log_d{n}} = n^{\log_d{c}}} (after some math). Intuitively, if the branching factor is greater than the inverse task decrease for \f{T(n/d)} (many branches of not sufficiently small size), we're going to have polynomial decrease in speed.

\f{T(n) = cT(n/d) + bn^k} and \f{c = d^k} means \f{T(n) = O(n^k)}. By the same process arrive at \f{T(n) \leq \alpha n^k \sum_{i=0}^{\log{n}} (c/d^k)^i}, but since \f{c = d^k} the sume is \f{\log{n}}. Intuitively, if the branching factor is equal to the inverse task decrease for \f{T(n/d)} we have a logarithmic decrease in speed.

\f{T(n) = cT(n/d) + bn^k} and \f{c < d^k} means \f{T(n) = O(n^k \log{n})}. By the same process arrive at \f{T(n) \leq \alpha n^k \sum_{i=0}^{\log{n}} (c/d^k)^i}, but since \f{c < d^k} the sum term is constant. Intuitively, if the branching factor is lower than the inverse task decrease the cost will not grow past \f{2} (or \f{c}) times the cost at \f{n}. Some of the results, seem counter-intuitive (esp. for the last one). The thing is, if the polynomial processing time is quadratic, even if you have \f{c=3} and \f{d=2} which would imply some sort of computation overlap it still is not enought to overcome the fact that we're doing \f{n^2} worth of work on the first.

= Linear search =

An incremental algorithm.

The worst case complexity is \f{\Theta(n)}. The average case complexity is still \f{\Theta(n)}, but we have a more precise \f{\Theta(n/2)}. Consider the array consisting of unique elements, each of which can be equal to the test element, independently with probability \f{1/n}. Then, the average time is \f{T_a(n) = 1 * P(X_1 = v) + \cdots + n * P(X_1 \neq v, \dots, X_{n-1} \neq v, X_n = v) = \sum_{i=1}^n i P(X_i = v) = 1/n \sum_{i=1}^n i = (n + 1) / 2}.

= Binary Search =

Synopsis: efficiently find an element \f{x} in a sorted array \f{A} of size \f{n}.

We first compare \f{x} with the midpoint element, with index \f{\floor{n/2}}. If they are equal, we have found the element we were looking for. Otherwise, \f{x} can be smaller or larger than the midpoint element. If it is smaller, than there's no point searching elements after the midpoint, since \f{A} is sorted. Therefore we restrict the search to only the first half of \f{A}, which has size \f{\floor{n/2}}. We do the reverse thing if \f{x} is larger than the midpoint. We repeat this procedure until we find the element, or we end up with an empty array, in which case the search fails.

The proof of correcteness is by induction. The base case is a one element array with \f{x} either equal or not. The induction step is an \f{n} element array, which we split into \f{\floor{n/2}} and \f{n - \floor{n/2} - 1} arrays and search for the element. Alternatively we induct on the difference between the high and low indices. The running time is \f{T(0) = O(1)} and \f{T(1) = O(1)} and \f{T(n) = O(1) + T(n/2)} which is make it \f{O(\log{n})} once we solve the recurrence relation. Alternatively if \f{P(k)} is the longest array such that the algorithm never makes more than \f{k} reads of the midpoint element and comparions, then \f{P(1) = 1} and \f{P(k) = 2P(k-1) + 1}. Basically we add another \f{P(k-1)} elements to an \f{P(k-1)} length array and one midpoint element to add another level of midpoint evaluations. Another proof that the search time is logarithmic.

We might even have trinary or \f{k}-nary search. At this point we have \f{T(n) = k\log_k{n}} steps, and this is optimal for \f{k=e} or \f{k=3}.

= Sorted List Merge =

Synopsis: given two sorted arrays \f{A} of size \f{n} and \f{B} of size \f{m}, produce a sorted array of size \f{n+m} containing each element from \f{A} and \f{B} exactly once.

Hold an index \f{i} into \f{A} and an index \f{j} into \f{B}, both equal to \f{0} at the start. Add \f{\min(A[i], B[j])} to the output array, and increment either \f{i} or \f{j}, depending on where the minimum actually comes. When we reach the end of an array, just copy the other one.

The proof of correctness is by induction.

The running time is \f{T(n) = O(n)}.

Uses \f{O(n)} memory, to store the output array.

= Sorting =

The \def{sorting} problem is: given a sequence of \f{n} numbers \f{A = [a_1,\dots,a_n]} as input, produce a permutation \f{A' = [a'_1, a'_2,\dots , a'_n]}, such that \f{a'_1 \leq a'_2 \leq \cdots \leq a'_n}. The items we wish to sort are known as \def{keys}.

Because many programs use it as an intermediate step, sorting is a fundamental operation in computer science. There are a large number of good sorting algorithms. Which algorithm is best for a given application depends on, among other factors, the number of items to be sorted, the extent to which the items are already somewhat sorted, possible restrictions on the item values, the architecture of the computer, and the kind of storage devices to be used: main memory, disks, or even tapes.

Given a list of elements \f{A} of length \f{n} from some set \f{X} and a comparison operator \f{<} on \f{X}, produce a list \f{A'} which is a permutation of \f{A} and which has every two consecutive elements in increasing order.

Most sorting algorithms assume the cost of accessing a random element of \f{A} is \f{O(1)}.

Important properties of a sorting algorithm:
* General approach
* Comparison - Only use \f{<} and swaps of elements to sort \f{A}.
* Distribution - Use splitting of \f{A} based on internal element structure in order to sort \f{A}.
* Other - Weird approaches to sorting \f{A}.
* Space complexity - Big-Oh bounds for extra space consumed apart from the input. Sorts with \f{O(1)} space complexity exist. A bad complexity would be \f{O(n)}, but this might be tolerated for advantages in external memory situations.
* In-place and out-of-place - When working on an array represented as a contiguous memory zone, makes use of only that space and at most a constant amount of other working memory, in order to perform the sort. In other words, it sorts the given array object instead of building a new one. The opposite behavior is called out-of-place. A slightly more technical definition includes the algorithms which use \f{O(\log{n})} working space, since requiring just constant space is too restrictive (only regular languages apparently) and does not technically allow using an index into the array as an extra memory, since that's logarithmic in \f{n}.
* Stable and non-stable - When sorting compound objects by key, keeps the original order of objects with equal keys. Non-stable sorts can be made stable by extending the comparison function to include all the fields, thus making the keys unique or by extending it to look at the original order (adt a hypothetical initial position index to the record which is compared upon), both of which have either extra memory or computational requirements or both.
* External or internal - Whether the algorithm is adaptable to the case where the data does not fit memory or not?
* Streaming - Can easily accommodate new values as they are adted to the input list. Assumes that the array grows.
* Cache behavior - What memory access patterns are employed and whether they have space locality, so that typical CPUs can cache chunks of the input for many uses.
* Serial or parallel
* Adaptability - Does the structure of the input, namely how sorted it already is, have any bearing on the time and/or space complexity of the algorithm?

General purpose sorting algorithms must make at least \f{n\log_2{n}} comparisions in the worst/average case in order to sort a sequence of \f{n} items. Given the input \f{X}, there is a permutation \f{\pi} that, when applied to \f{X}, produces the sorted result. There are \f{n!} permutations of the input (considered as distinct elements \f{\hcrange{1}{n}}), and each one has an associated permutation for sorting. The goal is to identify what permutation the input is in. This is done by looking at the order in which elements are placed. We perform \f{t} tests on elements. After one such test, we have that some permutations are valid (they contain the elements we've tested in the same order) while some are not, and we can discard the latter. Considering the worst case, at less than half of these will be removed. If we do \f{t} tests then, we have that at the end we have at least \f{n! / 2^t \leq N_t \leq 1} where \f{N_t} is the number of elements at time \f{t}. We then have that \f{\log_2{n!} \leq t} or \f{t = \Ometa(n\log{n})} by Stirling's approximation.

Stable sorts can be used when sorting complex data. First sort by the second key, then stable sort by the first.

While there are a large number of sorting algorithms, in practical implementations a few algorithms predominate. Insertion sort is widely used for small data sets, while for large data sets an asymptotically efficient sort is used, primarily heap sort, merge sort, or quicksort. Efficient implementations generally use a hybrid algorithm, combining an asymptotically efficient algorithm for the overall sort with insertion sort for small lists at the bottom of a recursion. Highly tuned implementations use more sophisticated variants, such as Timsort (merge sort, insertion sort, and additional logic), introsort (quicksort and heap sort).

A general approach for sorting large and complex data sets is to build an index of only the sorting key and a link to the record and sort that. The index might fit in memory, but even if it does not, it might still be cheaper than doing all the transfers. This is known as tag sort.

== Selection Sort ==

Synopsis: sort an array \f{A} of size \f{n} by repeatedly finding the minimum of a smaller and smaller subarray.

First, find the minimum of \f{A} and write it at position \f{0}. Then, find the minimum of \f{A[1:n]} and write it at position \f{A[1]}. Repeat the procedure until we reach position \f{n}, which is already the maximum.

The proof of correctess is by induction on the current iteration of the outer loop, bounded to only \f{1 \leq i \leq n}.

The running time is \f{T(n) = O(n) + \dots + O(1) = O(n^2)}, regardless of input form.

Uses \f{O(1)} memory.

== Insertion Sort ==

This is an incremental algorithm.

Synopsis: sort an array \f{A} of size \f{n} by shifting elements to make room for elements at the proper position -- hard to write a short description here. 

First, assume only the first element of the array is sorted. Then, at the \f{i}th repetition, we insert element \f{i} at its proper position in the array \f{A[0:i]}, such that all elements before it are smaller and all after are larger. We do the insertion, by shifting elements starting from \f{i-1} until we find an element \f{A[j]} which is smaller than the starting value of \f{A[i]} (which we must save somewhere). We write the previous version of \f{A[i]} at \f{j+1}.

The proof of correctness is by induction on the current iteration of the outer loop, bounded to only \f{1 \leq i \leq n}, with a loop invariant that at the start of each iteration of the for loop, the subarray \f{A[1:j-1]} contains the original elements in \f{A[1:j-1]}, but in sorted order.

The running time is \f{T(n) = O(1) + O(2) + \dots + O(n) = O(n^2)} on average, but is \f{T(n) = O(n)} if the array is already sorted or almost sorted. More precisely, we have \f{T(n) = O(n+I)}, where \f{I} is the number of inversions in the input.

Uses \f{O(1)} memory.

The algorithm is \ref{in place}.

== Bubble Sort ==

Synopsis: sort an array \f{A} of size \f{n} by repeatedly swapping adjacent elements which are inversions. Once no such elements are found the array is sorted.

Two nested loops are used to walk the array \f{n} times, the inner one going from \f{i+1} to \f{n}. If the two adjacent elements are an inversion, we swap them.

The proof of correctness is based on the fact that there can be at most \f{n(n-1)} inversions, and at each iteration we decrease their number by \f{1} if we find one, and do not increase it at all. At the end we have no inversions left. 

The running time is \f{T(n) = O(n^2)}, regardless of input form.

Uses \f{O(1)} memory.

== Merge Sort ==

This is a divide-and-conquer algorithm.

Synopsis: efficiently sort an array \f{A} of size \f{n} by sorting two equal sized subarrays.
First, sort the first half of \f{A}, which is an array of size \f{\floor{n/2}}. Then, sort the second half of \f{A}, which is an array of size \f{n - \floor{n/2}}. Then, merge the two sorted arrays to obtain the final one, using \ref{Sorted List Merge}. We stop when we find an empty array or an array with a single element.

The proof of correctenss is by strong induction. The base case is that of an empty array or an array with a single element, which is, by definition, already sorted. The induction step is an \f{n} element array, which we split into two. By the induction assumption, they will be sorted. We have two arrays, and, because we've proven that \ref{Sorted List Merge} is correct, the final array, a merge of the two arrays, is sorted as well.

The running time is \f{T(0) = O(1)}, \f{T(1) = O(1)} and \f{T(n) = O(n) + T(n/2) + T(n/2)}, which means that, by solving the recurrence relation, we have \f{T(n) = O(n\log{n})}. The \f{O(n)} term comes from the \ref{Sorted List Merge} operation.

One can do \f{k}-merge sort for \f{k \leq 2} and an integer. The merge operation becomes more complex (needs to do a minimum on \f{k} elements at each step) and it costs \f{O(kn)}. The complexity becomes \f{O(kn\log_k{n})}. We choose optimum \f{k^\star = \arg\min_{k \leq 2} \int_{1}^{+\inf} kn\log_k{n} = \arg\min_{k \leq 2} k / \log_2{k} \int_{1}^{+\inf} n\log_n dn = \arg\min_{k \leq 2} k / \log_2{k}} (because the integral is a 'constant', though it diverges). We get that \f{k^\star = e}. As for integers a \f{k} of \f{3} is better than one of \f{2}.

Uses \f{O(n)} memory, because of the need to copy the output of the merging.

Merge sort can be combined with \ref{insertion sort} for the lowest parts, which results in a runtime of \f{\Theta(nk + n\log(n/k))}. While this isn't strictly better than merge sort in the asymptotic case, for a smallish \f{k}, say the size of a cache-line, one sees performance improvements.

== Quick Sort ==

Synopsis: efficiently sort an array \f{A} of size \f{n} by sorting two subarrays of roughly equal size.

First, pick an element from \f{A}, called the pivot. For simplicity suppose this is \f{A[0]}. Then, swap elements from such that all elements smaller than \f{A[0]} are to the left of \f{A[0]} and all elements larger are to the right. \f{A[0]} is in its final position, and we sort the array on the left and the array on the right, using the same method, until we find an empty array or an array with a single element, in which case we stop, as it is already sorted.

The proof of correctess is by strong induction. The base case is that of an empty array or an array with a single element, which is sorted, by definition. The induction step is an \f{n} element array, which we split into three pieces, the middle one having a single element at position \f{i}. Since we apply the algorithm on the first and third pieces, by the induction assumption, they will be sorted. Thus elements \f{A[0:i]} are sorted, \f{A[i]} is greater than all in \f{A[0:i]}, therefore \f{A[0:i+1]} is sorted, and \f{A[i+1:n]} is sorted and all are greater than \f{A[i]}, therefore \f{A[0:n]} is sorted, which is what we want.

The running time is \f{T(0) = O(1)}, \f{T(1) = O(1)} and \f{T(n) = O(n) + T(n_1) + T(n_2)}, where \f{n_1 + n_2 = n-1}. The split must be such that \f{n_1 = n_2 = O(n/2)}, and we then get, by solving the recurrence relation \f{T(n) = O(n\log{n})}. The \f{O(n)} term comes from the need to partition the array after the pivot. This holds, on average, but, for very common cases, such as the array being already sorted or almost sorted, we have \f{T(n) = O(n^2)}.

If we choose the pivot randomly, then we have an expected time \f{T(n) = O(n\log{n})}. If we choose the pivot by the \ref{Median Rule}, then we have a time \f{T(n) = O(n\log{n})}, regardless of input ordering.

Uses \f{O(\log{n})} memory, because of recursive calls.

Typically the fastest of the sorting methods in practice, barring hybrids of \ref{Quick
Sort} and \ref{Insertion Sort}.

== Heap Sort ==

Synopsis: efficiently sort an array \f{A} of size \f{n} by treating the elements as priorities for a \ref{Heap}.

First, \ref{Heap:heapify} the array. This is \f{O(n)}. Then perform \ref{Heap:deleteMax} on the heap and store the returned maximum as the last element of the array, which is done implicitly by \ref{Heap:deleteMax}.

The proof of correctness is by induction with the statement "\f{S(i)} if \f{1 \leq i \leq n+1} then \f{A[0]} contains the largest element in \f{A[0:n-i-1]} and \f{A[n-i-1:n-1]} contains the largest \f{i-1} elements in sorted order". \f{S(n+1)} gives the statement we want.

The \ref{Heap:heapify} operation is \f{O(n)}, a single \ref{Heap:deleteMax} is \f{O(\log{n})}, and the repeated deletion, is \f{O(n\log{n})}, which gives the final time complexity.

Uses \f{O(1)} memory, thanks to the \ref{Heap}.

== Counting Sort ==

Assume the input is a series of \f{n} distinct integers in \f{\hcrange{0}{2n-1}}. Build an array \f{C} of size \f{2n} and walk the input \f{X}. For each element encountered mark \f{C[X[i]]} true. After this, build a result array bywalking \f{C} and copying the index of every which has been marked. The total time is \f{O(2n) = O(n)} and space is \f{2n}.

== Counting Sort 2 ==

 Assume the input is a series of \f{n} distinct integers in \f{\hcrange{0}{n^2-1}}. Build an array \f{C} of size \f{n} of initially empty lists and walk the input \f{X}, adding element \f{X[i]\%n} to list \f{X[i]/n}. After this, walk the array and sort each list according to Counting Sort, but for range \f{\hcrange{0}{n-1}}. Finally, build the output array by scanning \f{C}, processing each list as \f{n*i + X[i].\text{iter}}. This seems \f{O(n^2)}. But, the initial scan is \f{O(n)}, the sorting of each array is \f{\sum{i=1}^n \abs{X[i]} = O(n)} since there are only \f{n} elements in the array, so the sizes of all the lists sum to \f{n}, therefore walking and sorting is \f{O(2n)} and the finaly building is \f{O(2n)} as well. Therefore total time is \f{O(2n) = O(n)} worst/average and total space is \f{1 + n + 2n = 3n} (assuming standard list structure). Same performance as Counting Sort but way larger range.

= Randomized Algorithms =

A \def{eterministic algorithm} always does the same thing on the same data.  Probabilistic algorithm: use a random number generator in the computational process - output is a probability distribution over possible outputs. A \def{Monte-Carlo algorithm} has binary output. Output true (in which case we can be sure that the true output is true as well) or "don't know" (in which the true output is true with a probability \f{p} and false with a probability \f{1-p}). The problem is then that of designing algorithms such that \f{P(Output = T \given Correct = T)} is high. Alternatively, if the answer is trully false, we return false, otherwise, if the answer is trully true, we return true, but with some probability (otherwise false). However, even if this probability is greater than zero, but small, we can build a good version by applying the algorithm \f{n} times. If one of the responses is true, we return true, otherwise we return false. Since these are independent, the probability that we get all falses is \f{(1 - P(Output = T \given Correct = T))^n}, which decreases very fast with increasing \f{n}. The probability of being correct is then \f{1 - (1 - P(Output = T \given Correct = T))^n}, which is high. This procedure is alwo a Monte-Carlo algorithm, as, if the ansewer is trully false, all tests will fail, and we'll return false, otherwise we'll return true with some probability.

== Determining if an array is sorted or not ==

Monte-Carlo algorithm for determining if an array is sorted or not. We assume that either the array is sorted or the numbers were generated uniformly and independently. Let \f{k \geq 1} be an integer. Then, we select \f{k} pairs of indices \f{(i,j)} independently, (without replacement?) and perform the comparison \f{X[i] < X[j]}. If this fails, we return "true" - we know the array is not sorted. If this passes, we continue with another test. If we've performed all \f{k} tests, then we output "I don't know". Based on the fact that if \f{X \perp Y} and both are uniformly distributed in an interval \f{\hcrange{1}{L}} (as numbers on computers must be) then \f{P(X \leq Y) = 1/2}. This, the probability of all \f{k} tests returning true is \f{(1/2)^k}. This is the probability of us outputting "I don't know" when the true answer is "true". For sufficient \f{k} it is small. Then \f{1 - (1/2)^k} is the probability of us outputting "I don't know" when the true answer is "false". For sufficient \f{k} is is nearly \f{1}.

= Hashing =

Suppose we have a set of objects or hash-keys \f{\mathbb{X}} and a number \f{K}. A hash-function is a function which maps an object \f{x \in \mathbb{X}} to a number in \f{\hcrange{1}{K}}, or, more precisely, \f{h \colon \mathbb{X} \rightarrow \hcrange{1}{K}}. \f{\mathbb{X}} can be a set of numbers, strings, complex structures, images etc. Hashing should assing each object a number, such that, given a reasonable distribution over \f{\mathbb{X}}, the distribution over \f{\hcrange{1}{K}} is uniform. A first requirement is that \f{\abs{\mathbb{X}} > K}. \f{K} is called the bucket number, from the fact that hash functions are famously used in hash tables, to assign objects to ``buckets'' - distinct storage areas.

If the objects are positive integers, then a common hash function is \f{h_K(x) = x \text{~mod~} K}, that is the remainder of \f{x} when divided by \f{K}. Care must be taken. If \f{K} and most of the objects have a common, then \f{K = fK'} and \f{x = fx'}, we have that \f{h_K(x) = f h_{K'}(x')}, and just \f{1/f} of the buckets are used. It usually is better to choose \f{K} a prime number, so it can't share any factors, and only the reduced risk of being a divisor to all objects is present. (We can write \f{x} as \f{f} groups of \f{x/f} counts and \f{K} as \f{f} groups of \f{K/f} counts - we then only need to look at how one group of \f{x/f} interacts with one group of \f{K/f} and multiply by \f{f} afterwards, for remainders and quotients).

In general, we can interpret any object as a collection of bits, therefore a number. However, it makes sense to have special rules for more complex objects.

If the objects are strings, we have, per our encoding scheme, a number associated with each character. We take groups of characters, such that the concatenation of the numbers yields ranges greater than \f{\hcrange{1}{K}} (for ASCII and \f{K = 100} we might take one character at a time, while for UTF-16 and \f{K = 10^9} we might take two characters at a time), sum these groups together and produce the remainder modulo \f{K}.

For records, we obtain the hash of every entry, sum it up and obtain the remainder modulo \f{K}.

For collections, we obtain the hash of every contained object, sum it up and obtain the remainder modulo \f{K}.

= Maximum Cost Subsequence =

A common subsequence of two lists is a list which is a subsequence of both lists. A longest common subsequence (LCS) of two lists the common subsequence of maximum length - it is not necessarily unique.

Given two lists, the problem is to find the longest common subsequence of the two. (Applications: diff command in UNIX, which shows differences between two files at the line level - elements of list are character strings and we first need to find an LCS of the two files as character strings in order to see where the changes occur (strings not in hte LCS)).

[Present naive approach] [The Dynamic programming algorithm for solving the LCS problem (also with weights)]. [Present reasoning behind it]. [Present a proof that it indeed works]. [Present a optimized algorithm + proof that it works]. [Present back-links approach to re-constructing optimal input and that it works]. All with time complexity.

Dynamic Programming Algorithm: [talk about it here]

= Strings =

Strings: lists of symbols (characters in some encoding). Need special storage capabilities, especially when they are used - en-masse. Strings can be stored either as an array of characters, using \f{'\\0'} - the NULL characteras - as a termination mark, or as an array of characters and a length, or as a list of characters (with more than one char per cell).

The set of strings of length \f{n \geq 1} from \f{k} symbols, where there are no two consecutive places holding the same symbol contains \f{k (k-1)^{n}} strings.

Lexicographic ordering: a total order for strings of symbols, which have a total order themselves. We say that \f{s^1 < s^2} if \f{s^1} is a prefix of \f{s^2} or if there is some \f{i \in \hcrange{1}{\abs{s^1}}} such that \f{s^1_j = s^2_j} for \f{j \in \hcrange{1}{i-1}} and \f{s^1_i < s^2_i}. An equivalent recursive definition for building two strings in lexicographic ordering is by using the recursive definitions \f{\epsilon < w} if \f{w \neq \epsilon}, if \f{c < d} then \f{cv < dw} for all \f{v, w} strings and if \f{v < w} then \f{cv < cw}.

Neat way to store a large index: have a single big array of characters. Strings are placed into this and separated by null characters. Separate symbol cell structures contain a pointer to the start of the symbol, a number of occurances and a list of all the placed where the words appear. Basically memory is a little less fragmented than in the other case, but nodes can still be arranged in a dictionary data structure and the list of position nodes still has to be placed somewhere. Though in this way we have just three blocks of memory dedicated to this instead of being fragmented. A nice insertion strategy first places a word at the tail of the big array, searches the search structure for the word. If found, we just increment its counter. Otherwise we create a new node, link it, point it to the new word, and increment the tail counter. We can compress the substrate array by using cells with \f{(base, length)} setups as well as storing overlapping strings together ('twomaniac' produces 'two', 'woman', 'man', 'mania', 'maniac' for example). Length can be one byte in most cases. How to build the best compressed sequence or even a good compressed sequence is another problem though.

= Operators =

Expressions:
* an expression is a strings of operands (symbols or numbers), binary operators and \f{()}. 
* the length of the string representing the expression is odd. \f{S(n) = there are some expressions of length n} if \f{n} is odd and \f{S(n) = no expression of length n} if \f{n} is even. Prove by induction.
* given a set of binary operators then an expression with \f{n} of them has \f{n+1} operands (prove by complete induction).
* given an expression with \f{+}, \f{E} and an operand \f{a}, one can rewrite it as \f{a + F}. Prove by induction and case analysis of what an expression looks like.
* given two expressions with \f{+}, \f{E} and \f{F}, one can transform one into the other. Reduce to standard form and use reverse of \f{F}.

Strings or balanced parantheses are strings of \f{(} and \f{)}, where, for each open \f{(} we have a correspondingclosed \f{)}, appearing after it. A profile for such a string is a function of length \f{n} which starts at \f{0} and is incremented for every \f{(} and decremented for every \f{)}. For balanced strings, it is never negative and ends at \f{0}. An equivalent (proof by structural inductions) definition is balanced parantheses are either \f{\epsilon} or \f{(x)y} where \f{x} and \f{y} are balanced parantheses. The parantheses in an arithmetic expression form a balanced parantheses.

Expression Trees: a rooted, binary and labeled tree used to describe simple mathematical expressions involving the numbers, variables and the operators \f{+, -, \star, /} and unary \f{-}. The recursive definition of expressions is \f{(1)} base case is either a number or a variable and \f{(2)} inductive case \f{E_1 + E_2}, \f{E_1 - E_2}, \f{E_1 \start E_2}, \f{E_1 / E_2}, \f{(E)}, \f{-E}, where \f{E_1}, \f{E_2} and \f{E} are other expressions. Such a tree is then either a node for a number or a variable or (uses number or variable as label), if \f{T_1}, \f{T_2} and \f{T} are expression trees for \f{E_1}, \f{E_2} and \f{T}, respectively, a node with one of the operations as root and the expression trees as children. No special care must be taken for \f{()} as they are automatically represented by the structure of the tree. To obtain the value of the expression we do a post-order traversal of a tree. The basis case is when we have a single node and it is an (1) number - we just return it or (2) variable - we return it from some variable store provided beforehand. The inductive case sees us with an node with an operand and one or two subtrees. We evaluate the algorithm recursively on the subtrees and then produce a result according to the operation.

Infix expressions are expressions written in the familiar form of arithmetic. Postfix and prefix expressions are the results of walking the expression tree in either postorder or preorder (and printing the label as an action). As long as operators use distinct symbols and have a fixed number of arguments, these expressions are unambiguous - don't need parantheses or precedence rules. To turn a prefix expression to infix, we find a triple \f{(op, lit, lit)} and replace it with a new literal. We repeat the process until only one literal remains. A tree is build in this fashion and it is the one corrsponding to the apporpriate infix expression. For postfix we search for \f{(lit, lit, op)}. Alternatively, we can do it recursively, in prefix form, if the current symbol is a literal, we return it, otherwise if it is a symbol, we parse from the next symbol for a new term, and then again from the new new symbol. This also works for evaluation. For postfix form, we can use a stack, and either build a tree, by pushing literals and popping when symbols occur, or actually evaluating everything.

Algebra: any sort of system in which there are operands and operators from which one builds expression. For it to be interesting, it usually has constants and laws that allow transformation of one expression into the other. Examples: numbers (with the arithmetic operators and their laws), sets (with union, intersection, difference etc.), relational algebra, algebra of regular expressions, boolean algebra etc.

An operator is idempotent if \f{S \start S = S}.

Prove two expression are equivalent: substitue any expression for any variable in an equivalence consistently. Substitute for an expression \f{E} an expression which is known to be equivalent to it. Prove that the resulting sets are each subsets of each other (consider an arbitrary element in the set defined by \f{E} and show that it is in the set defined \f{F} and vice-versa - textbook inclusion stuff).

= References =

There are many excellent texts on the general topic of algorithms, including those by Aho, Hopcroft, and Ullman [5, 6]; Baase and Van Gelder [28]; Brassard and Bratley [54]; Dasgupta, Papadimitriou, and Vazirani [82]; Goodrich and Tamassia [148]; Hofri [175]; Horowitz, Sahni, and Rajasekaran [181]; Johnsonbaugh and Schaefer [193]; Kingston [205]; Kleinberg and Tardos [208]; Knuth [209, 210, 211]; Kozen [220]; Levitin [235]; Manber [242]; Mehlhorn [249, 250, 251]; Purdom and Brown [287]; Reingold, Nievergelt, and Deo [293]; Sedgewick [306]; Sedgewick and Flajolet [307]; Skiena [318]; and Wilf [356]. Some of the more practical aspects of algorithm design are discussed by Bentley [42, 43] and Gonnet [145]. Surveys of the field of algorithms can also be found in the Handbook of Theoretical Computer Science, Volume A [342] and the CRC Algorithms and Theory of Computation Handbook [25]. Overviews of the algorithms used in computational biology can be found in textbooks by Gusfield [156], Pevzner [275], Setubal and Meidanis [310], and Waterman [350].
