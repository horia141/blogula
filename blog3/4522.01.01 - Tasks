Loss function: \f{L \column \hcrange{Y}{Y} \rightarrow \mathbb{R}} - the cost of predicting \f{\hat{y}} instead of \f{y}.

Binary classification: \f{L(y,y') = 1_{y \neq y'}}.

Regression: \f{L(y,y') = (y'-y)^2}.

Hypothesis set: \f{H \subseteq Y^X}, subset of functions out of which the learned selects his hypothesis. The learning algorithm chooses a hypothesis from this space. Depends on features, prior knowledge etc.

Scenarios: supervised, unsupervised, semi-supervised, transduction, active learning, reinforcement learning. We worry about supervised learning.

Training data: sample \f{S} of \f{m} iid from \f{\hcrange{X}{Y}} according to a distribution \f{D}.

The learning algorithm takes \f{S} and finds \f{h \in H} with small generalization erro \f{E_D [L(h(x),y)]}.

Deterministic case: \f{y = f(x)} - output label is uniquely determined on a input \f{x}.

Stochastic case: \f{P(y \given x)} or \f{y = f(x) + \epsilon} - output label is a probability distribution for a given \f{x}.

Generalization error for a given \f{h \in H} is \f{R(h) = E_D [L(h(x), y)]}

Empirical error: for \f{h \in H} and a sample \f{S} it is \f{\hat{R}_S(h) = 1/m \sum_{i=1}^m L(H(x_i),y_i)}.

The empirical error is an estimator of the generalization error. It is a unbiased one.

Measuring the quality of fit for regression:
* MSE: 

For classification:
* error rate: proportion of mistakes that we make if we apply our estimate of \f{f} to the training observations. \f{1/N \sum_{i=1}^N I[y_i \neq \f{\hat{f}}(x_i)]}. This is the fraction of incorrect classifications. any error measure must be computed on a test set - a portion of the full data we have, on which we don't perform the training procedure, and which is only used to test the quality of the estimator. We are interested in the quality on "future data", not on previous one. Avoids memoization/overfitting.

If we have a set of learning methods / models, then just selecting the one which produces minimum training MSE/error rate might not be good, because there is no reason to suspect correlaction between the two. Many methods especially select parameter estimates with the goal of minimizing training MSE/error rate, not overall error. Graph of test MSE/error rate vs training MSE/error rate as we improve complexity of the model - U form. Training MSE/error rate goes down to zero, while test MSE/error rate has the U form. This is a fundamental property all all methods and all datasets.

Consider that the minimum error is \f{\Var{\epsilon}} - if we find the correct model we still have this thing. Can't really evaluate it in general.

Note that regardless of whether or not overﬁtting has occurred, we almost always expect the training MSE/error rate to be smaller than the test MSE/error rate because most statistical learning methods either directly or indirectly seek to minimize the training MSE/error rate. Overﬁtting refers speciﬁcally to the case in which a less ﬂexible model would have yielded a smaller test MSE/error rate.

Given a test point \f{x_t}, we can define the test MSE for a given procedure as \f{(y_t - \hat{f}(x_t))^2}. We aim to reduce the expected test MSE as \f{E[(y_t - \hat{f}(x_t))^2]}. There is a similar decomposition of this point error, to the one for the whole space as: \f{E[(y_t - \hat{f}(x_t))^2] = (y_t - \hat{f}(x_t))^2 + \Var{\hat{f}(x_t)} + \Var{\epsilon}}. Here the randomness happens over \f{\hat{f}}, or rather over all the samples of size \f{N} we extract, and on which we build \f{\hat{f}}, and the test point is considered fixed. The overall test MSE is the average over all such test MSE for \f{x_t} (which is the one which varies). We needs to have low variance and low squared bias. Both are nonnegative, therefore the lowest error we can achieve is \f{\Var{\epsilon}}.

The variance we are referring to is that of the estimation procedure, when producing an estimator with different dasta sets. More flexible methods have higher variance. The bias is the error we are introducing by using a simpler model than the real function. It is unlikely any real world phenomenon is linear, for example. More flexible methods have less bias.

As a general rule, using more flexible methods results in high variance and low bias. The test error is a sum of these two, so wheter it increases or decreases when increasing felxibility depends on the relative rates of change of the two quantities. In general, the bias decreases faster, therefore the test MSE/error rate goes down. After a point, the limits of the method are reached, but the model starts capturing noise, so the variance increases, resulting in the test MSE/error rate going up.

The typical behaviour is that of the U-curve. The relationship is reffered to as the bias-variance tradeoff. Easy to find a method with low bias or low variance, but hard to find one with both.

The expected test error is minimized by the Bayes classifier: assign an observation to the class of maximum probability. The Bayes classifier produces the basyes decision boundary. The error rate of this classifier is the Bayes error rate, and it is the minimum pussible error rate: \f{1 - \max_k Pr(Y = k \given X = x_t)}. Overall, we have \f{1 - E[\max_k Pr(Y = k \given X)]}. This is analogous to the irreducible error (and, with generalized metrics it is probably the same concept).

The Bayes classifier is an unnataibale gold standard against which we compare other methods. Since we don't know \f{P(Y \given X)}, we can't really use it.

As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor.

Two bits about classification:
* conceptually diffrent from regression.
* many times one estimates the probability of a class, so that's a sort of regression problem.
* just coding qualitative values to a quantitative model yields bad results - they depend on the ordering of the variables, and assume equal difference in magnitude between the levels, which is rarely the case.
* for binary variables, using linear regression does "work", since the output estimates \f{P(outcome \given X)}, but the estimates will be outside the range sometimes. Need something better, and which scales.
* [Tp/Fp and associated discussions]
* can also use simple feature transformations to improve results.

Confounding: sometimes an average effect has a certain magnitude, while when lookin at specifics we see a different effect. For example, for some data, students are more likely to default than non-students, across all balance values. But, for a given balance level, a student is less likely to default. This is because there is correlation between student and balance - students tend to have higher balances overall (since it costs to go to college), therefore they tend to default more often, because that's associated with higher balances apparently, but an student with a certain balance is less likely to default than a non-student, because, if he's a student and has that kind of money he's more likely to be a better debitor.

Transforming binary classifiers to multi-class:
* one-vs-one
* one-vs-all

\def{Residual Plot} The scatterplot of the residuals from a regression / modeling setup.

\def{Descriptive Statistics} Numbers used to summarize and describe data Any number computed from the data Do not involve generalizing from the data at hand. They only speak about the single data we have.

\def{Inferential Statistics}  A number computed from a sample, which is then used to draw conclusions about the population. Usually, start with a descriptive statistics, compute it on the sample, and draw conclusions about the value this statistic would have, had it been computed on the population. Take sample size into consideration - the larger the sample, the better the results.

= Building block concepts =

data types by structure:
* structured: a table or collection of tables, basically a database. Feature extraction may or may not be required.
* unstructured: free form text (sentences, paragraphs etc.), audio and image databases etc. Feature extraction will almost always be required to get it into a format suitable for later processing.
* semi structured: collections of both structured and semi structured data.

data types by by rate:
* batch: one initial set of data and others at later moments, usually in discrete and large chunks.
* online: a never ending torrent of data.

analytical classes for transforming:
* preprocessing: data cleaning, separation, missing value computation, outlier detection/handling (although this might be seen as a more advanced topic), feature extraction (again might be more advanced topic) etc.
* aggregation: simple statistics, visualizations, fitting distributions etc.
* enrichment: add extra data (metadata?) to the already available data.

analytical classes for learning:
* classification.
* regression.
* clustering.
* recommendation.
* similarity matching.

analytical classes for prediction:
* simulation.
* optimization.

learning models:
* supervised.
* unsupervised.
* semi-supervised.
* transfer.

training style:
* offline: looks at the data at the start and produces the data product or a component of it. When new data is added, the system looks at it all again and produces a new version, which is swapped for the old one. Tied closely with batch execution.
* online: looks at the data as it arrives, updating the component or data product as it goes along. Tied closely with streaming execution. Deployment is only done initially, after that the component keeps evolving.

execution models:
* latency: average time between when data enters the system and it becoming available for users.
* timeliness: average age of the data incorporated in a response to a user.
* different systems have different latency and timeliness requirements, and in general one should choose the smallest latency and largest timeliness that make sense, given that improving on them is expensive from a resources point-of-view.

scheduling:
* batch: data is split into large chunks and processed one chunk at a time. Latency and timeliness are of the order of minutes, hours etc. Has a definite start and end time and it's easy to reason about performance. Easier to give time bounds for latency.
* streaming: data is processed as soon as it is available. Harder to give time bounds for latency.

sequencing:
* serial.
* parallel.

Data science products are iterative and the process of developing them has a self similar structure.
