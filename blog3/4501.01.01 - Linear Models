The linear model for regression problems consists of assigning the model \f{Y = \alpha_0 + \sum_{i=1}^p \alpha_i X_i + \epsilon}.

A set of weights \f{w}. Compute score \f{w^Tx}. For regression we can have \f{h_w(x) = w^Tx}. For classification we can have \f{h_w(x) = sign(w^Tx)}. For classification we can have (log-linear models) \f{h_w(x) = \sigmoid(w^Tx)}.

How to limit complexity? Maybe make \f{\sum_i w^2_j} small - limit the ability of hypotheses to give different scores to examples (sum zero is just a single hypothesis). This is \f{L_2} regularization - limit \f{\norm{w}^2 \leq B} - we can then prove generalization bounds in terms of \f{B}. We have \f{L_1} regularization. It leads to "sparsity", which helps select relevant features. The complexity of classes with only \f{k} non-zero entries is smaller than that of general vectors. We have \f{\choose{d}{k} 2^{kb} \leq d^k 2^{kb} \leq 2^{db}} - just bits. If we have \f{k}-sparsity, we wouldn't have to worry to much about irrelevant features. [super cool discussion in slides about why \f{L_1} encourages sparsity]

Training:
* Even minimizing the empirical error is not always easy. It is NP-hard.
* We have \f{\hat{R}(w) = \sum_i \mathbb{I}[sign(w^Tx_i) \neq y_i] = \sum_i \mathbb{I}[sign(y_i w^Tx_i) = -1]}. The last step is just an equivalent rewriting as a "sign margin".
* Much of the issues stem from the fact that the signed margin is not convex.
* We use a surrogate loss, which is convex, and is an upper bound of the signed margin loss - so it overpenalizez it - a pessimistic version of the thing we're trying to minimize.
* One example of this is \f{exp(-w^Tx_i)}. The sum of such things is also convex - so easy to work with.
* Why does it work? If the data is separable, then minimizing the surrogate (all the commonly chosen ones) will give you the right answer.
* If we were minimizing over all functions, we would still get the right answer (consistency).
* It works well in practice.

Surrogates: When the surrogate is very steep on the negative side (where mistakes are), then it penalizez misclassifications but can be overly sensitive to noise. Examples (as functions of \f{y = w^Tx}):
* zero-one loss: \f{\mathbb{I}[y < 0]}.
* hinge loss: \f{max(1 - y, 0)}.
* logistic loss: \f{log_2(1 + e^{-y})}.
* boosting loss: \f{e^{-y}}.
* square loss: \f{(1 - x)^2 \mathbb{I}[y \leq 1]}.

For the \f{L_2} norm, the optimization problem becomes: \f{L(w) + \lambda \norm{w}^2}. \f{\lambda} controls how much to penalize \f{w} and is selected by validation. Still convex. For the \f{L_1} norm, the optimization problem becomes: \f{L(w) + \lambda \norm{q}^2_1}. This produces sparse solutions (best approximation to actual sparse stuff, which is NP-hard).

Basic linear classifier: a very simple (geometric, global, paramteric) model for classification consisting of a plane, and described by \f{(w,t)} - the plane parameters. Let \f{\mu^+} and \f{\mu^-} be the means of the positive and negative examples. We choose \f{w = \mu^+ - \mu^-} as so it is perpendicular on the line between the two centers. We also want it to be placed at the midpoint between \f{\mu^+} and \f{\mu^-}, which is \f{1/2 (\mu^+ - \mu^-)} and which lies on \f{\alpha} as well. We have \f{w^Tx = t} as the decision boundry and \f{w^T 1/2 (mu^+ + mu^-) = t} or \f{(\norm{\mu^+}^2 + \norm{\mu^-}) / 2 = t} by this.

\f{w} is a linear combination of the examples.

Training consists of simply computing these \f{w} and \f{t}, by the formula.

If data is data from the two classes is normally distributed with equal covariance matrix - it is the best we can do. Notice that if the covariance matrices differ, and one set is larger than the other, we have problems - simply the midpoint will not do.

Assumes feature space is a vector space (usually an inner product space though).

Has translation, rotation, scaling/variance and invertible linear transformation invariance.

The actual \f{t} can be further varied, in a last step of learning, in order to maximize performance (see ROC).

The linear model for regression: A more sophisticated geometric, global, parameteric, grading model for regression. Assumes feature space is a vector space (though the notion of distance does creep up). Assumes there is only one output variable, and it is a linear function of the inputs, that is \f{y = a^Tx + b} or \f{y = a^Tx + b + \epsilon} where \f{\epsilon \tilde \mathcal{N}[0,\sigma^2]}. \f{(a,b)} describe a plane in \f{d+1}-dimensional space. \f{(x,y)} pairs live on this plane but are corrupted by noise in \f{y}. 

Linear models are simple because: they are parametric with small number of parameters, have low-variance (do not vary that much with traing data), have high-bias (assume a lot of the relationship). Preferable with limited data and want to avoid overfitting.

In the case of \f{d=1} we have univariate regression. Least-squares method: a learning algorithm which tries to minimise the expected average square error loss function on the residuals. \f{(a^\star,b^\star) = \arg\min_{a,b} \mathbb{E} [(Y - (a^TX + b))^2]}. We build estimates of these as \f{(\hat{a}^\star,\hat{b}^\star) = \arg\min_{a,b} \sum_{i=1}^N (y_i - a^Tx_i + b)^2 = \arg\min_{a,b} \norm{Y - Xa - b}_2^2}. This has a closed form solution called the Normal Equation and gives \f{\hat{a}^\star = (X^TX)^{-1}X^Ty}.

Used when we have two variables of interest. One is called the predictor, denoted by \f{X} and the other is called the cirterion/dependent, denoted by \f{Y}. We want to predict the value of \f{Y} from the value of \f{X}. The model is \f{Y = aX + b + \epsilon}. \f{\epsilon} must have mean \f{0}, and in general is \f{\epsilon \distas N[0,\sigma_\epsilon]}. Thus, \f{Y} dependes on \f{X} by a linear (affine) function \f{f(x) = ax + b}. The graph of this function is a line. Data in the real world will contain "noise".

How does one find \f{a,b?}. We have a sample of \f{(x,y)} pairs. We would want to find the best "fitting" line between \f{X} and \f{Y}. The usual criterion is that of minimizing the sum of squares \f{J(a,b) = E[(Y - aX - b - \epsilon)^2] = E[(Y - aX - b)^2]}. The estimator for this is \f{\hat{J}(a,b) = \sum_{i=1}^N (y_i - a x_i - b)^2}. The distance from each point to the line on the \f{y} axis will be minimized. This is a convex optimization problem, with closd solutions as \f{a = r s_y / s_x} and \f{b = m_y - a m_x}, where \f{r} is the estimate of Pearson's coefficient.

For stadardized variables we have: \f{a = r} and \f{b = 0}. That is \f{Y = rX} - one variable is simply the other one times the coefficient of linear dependency. \f{r} itself becomes an estimate for \f{E[XY]} - the correlation between the two variables.

For regular variables we have: \f{Y = r s_y / s_x X + (m_y - r s_y / s_x m_x) = m_y - r s_y / s_x m_x + r s_y / s_x X = m_y + s_y r (X - m_x) / s_x}. This has the nice interpretation of first, standardizing \f{X} (thereofe only the "shape" of the distribution of \f{X} is mantained, not its position or spread) and then spreading for \f{Y}, multiplying by the correlation coefficient \f{r} and then all this is around the mean of \f{Y}. \f{X} is standardized, stetched/mirrored by \f{r}, then restandardized in the domain of \f{Y}.

Notice that the computations are "assumption free". If we want to do inference however (compute confidence intervals, etc.) then we have assumptions.

Notice that we have by \ref{Biad Variance Decomposition} a setup of \f{V[Y] = V[f(X)] + V[\epsilon] = V[f(X)] + J(a,b)}. We have the proportion of variance explained by the model is \f{V[f(X)]/V[Y]} and the unexplained part \f{J(a,b) / V[Y]}. The latter expression is \f{r^2 V[Y]/V[X] V[X]/V[Y] = r^2}. Therefore \f{r^2 = V[f(X)]/V[Y]} - the square of Pearson's coefficient tells us something about the amount of variance explained by the linear approximation.

The standard error of the estimate is simply the standard error of \f{Y - f(X)} or \f{SD[\epsilon]} and is computed as \f{\sigma_{est} = \sqrt{\sum(y - f(x))^2/N}}. This can be expressed as \f{\sigma_{est} = \sqrt{(1 - \rho^2)V[Y] / N}}, from \f{V[\epsilon] = V[Y - f(X)] = V[Y] - V[f(X)] = V[Y] - \rho^2V[Y]} (by the previous link). Sample versions of this exist, but the division is by \f{N-2}, since both the intercept and slope must have been estimated.

We make inferences about \f{a} and \f{r}. We want to compute them, compute confidence intervals and hypothesize if \f{a} and \f{r} truly are different from \f{0}.

Assumptions for inference:
* The relationship is linear.
* Heteroscedasticity: the variance around the regression line is the same for all values of \f{X}. That is, the variance is independent of \f{X}. 
* \f{\epsilon} is distributed normally. Usually violated by outliers or "influential" points.
* Notice that no assumptions on \f{X} and \f{Y} were made - we've passed the buck from the variables themselves, to the form of the link.

To test that \f{a \neq 0} we have that the sampling distribution of \f{a} is \f{t} with \f{N-2} degrees of freedom and the standard error of \f{a} is estimated by \f{s_a = \sigma_{est} / \sqrt{SSX}} (where \f{SSX} is the sum of squared deviations from \f{X}). [Have to investigate where this comes from]

The confidence interval is computed using the same method, with the \f{t} test and same sampling distribution, of course.

For Pearson's correlation we have the classical hypothesis tests we had before. The same value is obtained as for \f{a} - so we get coupled results.

Influential observation: one which contribues a lot to the regression - leaving it out would result in significantly different results.

Cook's D - a measure of the influence of an observation. Proportional to the sum of squared differences between predictions made from a model learned using that observation and predictions made without it. A positive value. Rule of thumb: Cook D greater than \f{1} means that the observation has too much influence. The levarage \f{h} of an observation is how far apart it has its \f{X} from \f{E[X]} (if it is equal it has no influence of the analysis, not contributing to the deviation). The distance of an observation is based on the error of prediction for the observation. Commonly used is the studentized residual. (error of prediction divided by the sd of errors of prediction, basically). More complex.

Logistic Function:
* \f{\sigma : \mathbb{R} \rightarrow [0,1]}.
* \f{\sigma(x) = 1 / (1 + e^{-x}) = e^x / (1 + e^x)}.
* \f{\sigma'(x) = \sigma(x) (1 - \sigma(x))}.
* maps, nonlinearly, the real line to \f{[0,1]}. Linear around \f{(-1,1)} and highly nonlinear otherwise.
* used as a link function in GLMs (producing Logistic Regression) as well as an activation function in Neural Networks.

Neural Network are a model in neuroscience and in machine learning / statistics.

A neuron/perceptron:
* building block of neural networks and a basic computational unit.
* it has multiple inputs (real numbers) and one output. The computation it performs is \f{y = \sigma(\sum_{i=1}^d \Phi_i x_i + b) = \sigma(\Phi^T x + b)}.
* a neural network consists of nodes and their parameters (neurons) - the parametrization and the connections between them (described as a directed graph) - the topology.

Types of nodes: input (corresponding to problem input variables) with no computation attached (more like symbolic), output (corresponding to problem output variables) and hidden.

Types of networks, by topology:
* general/recursive: there can be cycles, and dependencies of one node on a 'future' node. [Not addressed here]
* feedforward: the graph is a DAG, nodes depend only on nodes closer to the inputs.
* layered: graph is a DAG, there are \f{k} layers. Input is layer \f{1} and output \f{k}. A node in layer \f{k} has inputs coming from all nodes in layer \f{k-1} and no other.

Forward propagation algorithm:
* given an input \f{X}, computes the output.
* for feedforward topologies: topologically sort nodes in the network, set values of \f{X} to input nodes, and for all other nodes, compute output based on ancestors.
* for layered topologies: in vector notation \f{O_1 = X} and \f{O_i = \sigma(\Phi X + B)} for \f{i \in \hcrange{2}{k}} for all others - machine efficient representation.
* the hypothesis is often \f{H[N,I,O,\Omega](x)} which is a vector function from input to output variables, \f{N} set of all nodes, \f{I} set of input nodes, \f{O} set of output nodes and \f{\Omega} the weight matrix for the connection graph (or set of matrices for each layer neurons in the layered topology - more compact).

Backpropagation:
* a training algorithm for a neural network.
* stochastic gradient descent on the cost function \f{\int_{\mathbb{R}^{d+o}} \norm{y - H[N,I,O,\Omega](x)} d(x,y) + \lambda / 2 \text{reg}(Omega)}.
* computing gradients:

The target variable is predicted from multiple variables. The model is \f{Y = a^Tx + b + \epsilon}. \f{\epsilon} must have mean \f{0}. We use the mean squared error loss and yet \f{\hat{a} = (X^TX)^{-1}XY}, where \f{X} is the data matrix. Much can be said about this.

The multiple correlation \f{R} is equal to the correlation between the predicted outcomes and the actual outcomes. It is always a positive value (a negative correlation would mean missing the mark seriously).

The regression coefficient \f{a_i} is the slope of the linear relationship between \f{Y} and the part of \f{X_i} that is independent of all other predictors. It represents the change in the output variable, when all other variables are held constant, and we vary \f{X_i} by \f{1}. In the plane model, we fix one coordinate, and we have a line/\f{n-1} dim hyperplane in the others.

It is advantageous to standardize the variables before a ML analysis. The resulting weights are \f{\beta} weight. \f{\beta_i} is the change in the outcome, in standard deviations, associated with a change of one sd in predictor \f{x_i}, when all others are held constant. We can do the same partition \f{V[Y] = V[a^TX + b] + V[\epsilon]}. The relationship is now \f{R^2 = V[a^TX + b]/V[Y]}.

Test for whether a subset of the variables is important: compute models with all variables and with all but the subset of variables. If there is a large difference in the explained variance, then we have something on our hand. [more complex computations here] Can test the significance of \f{R^2} by comparing with a model with no variables.

Assumptions for inference. Notice that moderate violations do not pose serious  problems for testing the significance of predictor variables. However, even small violations of these assumptions pose problems for confidence intervals for specific observations.
* Residuals are normally distributed. \f{\epsilon \distas N[0,\sigma_\epsilon]}.
* Homoscedasticity: the variance of the errors of prediction are the same for all predicted values.
* The relationship is actually linear.

If the \f{1}-padding is added to \f{x}, then this also describes a hyperplane, passing through the origin (without it, it is just an affine set).

We can find values for this model by minimizing the error between \f{\hat{a},b} as a predictor and the data. [standard differentiation etc.] We have the normal equation \f{\hat{a'} = (X^TX)^{-1}X^Ty}. The matrix must be invertible, which happens when all of the features are independent. We only have \f{d+1} parameters - don't need that much data to fit the model. For regression with the squared loss, we have \f{\hat{f}^\star = (\beta^\star)^T X} where \f{\beta^\star = \arg\min_\beta E_(X,Y) [(Y - \beta^T X)^2]}. By differentiating we get \f{d / d\beta \int (y - beta^T x)^2 P(x,y) dx dy = \int 2 (y - beta^T x) d / d\beta (y - beta^T x) P(x,y) dx dy = \int 2 (y - beta^T x) (-x) P(x, y) dx dy= - 2 \int xy P(x,y) dxdy + 2 \int \beta^Tx x P(x,y) dx dy = 0} which means \f{E[XY] = beta \int (x x^T) P(x, y) dx dy} or \f{E[XY] = \beta E[XX^T]} or \f{\beta^\star = E[XX^T]^{-1} E[XY]}, which is the population normal equation. The sample version of this is the well known normal equation.

For classification with the \f{0-1} loss, if we wish to dummy code the variables, we have that \f{f^\star(x) = E[Y \given X = x] = P(Y = 1 \given X = x]}, which \f{\hat{f}^\star(x) = {\beta^\star}^T x} approximates. However, this is not such a good model, since nothing bounds the values to \f{[0,1]} required for it to be a valid probability.

Model is stable, but biased.

= Log-Linear Models =

We have some input domain \f{\mathcal{X}} and a finite label set \f{\mathcal{Y}}. The aim is to provide a conditional probability \f{p(y \given x)} for any \f{x,y} where \f{x \in \mathcal{X}} and \f{y \in \mathcal{Y}}.

A \emph{feature} is a function \f{f_k(x,y) \in \mathbb{R}} where \f{x \in \mathcal{X}} and \f{y \in \mathcal{Y}}. These are often binary valued (indicator functions) - each feature asks a particular question wrt \f{x} and \f{y}. If we have \f{m} features \f{f_k} for \f{k = \hcrange{1}{m}} we can build a feature vector \f{f(x,y) \in \mathbb{R}^m} and \f{f(x,y) = \langle f_1(x,y) , \dots , f_m(x,y) \rangle}. In practice, the resulting vectors have very high dimensionality (hundred of thounsands or millions), but they are also very sparse (tens or hundreds are actually equal to \f{1}).

We define a \emph{parameter vector} \f{v \in \mathbb{R}^m}.

Each pair \f{(x,y)} is mapped to a \emph{score} by computing the inner product between \f{v} and \f{f(x,y)} or \f{v \cdot f(x,y) = \sum_{k=1}^m v_k f_k(x,y)} - a weighted sum of the features by \f{v}.

We define the probability of \f{y} given \f{x} under a parameter vector \f{v} as:

%formula{
p(y \given x ; v) = \frac{e^{v \cdot f(x,y)}}{\sum_{y' \in \mathcal{Y}} e^{v \cdot f(x,y')}}
}

Every score is first mapped to \f{e^{v \cdot f(x,y)}}, which is larger as the score is larger and tends to \f{0} as the score is larger. The denominator is used for normalization, so that the resulting quantity, when summed over all values of \f{y}, is \f{1}.

We also have \f{\log{p(y \given x; v)} = v \cdot f(x,y) - \log{\sum_{y' \in \mathcal{Y}} e^{v\cdot f(x,y')}}} which is a difference between a linear term and a normalization term dependent on \f{x}. So we have a logarithm term which is dependent on a linear term - hence the name.

Parameter estimation can be done with Maximum-Likelihood estimation. Suppose we have a training sample \f{\{ (x^{(i)}, y^{(i)}) \}_{i=1}^n}. We want to find \f{v_{ML} = \arg\max_{v \in \mathcal{R}^m} L(v)} where \f{L(v)} is the log-likelihood of the data, or \f{L(v) = \sum_{i=1}^n \log{p(y^{(i)} \given x^{(i)} ; v)} = \sum_{i=1}^n v \cdot f(x^{(i)},y^{(i)}) - \sum_{i=1}^n \log{\sum_{y' \in \mathcal{Y}} e^{v\cdot f(x^{(i)},y')}}}. \f{L(v)} is concave, therefore \f{-L(v)} is convex. We can use Gradient Ascent to find the maximum of this function. We have the gradients:

%formula{
L(v) & = \sum_{i=1}^n v \cdot f(x^{(i)},y^{(i)}) - \sum_{i=1}^n \log{\sum_{y' \in \mathcal{Y}} e^{v\cdot f(x^{(i)},y')}} \\
\frac{dL(v)}{dv} & = \underbrace{\sum_{i=1}^n f_k(x^{(i)},y^{(i)})}_{\text{Empirical Counts}} - \underbrace{\sum_{i=1}^n \sum_{y' \in \mathcal{Y}} f_k(x^{(i)},y') p(y' \given x^{(i)};v)}_{\text{Expected Counts under Current } v} \\
}

A first algorithm would be: Initialize \f{v = 0} and iterate until convergence the following: Calculate \f{\Delta = dL(v) / dv}, compute \f{\beta^\star = \arg\max_\beta L(v + \beta \Delta)} (A Linea Search) and set \f{v} to \f{v + \beta^\star \Delta}. This can be rather slow. Maybe use Conjugate Gradients! Or LBFGS!

We can also add a regularization parameter - the sum of \f{v} squared, which adds a penalty for large weights. It turns out that in typical ML estimation, with few examples of certain features (which is usually the case), the terms which appear always have probability \f{1} and \f{v \rightarrow \infty}. This extension makes learning in very high dimensions possible.

%formula{
L(v) & = \overbrace{\sum_{i=1}^n v \cdot f(x^{(i)},y^{(i)}) - \sum_{i=1}^n \log{\sum_{y' \in \mathcal{Y}} e^{v\cdot f(x^{(i)},y')}}}^{\text{Reconstruction}} - \overbrace{\frac{\lambda}{2} \sum_{k=1}^m v_k^2}^{\text{Regularization } \propto \norm{v}^2} \\
\frac{dL(v)}{dv} & = \underbrace{\sum_{i=1}^n f_k(x^{(i)},y^{(i)})}_{\text{Empirical Counts}} - \underbrace{\sum_{i=1}^n \sum_{y' \in \mathcal{Y}} f_k(x^{(i)},y') p(y' \given x^{(i)};v)}_{\text{Expected Counts under Current } v} - \underbrace{\lambda v}_{\text{Magnitude Control}}\\
}

The same algorithms as before work. \f{\lambda} is chosen by cross-validation and it controls the trade-off between ``reconstruction'' power and ``generalization'' power.

They are sometimes called Maximum-Entropy models.

= Ordinary Least Squares Regression =

We get \f{\hat{beta}_1 = \sum_{i=1}^N (x_i - \mean{x})(y-\mean{y}) / \sum_{i=1}^N (x_i-\mean{x})^2} and \f{\hat{beta}_0 = \mean{y} - \beta_1 \mean{x}}. We can rewrite the model as though \f{Y = \mean{y} + \Cov(x,y) (X - \mean{x}) / \Var(x,y)} using this.

We have the population line, which is unknown, and the least squares line, which is determined by these coefficients, and which is the sample line. It is an unbiased estimator of the population line. The average of the estimators from many samples will tend to be the actual values.

The standard errors are \f{SE(\hat{\beta}_0) = \Var(\epsilon) [1/N + \mean{x} / \var{x}]} and \f{SE(\hat{\beta}_1) = \Var(\epsilon) / \var{x}}, where lowercase latex is estimated. Notice that if \f{X} has mean \f{0}, \hat{\beta}_0 is a simple estimation of this, and has equivalent variance. Also, the larger the variance of \f{X}, the better we can estimate the slope (natural). This takes place at the population level, when the errors are uncorrelated and all with the same variance \f{\Var{\epsilon}}. We need to replace these with estimates, and \Var{X} with the estimate \f{RSE = \sqrt{RSS/(N-2)}} - the residual standard error (which sort of assumes the linear model, if you will).

We can then go about computing confidence intervals, using the standard errors. We should use the t-distribution with \f{N-2} degrees of freedom here, to have better setup, though, with the t statistic \f{t = \hat{\beta_1} / \hat{SE}(\hat{beta}_1)}.

\f{\beta_0} is output in the absence of signal, while \f{\beta_1} is increase in output for one unit increase in input.

We can also do hypothesis tests for if there is a link or not, which reduce to having the null \f{\beta_1 = 0} and the alternative \f{\beta_0 \neq 0}.

Model quality is assesed by:
* RSE/RSS: an estimate of the average deviation of the output from the true regression line (see formula). Small values are better than large values (a measure of the lack of fit). Measured in the units of \f{Y} - hard to interpret.
* \f{R^2}: this is the proportion of the variance explained by the model to the total variance. Large values are better (more variance explained) (a measure of goodness of fit). Measured in relative units in \f{[0,1]} - easy to interpret. The TSS is the total sum of squares measures the variability of the output in general \f{TSS = \sum_{i=1}^N (y_i - \mean{y})}. Thus we have \f{R^2 = (TSS - RSS) / TSS = 1 - RSS / TSS}. RSS describes the variance left after the model has been included. Small values of \f{R^2} mean either the model is wrong or the variability is high (both cause high RSS).
* For the ordinary linear regression we have that \f{R^2 = \hat{Corr}^2(X,Y)}.
* F-statistic:

The line always passes through \f{(\mean{x},\mean{y})}.

If we do a fit without an intercept (substract the mean from the Xs I think) we get with \f{\hat{Y}_i = \sum_{j=1}^N ((X_i Y_j) / \sum_{k=1}^N X_k^2) Y_j} - the output is a linear combination of all the other recorded outputs.

A method for understanding data and for prediction. Fit a line to data. We assume \f{Y = \alpha X + \beta + \xi} where \f{\xi} is everything we didn't measure. The best approximation according to Least Sqaures is \f{\sum_{i=1}^N (Y_i - (\alpha X_i + \beta))^2}. The \emph{residuals} are \f{\xi_i} for each observation. They are centered and spread around the best fitted line.

[image of OLS]

\f{\beta} is the \emph{intercept} term while \f{\alpha} is the \emph{slope} of \f{X}. The interpretation is that if we increase \f{X} by \f{1} unit, \f{Y} will increase by \f{\alpha} units. \f{\beta} is the value of \f{Y} when \f{X} is \f{0}.

We also have that, in an inferential sense, \f{\hat{\alpha} \sim \mathcal{N}[\alpha,\text{Var}[\hat{\alpha}]] \approx \mathcal{N}[\alpha,\hat{\text{Var}}[\hat{\alpha}]]} and \f{\hat{\beta} \sim \mathcal{N}[\beta,\text{Var}[\hat{\beta}]] \approx \mathcal{N}[\beta,\hat{\text{Var}}[\hat{\beta}]]} where \f{\alpha} and \f{\beta} are the true value of our population parameters and \f{\hat{alpha}} and \f{\hat{beta}} are their estimates on a sample and \f{\hat{\\text{Var}}[\hat{\alpha}]} and \f{\hat{\text{Var}}[\hat{\beta}]} are the estimates of the variance of these two.

If we have a factor variable as \f{X}, with \f{k} levels \f{\{L_1,L_2,\dots,L_k\}} we might write the regression as \f{Y = \beta_1 + \beta_2 \mathbb{1}[X = L_1] + \beta_2 \mathbb{1}[X = L_2] + \dots + \beta_k \mathbb{1}[X = L_k] + \xi}. The interpretation is that \f{\beta_1} is the average value of \f{Y} when the level is \f{L_1}, and \f{\beta_1 + beta_i} is the average value of \f{Y} when the level is \f{L_i} with \f{i > 1}. We can ask questions of the form, what is the average difference between the \f{Y} from \f{L_i} and \f{L_1}. We can swap \f{L_i} for \f{L_1} to compare with other levels \f{L_j}. This way we can see confidence intervals and P-values for the differences as well. This is basically a one-hot encoding.

For prediction one simply builds \f{Y_p = \hat{\alpha} X_p + \hat{\beta}} for the \f{X_p} you want to see. We can also produce prediction intervals, that is, for each \f{X_p}, a \f{\alpha\%} prediction interval. It says that the probability of a point from our model falling into that interval is \f{\alpha}. It assumes data that is normally distributed. Data which is highly nonlinear will have problems with these intervals.

= Multivariate Least Squares Regression =

\f{\beta_j} is the average increase in the output of a one unit increase in the \f{j} input, while holding all others fixed.

Big hypothesis test:
* \f{H_0 \colon \beta_1 = \beta_2 = \cdots = \beta_p = 0} and \f{H_p \colon} at least one \f{\beta_j} is non-zero.
* Used to test whether there is some some sort of influence or not.
* Use the F statistic, computed as \f{F = (TSS - RSS) / p / (RSS / (n-p-1))}. The last fraction has a mean of \f{\Var{\epsilon}} (naturally), while the first one has it as well (more thinking required), but only if \f{H_0} is true. Then, the ratio should be \f{1}. If we see something that is way bigger, we have something interesting. In contrast, if the linear assumptions held, and the \f{\epsilon_i} have a normal distribution, the F statistic follows an F distribution. And we can use that to compute hypotheses tests for \f{N} and \f{p}.

Restricted hypothesis tests:
* Test just a subset to be zero (usually the features are shuffled so that they are the last ones).
* We can fit a second model that uses all of the variables except the target ones (since by our assumptions they don't count, so why include them). If we compute the F statistic, but using the RSS of the second model instead of the TSS we get the same type of implementation.
* If we do this for just a single variable, but for all of them, the result is equivalent to treating each one as a ordinary linear model - in the sense of the statistics that we obtain.
* It is still worthwile to have the full model. Using one model for each variable, and with many variable, we'll get some variables be significant by chance. The F test corrects for this.
* Thinsg break down for \f{p > N} - can't even fit, let alone build statistics of fit.

Assumptions for inference:
* The output is linearly dependent on each variable (not on all of them? I guess it is a subset thing - if on all then on each part). Plot of output versus each variable.
* The residuals are normally distributed. A q-q plot can be used to verify. Look for outliers mostly here.
* The variance of the residuals is the same everywhere. Use a plot of absolute values of the residuals against their fitted values - mostly checks that variance does not depend on output value.
* The residuals are independent of each other. Correlation analysis etc. to find whether there are dependencies.
* It is necessary to summarize diagnostics for any model fit. If the diagnostics support the model assumptions, this would improve credibility in the findings. If the diagnostic assessment shows remaining underlying structure in the residuals, we may still report the model but must also note its shortcomings.

Variable selection:
* Just looking at individual variable p-values is not a good approach, because of chance issues. We need variable selection.
* Approach: test subsets of the \f{p} variables and select the best one.

Criterions for chosing this:
* Mallow's \f{C_p}
* Akaike information criterion (AIC)
* Bayesian information criterion (BIC)
* Adjusted \f{R^2}.

There are \f{2^p} such models, so the task is intractable. There are other approaches.

Forward selection: [described elsewhere] Can always be used, but will stop when it has \f{N} features selected if \f{p > N}.

Backward selection: [described elsewhere] Cannot be used if \f{p > N}, because we can't fit the model.

Mixed selection: do forward selection. At some point, a variable will have a larger p-value than a threshold. We remove it. We continue with adding until all variables in the model have a sufficiently low p-value and all variables outside have a large p-value if added to the model.

Model quality is assesed by:
* \f{R^2} : same computation; equal to squared \f{\Corr(Y,\hat{Y})} - for ordinary its the same thing, only \f{aX + b} comes out as \f{X} in the corr. Maximizes correlation among all linear ones. Adding variables always increases the training \f{R^2}, though if some variable has a small increase it is a sign it's not a good one.
* RSS/RSE: we have \f{RSE = \sqrt{1/(N-p-1) RSS}}. Notice that for small improvements in RSS by adding a variable, the RSE might actually increase.

Prediction:
* the estimates aren't perfect - one source of reducible error. The model has bias - another source of reducible error.
* the process has a random component - irreducible error.
* we can use confidence intervals to capture average behaviour given the inputs (looks only at the irreducible error, looks at \f{f(X)}), and prediction intervals (looks at all source of errors, looks at \f{Y} - the approximation) to capture behaviour given the inputs [not really clear here tho']?

Coding qualitative variables: such variables are called factors and they have levels.
* Binary case: use \f{0} for one level and \f{1} for the other. The model then becomes \f{Y = \beta_0 + \beta_1 X + \epsilon}. If \f{X = 0} we get \f{Y = \beta_0 + \epsilon} and if \f{X = 1} we get \f{Y = \beta_0 + \beta_1 + \epsilon}. Thus \f{\beta_0} is the average in obs. with value \f{0}, and \f{\beta_0 + \beta_1} the average in obs. with value \f{1} and \f{\beta_1} is the average difference between the two. If we use \f{-1} and \f{1} encoding, \f{\beta_0} will be the average, and the total \f{\beta_1} the amount above and below the average for the two classes, respectively. Same results tho'.
* Multi case: select one variable as the baseline, and use \f{k-1} other variables to code for the difference between the respective class and teh baseline. However, even though the numerical results might be the same, when looking at p-values there's an issue. The difference between a coeff and the baseline might be or not be significant depending on their values. Selecting another baseline might make things be significant. Better to test \f{H_0 \column} all \f{k-1} other coefficients are \f{0}. Use an F-test as well.

Classical removing of assumptions:
* include interaction effects: the \f{p(p-1)/2} products of \f{X_i} and \f{X_j} besides the main effects. Reduce some of the additive limitations. Could go up to \f{p}-sized pairs, but then we'd have \f{2^p} terms, which is very large. The hierarchical principle states that if we include in a term some variables, we should also include the main effects, regardless of their p-values. Still technically a linear model, just with more variables.
* include higher order terms (for ordinary). Reduces some linearity limitations. Still technically a linear model, just with more variables.
* include nonlinaer transformations of the terms (for ordinary). Reduces some linearity limitations. Still technically a linear model, just with more variables.

Issues:
* nonlinearity: use a residual plot to maybe detect it (between residuals and fitted values). Should have no pattern. use the tools to combat nonlinaerity presented previously.
* correlation among error terms: If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be. For example, a \f{95%} confidence interval may in reality have a much lower probability than \f{0.95} of containing the true value of the parameter. In addition, p-values associated with the model will be lower than they should be; this could cause us to erroneously conclude that a parameter is statistically significant. In short, if the error terms are correlated, we may have an unwarranted sense of confidence in our model. this occurs often in time series data - we can see tracking when plotting the error terms - adjacent residuals tend to take the same values. Good experimental design is important in order to mitigate such issues.
* non-constant variance of the residuals / heteroscedasticity - one can see a funnel shape in the residual plot (variance increases with magnitude of response). Transform response using a concave function such as \f{\log{Y}} or \f{\sqrt{Y}} which compresses the higher ranges. if the we have a clue about the variance at each observation (maybe it's an average of \f{N_i} elements of equal variance) we can use weighted least squares, with weights porportional to the inverse of the variance, \f{w_i = N_i} in this case.
* outliers: unusual value given \f{x_i}. affect the fit model, as well as artificially might increase computed variances/RSE which result in bad confidence intervals, \f{R^2}s etc. Residual plots and studentized residuals (residuals divided by the estimated se) are useful to detect such points (typical values should be between \f{[-3,3]}).
* high leverage points: unusual \f{x_i}. even more influence than outliers. Easy to spot in \f{1} and \f{2} dimensions. For other situations we compute a leverage statistic. [more details here]
* collinearity: two or more inputs are closely related. makes optimization surface have very tight valleys [image of two uncorrelated and two very corrlated variables and the surface] which makes for very big regions in the domains of the individual coefficients with the same value of RSS. big variance now, and low stability issues with changing datasets. we get larger standard error, decline of the t-statistic, failure to reject \f{H_0}s, loss of power. One solution: look at correlation matrix of the data - high values indicate big colinearities. Does not address multicollinearity. Compute the variance inflation factor - variance of \f{\hat{beta}_j} fit on the full model and \f{\hat{beta}_j} fit only with this - smalles value if \f{1} and means lack of collinearity. values greater than \f{5} or \f{10} means issues. Computes as \f{VIF(\hat{\beta}_j) = 1 / (1 - R^2_{X_j | X_{-j}})}, wher the \f{R^2} term is computed by fitting the \f{X_j} agains the other terms. Either drop the variable or replace the sepcific set with a new variable which is an average of the standardized variables.

A method for understanding data and for prediction. Fit a hyperplane to data. 

Things that can go wrong:
* confounders: A variable that is correlated with both the outcome and the covariates. Can change regression line (even opposite lines) if we don't account for them. Can be detected by careful exploration in the exploration side.
* complicated interactions
* skewness: Skewness in distributions of variables. Maybe take a log + 1.
* outliers: Outliers are points that do not appear to follow the patter of the other data points. May be simply extreme, or just weird points. Could be real (remove only with caution) or fictios (error in recording/processing etc.). Solutions: fit model w/wo outliers and see if there is any difference. Apply logs and see if it still works. Use robust methods/statitics.
* non-linear patterns: Lines aren't always the solution. 
* variance changes: Regression assumes the variance is constant across the domain of \f{X}. A funnel shape of the scatterplot. Solutions: box-cox transform, variance stabilizing transform, weighted least squares, huber-white standard errors.
* units/scale issues: Especially when working with populations. Number of deaths vs. number of deaths per \f{1000}. This does affect model fits, interpretation (changed units, what changes in the coefficients) and inference.
* overloading regression: The fewer terms in the model, the more interpretable the model is. 
* correlation and causation: Can we actually discover the sources of causation? Use caution when interpreting regression results - if there are surprising associations, could it be that there is a confunder, instead of an actual dependence?

= Count Regression =

Frequently the outcomes are variables with natural values. The Poisson distribution appears a lot around here. The mean and the variance are equal - important assumption. We do \emph{Poisson regression} or \emph{log-linear model}. We're interested, as in all regression models in \f{E[Y \given X,\alpha,\beta]}. Here, though, we focus on modeling \f{log(E[Y \given X,\alpha,\beta]) = \alpha X + \beta} - a linear regression therefore, and, we have \f{E[Y \given X,\alpha,\beta] = \exp(\alpha X + \beta) = \exp(\alpha X)\exp(\beta)}. The interpretation of \f{\alpha} is therefore the multiplicative increase of \f{Y}, given an increase of \f{1} in \f{X}. The residuals should have increasing variance as \f{X} increases, because of the link between mean and variance.

Perceptron: An online algorithm for binary/multiclass classification. Linear classifier. Hold \f{(w,b)} and precit \f{h(x) = \sign\{\inner{w}{x} + b\}}. At each example, if \f{h(x) = y} we do nothing, otherwise we update \f{w \rightarrow w + yx} and \f{b \rightarrow b + y}. This is a very weird perceptron. If the set is linearly separable with margin \f{\gamma} for plane \f{(w^\star,b^\star)} and maximum norm of examples \f{R}, according to Novikoff's theorem, we have that the algorithm makes at most \f{(1 + R^2)(1 + (b^\star)^2) / \gamma^2} mistakes in the training process. Space for storage is \f{\Theta(d + 1)}. Training time is \f{O(N (1 + R^2)(1 + (b^\star)^2) / \gamma^2)}. Prediction time is \f{\Theta(d + 1)}. Can be kernelized.

Novikoff's Theorem: If the set is linearly separable with margin \f{\gamma} for plane \f{(w^\star,b^\star)} and maximum norm of examples \f{R}, the perceptron training algorithm makes at most \f{(1 + R^2)(1 + (b^\star)^2) / \gamma^2} mistakes in the training process. Does not depend on dimensionality, but on geometry of the dataset. If \f{(w_t,b_t)} is the perceptron plane at time \f{t} (after the \f{t^{\text{th}}} mistake has been made), we look at its relationship with \f{(w^\star,b^\star)} and see that \f{\inner{w_t}{w^\star} + b_tb^\star \leq t\gamma} - the margin of the head point of the plane keeps increasing. On the other hand the norm \f{\norm{w_t}^2 + b_t^2 \leq t(R^2 + 1)}. Putting it all together and using Cauchy-Scwarz we get what we want.

Neural Networks: built from perceptrons, mainly. Objective is not convex. Hard to optimize, in general.

Residuals: \f{e_i = y_i - \f{hat{f}(x_i)}} and Squared Sum of Residuals \f{SSR = \sum_{i=1}^N e_i^2}. Fit parameters by minimizing SSR.

Key assumptions: effect is additive (effect of predictor \f{X_j} on \f{X} is independent of ther others) and linear (the effect of a unit change on any \f{X} on \f{Y} is independenet of the magnitude of \f{X}).

Coefficients we obtain are scale invariant - multiply a feature by \f{c} leads to scaling of them by \f{1/c}.

= Logistic Regression =

Frequently the outcomes are variables with binary values. Linear regression does not work as well - values outside our domain, susceptibility to variance etc. We can look at the outcome \f{Y}, probability \f{P(Y \given X,\alpha,\beta) \in (0,1)}, odds \f{P(Y \given X,\alpha,\beta) / (1 - P(Y \given X, \alpha,\beta)) \in (0,+\infty)} (ratio of probability of \f{1} and probability of \f{0} given \f{X} and our parameters) and log-odds \f{\log{P(Y \given X,\alpha,\beta) / (1 - P(Y \given X, \alpha,\beta))} \in (-\infty,+\infty)}. What we are going to do is first, model not \f{Y} but rather \f{P(Y \given X,\alpha,\beta)}. Second, we are going to assume that this has the form \f{P(Y \given X,\alpha,\beta) = \exp(\alpha X + \beta) / (1 + \exp(\alpha X + \beta))}. This is the \emph{sigmoid} function of the linear function \f{\alpha X + \beta}. Finally, we are going to model the log-odds as as linear function of \f{X}, or, more precisely, \f{\log{P(Y \given X,\alpha,\beta) / (1 - P(Y \given X, \alpha,\beta))} = \alpha X + \beta}. Notice that what we're really interested in is \f{E[Y \given X, \alpha,\beta]}. However, since we assume a \emph{bernoulli} distribution for \f{Y \given X,\alpha,\beta \sim \text{Binom}[\logit(\alpha X + \beta)]} we have that the mean is equal to the probability of seeing a \f{1}. Since the left term is from \f{-\infty} to \f{+\infty}, then we won't have domain problems from the linear form. After we estimate \f{\alpha} and \f{\beta}, we can compute the log-odds, and from those, compute the desired estimate \f{P(Y \given X,\alpha,\beta)}. \f{\beta} will be the log-odds of \f{Y = 1} if \f{X = 0}. \f{\alpha} is the log odds ratio of \f{Y = 1} for each increase in \f{X} compared to \f{X = 0}. This is called \emph{logistic regression}.

For prediction, you have, for a given \f{X_p} to compute \f{\hat{\alpha} X_p + \hat{\beta}} and then obtain a probability from this. You can plot, for the training sample, the distribution of these probabilities (and maybe see, for each class, the distribution of \f{P}s [draw graphs and barplots of these like in week 6 lecture 3 @ 17:30]). You have to choose a treshold which converts the probabilities into \f{0}s and \f{1}. One could use \f{0.5}, but it's not the best. We can use a series of cutoffs and select the one which has smallest error. This should be done on cross-validation.

Models the probability that \f{Y} belongs to a class or not.

Prediction is done by comparing the estimated probability with a threshold.

Ordinary Logistic Regression: The model is \f{p(Y=1\given X) = \logistic{\beta_0 + \beta_1 X}}. \f{\logistic} is a function which compresses the range into \f{[0,1]} with an S-shape. We can turn this into \f{\logit{p(Y=1\given X)} = \beta_0 + \beta_1 X}. There is a linear dependence between the logit of the probability and \f{X}. Many models are like this. An increase of one unit in \f{X} causes an increase of \f{\beta_1} units in the logit, or a multiplication by \f{e^{\beta_1}} of the odds. The change in probability is dependent on \f{X}, as would be expected (small towards the extremities). The logit is the logarithm of the odds.

Fitting is done by Maximum Likelihood usually. Can still compute confidence intervals and stuff. Qualitative variables can be incorporated into the model, as well.

Logistic Regression: [typical discussion] same discussion on interpretation as for Linear Regression.

For multiple classes: one-vs-one, and one-vs-all.

Issues:
* when the classes are well separated, the estimates from logistic regression are surprisingly unstable.
* small \f{N} and if the distribution of \f{X} is normal there are issues.
* not so nice multiclass.

= Shrinkage methods or Regularization =

Constrain the coefficients towards zero in some way. Reduces variance and helps with interpretability.

== Ridge Regression ==

Linear model but fitting procedure minimises \f{\beta^R = \arg\min \{RSS + \lambda \sum_{\beta_i^2}}} with \f{\lambda \geq 0} is a tuning parameter. Don't apply shrinkage to \f{\beta_0} as that is a measure of the central tendency of the data, not of interaction. \f{\lambda} controls who has more influence: fitting data or keeping coefficients small. Small coefficients cause small regularization term which is good for fitting.

Very much scale dependent, in stark contrast with least squares. Better apply this one to standardized data.

Why such power? bias-variance tradeoff - at \f{\lambda=0} we get the least squares model and at \f{\lambda=+\infty} we get the null model. In between we have a bias-variance tradeoff to make. For many high variance cases (high variance in data or less than or roughly equal observations to predictions) we can find a better model on the test set, with substantially lower variance and small bias, than selecting a high variance low bias ls estimator.

Another form of model selection, by controlling magnitude instead of number of predictors, as in subset methods. Still, selects all variables. Something better might select only a subset? bad for model interpretation.

== The LASSO ==

Linear model but fitting procedure minimises \f{\beta^R = \arg\min \{RSS + \lambda \sum_{\abs{\beta_i}}}} with \f{\lambda \geq 0} is a tuning parameter. Will set some variables to \f{0} - does subset selection and generates a sparse model. Good for model interpretation and computation in high \f{p} prediction situations.

= Alternative views =

Best subset: \f{\beta_B = \arg\min\{RSS\}} subject to \f{\sum{I[\beta_j \neq 0]} \leq s}. Uses the 0-1 norm.

LASSO: \f{\beta_B = \arg\min\{RSS\}} subject to \f{\sum{\norm{1}{\beta}} \leq s}. Use the L1 norm.

Ridge regression: \f{\beta_B = \arg\min\{RSS\}} subject to \f{\sum{\norm{2}{\beta}} \leq s}. Use the L2 norm.

For each \f{\lambda} there is an \f{s} which leads to equivalence.

Find the best RSS solution in a sphere of radius \f{s} from the center. If \f{s} is large enough then the sphere contains the ls solution. Also, you can understand why scaling is important, as \f{s} does not depend on the data, so making the data very large in magnitude causes us to have different coefficients on the sphere boundary. In best subset, we're intractable. But this is technically the best. All others are approximations. The closest we get to the strictness of the 0-1 norm, the better we are (do subset selection instead of just shrinkage). One can see why the Lasso leads to variable selection: if the ls estimates are outside the sphere of area \f{s}, then we can imagine equal RSS isohypses. These are tangent to ridge regression at any point on the sphere, but they are very likely to hit a corner of the polyhedra of the lasso, since it has exponentially many on dimension. Which will be better depends on goals (prediction (just test and see) vs interpretation (lasso usually)).

Bayesian view: Ridge regression model is a linear one, with gaussian errors and a gaussian prior on the parameters, all independent of each other. The solution is the mode and mean of the posterior distribution of the parameters. The LASSO model is a linear one, with gaussian errors and a double-exponential prior on the parameters, all independent of each other. The solution is the mode, but not the mean of the posterior distribution of the parameters. The Best subset model is a linear one, with gaussian errors and a diract delta prior on the parametsrs, all independent of each other.

Selection of \f{\lambda}
* throug cross-validation for each value of \f{\lambda} in a grid.
* might use the one sigma rule for selecting this for flat cost regions.

= Dimension Reduction Fitting techniqes =

first transform the data to a smaller dimension, then do least squares in that space. linear transform \f{Y = \beta_0 + \sum_{m=1}^M \theta_m Z_m + \epsilon = \beta_0 + \sum_{m=1}^M \sum_{i=1}^p \theta_m \phi_{mi} X_i + \epsilon}. This constraints \f{\beta_i} to have the form \f{\beta_i = \sum_{m=1}^m \theta_m \phi_{mi}}. Again, drop variance for a slight increase in bias. good when \f{M \leq\leq p} - we fit only \f{M} variables, but transformed, not subsets of the original.

== Principal Component Regression (PCR) ==

A version of dimension reduction fitting techniques. apply PCA to transform input space to \f{M} dimensions and do least squares in that space. good to standardize predictors. good when the first few principal components determine the data. select \f{M} via cross-validation. technically not variable selection, but transformation so that it is more suitable - uses all variables. PCA is unsupervised - no guarantee directions found are good for regression/classification. They just structure the data.

== Partial Least Squares ==

A version of dimension reduction fitting techniques. Similar to PCR, but tries to find dimensions that explain both \f{X} and \f{Y} (aid in explaining the response). perform standardization. [describe here] not much better than PCA or Ridge regression or the LASSO.

= High-dimensional case =

What happens when \f{p >> N}? perfectly fit data (if we can fit it) and too flexible solution (infinity of solutions to least squares for example). cannot use model assesment tools since we can't estimate \f{\hat{\sigma}^2} well - it is zero for higher \f{p} than \f{N}. Can't use p-values, simple \f{R^2}, RSS etc. in this setting as well. Use results on an independent data-set or cross-validation. use less flexible models (subset models, rigde, lasso) with higher bias but very low variance. cruse of dimensionality. adding extra features correlated with the output will cause the fit to become better, while adding uncorrelated features will cause a decrease in test error performance. a model we obtain might be a good predictor, which is in itself, useful. However, for interpretability it might not be so. Because of extreme collinearity, there can be many more subsets of features which are useful, for example. We just found some of them. Without independent experimentation and validation, it is unwise to say that the \f{\beta_i \neq 0} coefficients are associated with the output, and only they. They are, but they might not be the source.

= Basis Functions =

If we have a set of functions \f{b_1,b_2,\dots,b_k} which map \f{X} into another domain, we can build the model \f{Y = \beta_0 + \beta_1 b_1(X) + \dots + \beta_k b_k(X) + \epsilon}, which is a linear model in the expansion. We can then use all the tools for the classical model to fit this (confidence intervals, p-values, prediction intervals, measures of fit for traning and test sets, regularization etc.).

Examples: polynomial regression, step functions, splines etc.

== Polynomial Regression ==

extension of the ordinary linear model to \f{Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \dots + \beta_k X^k + \epsilon}. for \f{d = N + 1} we can perfectly fit (overfit) any dataset. So we usually keep \f{k} to small values like \f{3} or \f{4} or select it through cross-validation. fitting is done via least-squares with \f{k} predictors in a transformed space. this is a dimensionality expansion if you will, not a reduction as in PCR/PLS. can use all the tools of standard linear models, by first applying them to the expanded model - confidence intervals and p-values for inference, prediction intervals etc.

== Step Functions ==

bin the domain of \f{X} into \f{k} bins, by selecting \f{k-1} cutoff points \f{c_0=-infty < c_1 < c_2 < \cdots < c_{k-1} < c_k=+\infty}, and building \f{k} binary indicator variables \f{C_i(X) = I[X \in [c_{i-1},c_i)]}. alternatively, we build the model \f{Y = \beta_0 + \beta_1 C_1(X) + \cdots + \beta_kC_k(X) + \epsilon}. In each interval we fit a constant, the mean of \f{Y} on that interval (that is, we assume the model \f{Y = \mu_i + \epsilon} for \f{X \in [c_{i-1},c_i]}. More precisely, for \f{X < c_1} we only fit \f{\beta_0} which means it is the mean for variables less than \f{c_1}. For an arbitrary \f{c_i} we predict \f{\beta_0 + \beta_i}, so \f{\beta_i} is the average increase in the response for \f{X} in \f{[c_{i-1},c_i)} relative to \f{X < c_1}. Where do the bins come from: prior knowledge (age groups for example, geographic regions etc.) or cross-validation (very simple forms of it) or uniform spread across the domain, or try to keep \f{1/K} of the probability mass in them (\f{1/K}-tiles).

== Picewise Polynomials ==

a combination of step functions and polynomial regression - use polynomials on \f{k} regions. we have a total of \f{\sum_{i=1}^k k_i = O(kd)} degrees of freedom - maybe too much.

== Regression Splines ==

picewise polynomials with a twist - continuous derivates. If the polynomials we have are of degree \f{d} then the first \f{d-1} derivatives must be continuous. Cubic splines are very popular. The function itself must also be continuous. we are guaranteed being joined at the cutoff points as well as smoothness. because of the constraints we have \f{k + d + 1} degrees of freedom, much better than the \d{kd} degrees of freedom of the picewise polynomials. there are many spline representations. A common one is \f{Y = \beta_0 + \beta_1 X + \dots + \beta_d X^d + \beta_{d+1} h(X,c_1) + \dots + \beta_{d+k} h(X, c_k)} where \f{h(x,c) = (x - c)^d} if \f{x > c} or \f{0} otherwise. one can show that such terms have all the desired properties, and are discntinuous at the \f{d}th derivative. at the two outmost bins there is great variance with splines, because of small amount of data and high polynomial, usually.

=== Natural splines ===

use degree one polynomials for the two outmost bins - boundary conditions. lower variance in those regions. Where do the bins come from: prior knowledge (age groups for example, geographic regions etc.) or cross-validation (very simple forms of it) or uniform spread across the domain, or try to keep \f{1/K} of the probability mass in them (\f{1/K}-tiles). the number of degrees of freedom (from which we can derive the number of knots given a degree-d spline etc.) can be selected by cross-validation. natural splines are better than polynomials: can accomodate data better and they don't have boundary effects from which really high degree polynomials suffer very badly (classical bell-shape after roots region).

=== Smoothing Splines ===

We'd like a function \f{g} which fits the data well, that is, we approximate \f{Y = g(X) + \epsilon} and we wish to have a low \f{E[(Y - g(X))^2]}. If we don't put any other constraint, we end up with a perfect approximation, which results in overfitting.

One way to place extra constraints is to add a smoothness regularization term to this: \f{\sum_{i=1}^N (y_i - g(x_i))^2 + \lambda \int g"(x)^2 dx}, which has the typical loss + penalty form. The penalty term is small for smooth functions (\f{0} for linear ones) and large for wiggly functions. \f{\lambda} is a regularization control parameter and it is set by cross-validation (which has an efficient form as well).

The function which minimizes this is a natural cubic spline, with knots \f{x_1,\dots,x_n}, but a different one than what we'd obtain via regression splines. one can show that as \f{\lambda} increases from \f{0} to \f{+\infty} the effective degrees of freedom go from \f{N} to \f{2}.

==  Local Regression ==

given a test point \f{x^t}, find the \f{k}-nearest neigbhors to it (where \f{k} is determined by the span \f{s = k/N}, assign a distance based weight to them (drop-off gaussian), and then fit a weighted regression model on them. return the estimate of this regression. Generalization of previous basis functions models, no?

== Generalized Additive Model (GAM) ==

The model is \f{Y = \beta_0 + f_1(X_1) + \dots + f_k(X_k) + \epsilon} where \f{f_i} is a 1D regression function, as in linear regression, polynomial regression, smoothing splines, regression splines, local regression etc. Solving them can be a least squares fitting problem or done through backfitting (fitting each variable independently, in an iterative fashion). nice iterpretation when we look at \f{Y} dependent on \f{X_i} through \f{f_i} - model of the variation of \f{Y} as \f{X} changes, with all others held constant. can add interaction terms for low-dimensional issues. can extend to logit and other generalized linear model forms.

= Regression Towards The Mean =

The tendency of subjects with high value of a measure that includes chance and skill to score closer to the mean on a retest.

Encountered wherever \f{E[Y] = E[f(X)] + E[\epsilon]} where \f{f(X)} is the skill component and \f{\epsilon} is the luck component.

The essence of the regression-toward-the-mean phenomenon is that people with high scores tend to be above average in skill and in luck, and that only the skill portion is relevant to future performance. Similarly, people with low scores tend to be below average in skill and luck and their bad luck is not relevant to future performance. This does not mean that all people who score high have above average luck. However, on average they do.

For standardized variables and linear dependency, one can see \f{Y = rX} and a value \f{Y} will be closer to \f{0} than \f{X} is, unless perfect linear relationship \f{r=1} occurs.

This also occurs in experimenting without a control group (which one should not do). Is the effect we're seeing (compared to a base rate), because of luck or because of a systematic change?

= Convex Minimization Algorithms =

Gradient Descent basically.  [standard description]

Good schedule for decreasing step sizes: \f{\alpha_t} - they should be square-summable - \f{\sum_{t=1}^{+\infty} \alpha_t = +\infty} and \f{\sum_{t=1}^{+\infty} \alpha^2_t \leq +\infty}. You can also select it fixed. Or adaptive gradient (based on observed geometry of the feature space).

Subgradient Descent: If the objective is non-differentiable (has kings), there may be several directions that are "tied" for steepest descent'. L1 regularization usually introduces such kinks. Pick any of the descent directions and go for it. Usually works.

Stochastic Gradient Descent: In ML many optimization problems are sums of terms, and we're estimating the gradient - in the sense of actual statistical estimation. Then, if we have just a sample, why not take a subsample of that (commonly even one example) and use that as the estimate, instead of doing it over the whole set. It is an unbiased estimate of the full gradient.

Coordinate Descent: Take steps parallel to the coordinate axes (features in our case). Don't have to compute the gradient.

Second Order Methods: use the curvature of the function at a point. Not often used for large-scale data. Requires fewer steps.

Distributed Gradient Descent:

= Least Squares Others =

The mean points of \f{(x,y)} are on the regression line. Interpreting the intercept is often not that useful. Just the coefficients are interesting.

For each coefficient, we might report the point estimate, a confidence interval, the estimated standard error associated (for reporting purposes), a t value and a p-value for it being different from zero (if we take the same "level" all these infos are equivalent though).

Applying a model estimate to values outside of the realm of the original data is called extrapolation. Generally, a linear model is only an approximation of the real relationship between two variables. If we extrapolate, we are making an unreliable bet that the approximate linear relationship will be valid in places where it has not been analyzed.

Points with high leverage are points further away in \f{x} from the center of the cloud. They have higher influence on the regression line.

Don't ignore outliers without good reasons. If they're not errors, they might be very important to the modeling process.

The regression parameters follow the t statistic as well (their estimates' sampling distribution).

Of course, check conditions first - if the regression contitions don't hold, then the results might not be valid.

The coefficient attached to a binary variable corresponds to the difference in the outcome between the two levels of the variable (usually coded as \f{0} and \f{1}).

We say the two predictor variables are collinear when they are correlated, and this collinearity complicates model estimation. While it is impossible to prevent collinearity from arising in observational data, experiments are usually designed to prevent predictors from being correlated.

We say the two predictor variables are collinear when they are correlated, and this collinearity complicates model estimation. While it is impossible to prevent collinearity from arising in observational data, experiments are usually designed to prevent predictors from being correlated.

Adjusted \f{R^2} is an unbiased estimate of the variability explained by the model. It is \f{R^2_{adj} = 1 - (n - 1) / (n - d - 1) \mathbb{V}[residuals] / \mathbb{V}[Y]}. Is smaller than \f{R^2}. This will also go down when adding a new predictor, and it does not help the variance explaining. \f{R^2} would not be affected, but this one would go down a little - principle of parsimony etc.
