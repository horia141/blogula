= The Empirical Distribution Function =

A nonparametric estimator of the CDF of a random variable. It is defined as a function \f{\hat{CDF}_n} with \f{\hat{CDF}_n(x) = \sum_{i=1}^n \indicator{X_i \leq x} / n}. Intuitively, the distribution built has mas \f{1/n} at each observation, and we're building the CDF for it.

Estimator properties:
* Crucial: notice that \f{\indicator{X_i \leq x} \distas Bernoulli[\prob{X_i \leq x}] = Bernoulli[CDF(x)]}.
* Unbiasedness: \f{\mean{\hat{CDF}_n}(x) = CDF(x)} for all points \f{x}. Proof Idea: we have \f{mean{\hat{CDF}_n}(x) = 1/n \sum_{i=1}^n \mean{\indicator{X_i \leq x}} = 1/n \sum_{i=1}^n CDF(x) = CDF(x)}.
* \f{\var{\hat{CDF}_n(x)} = CDF(x)(1 - CDF(x)) / n}. Proof Idea: the sum of \f{n} IID Bernoulli variables, and the variance of one such variable is \f{p(1-p)}.
* \f{\lim{MSE_{\hat{CDF}_n(x)} = \var{\hat{CDF}_n(x)}}{0}}. Proof Idea: use the \ref{Fundamental problems of inference:Estimation / point estimation:Bias Variance Decomposition of MSE}{Bias-Variance decomposition}.
* Consistency: \f{\limprob{\hat{CDF}_n(x)}{CDF(x)}}. Proof Idea: if the bias is \f{0} and the standard error tends to \f{0} then we have what we want.

Glivenko-Cantelli Theorem: \f{\limprob{\sup_x{\abs{\hat{CDF}_n(x) - F(x)}}}{0}}. Intuitively, the largest/supremum absolute difference between any point in the function and its approximation tends to \f{0} as \f{n} increases. It's a good thing.

Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality: \f{\prob{\sup_x{\abs{\hat{CDF}_n(x) - F(x)}} > \epsilon} \leq 2e^{-2n\epsilon^2}} for any \f{\epsilon > 0}. While the \ref{Glivenko-Cantelli Theorem} tells us that this quantity goes to zero as \f{n} increases, this theorem tells us how fast to expect it to go down. We can build \f{1 - \alpha} confidence intervals by selecting \f{\epsilon_n = \sqrt{1/{2n} \log{2\alpha}}} and \f{L(x) = \max{\hat{CDF}_n(x) - \epsilon_n, 0}} and \f{U(x) = \min{\hat{CDF}_n(x) + \epsilon_n, 1}}, such that \f{\prob{L(x) \leq CDF(x) \leq U(x)} \geq 1 - \alpha} for all \f{x}.

For Bernoulli variables:
* We only have two positions where we can place mass.
* They add, therefore the mass at \f{0} is \f{\sum_{i=1}^n \indicator{X_i = 0}/n}, and the same for \f{1}.

= The Plug-in Estimator Of A Statistical Functional =

A nonparameteric estimator of the CDF of a random variable. It is defined as \f{\hat{T}_n(\hat{CDF}_n)}, that is, just plug in \f{\hat{CDF}_n} for \f{CDF}. In the linear case, this becomes \f{\hat{T}_n(\hat{CDF}_n) = 1/n \sum_{i=1}^n r(X_i)}.

Supposing we can find \f{\hat{se}}, then the estimator is asymptotically normal, that is, \f{\limdist{\hat{T}_n(\hat{CDF}_n)}{\mathcal{N}[T(CDF), \hat{se^2}]}}. This leads to a confidence interval built from the normal model, using Z-scores as is usual.

Examples of statistical functionals through the plug-in method:
* Mean: \f{\hat{\mu}_n = \int x d\hat{CDF}_n(x) = \samplemean{X}_n}. We know this to have certain properties [insert here all other properties].
* Variance: \f{\hat{\sigma^2}_n = \int (x - \mu)^2 d\hat{CDF}_n(x) = \int x^2d\hat{CDF}_n(x) - (\int x d\hat{CDF}_n(x))^2 = 1/n \sum_{i=1}^n (X_i - \samplemean{X}_n)^2}. This turns out to be biased [insert mean here]. A better alternative is the sample variance. [talk about it here].
* Skweness: [simply replace fraction with the components].
* Correlation: [simply replace fraction with the components]. All of statistics does a wierd decomposition here, but I'm not sure why it's needed.
* Quantiles: Since \f{\hat{F}_n} is not invertible, we must define quantiles a little bit differently. We have \f{\hat{Q}(q) = \hat{CDF}_n{-1}(q) = \inf{\set{x} {\hat{CDF}_n(x) \geq q}}}, or, more intuitively, the smallest value for which the empirical CDF is greater than the desired quantile. This is called the sample quantile.

= Smoothing =

Sometimes there are non-linear trends in data. Linear models might not work. We want to fit a curvy-but-smmoth line to fit the data. There is the risk of overfitting, of course. The mode is no longer easily interpretable, in general. They are often more useful for prediction. We talk for one variable case here.

In general, one should avoid predicting too-far out of the data interval, as the predictions might become wildly innacurate. Even more so than in the linear term.

= Filtering / Weighted Sum =

One example of smoothing is a running avarage of \f{2, 3, \dots} points, especially when we have time data. We're doing local averages, in essence. This will probably overfit. In general, this is a low-pass filter (from DSP), which removes the high-frequency ``noise'' components from the data set. The average can be weighted, with exponential decay, gaussian or tukey (\f{\max(1 - x^2,0)^2}) kernels, but with a sum equal to \f{1}. If the sum is one and some other properties hold, the smoothes signal is a good estimate of the function.

[maybe examples on CDC data]

= Lowess and Loess =

Lowess/Loess is locally weighted average of the data. Uses polynomials for interpolation. A span parameter controls the bias-variance tradeoff, a span percentage of the data is used to fit locally. This can be used for prediction as well.

[maybe examples on CDC data]

Span should be selected via cross-validation.

= Splines =

We fit like \f{Y = b_0 + \sum_{i=1}K b_i s_i(X) + \epsilon} - a linear combination of terms. Natural cubic splines are a good example of these. Can be used for prediction.

[maybe examples on CDC data]

Parameters should be selected via cross-validation.

