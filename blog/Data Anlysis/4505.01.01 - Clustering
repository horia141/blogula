=== Clustering ===

\subsection{Clustering}

One form of unsupervised learning is clustering. Clustering tries to find "patterns" in the data, or, in more precise terms, it tries to group items that are similar, by some choice of metric, in a way that's easy to interpret.

In clustering, as opposed to classification, it not necesarilly a good thing to have more observations than variables/features. This leads to non-sensical clusters, especially in high-dimensions with Euclidean similarity.

Clustering organizes things that are ``close'' into groups.

\subsubsection{Hierarchical Clustering}

This is a form of agglomerative clustering. Suppose we have a distance function $d$ on our input space $\mathbb{X}$, which can be the euclidean or correlation (for fully continuous variables), manhattan or hamming (for binary) etc, and which respects the axioms of a distance $d(x,y) = d(y,x)$ and $d(x,y) > 0$ if $x \neq y$ and $d(x,x) = 0$ and $d(x,y) \leq d(x,z) + d(z,y)$. Suppose also that we have a way to merge two observations $m(x,y)$, which can be an arithmetic mean, or some sort of weighted mean etc, and which produces another observation.

The algorithm proceeds by first finding the closest two observations according to $d$ and then removing these observations from the sample and adding the merged observation back to the sample. The size of the sample drops by $1$. We repeat this process a total of $N-1$ times until we are left with a single observation. A binary tree is built and returned as the output of the procedure. We start with $N$ initial trees which consist of a single binary node, each one associated to an observation. At each iteration of the algorithm, a new node is created and is associated with the new merged observation. Its children are the two closest nodes we've found. At the end of the algorithm, a complete binary tree results, with $N$ leaves and $N-1$ internal nodes.

A nice visualization of this tree is a dendogram.

[image of dendrogram]

To build actual clusters, we have to cut the tree somehow. Depending on where we cut the tree, we can have between $1$ and $N$ clusters, so maybe a cross-validation could be used to select good things.

Another way to proceed is to not compute a merge at each step, but, instead, keep the whole list of observations we've found so far. Then distance between two clusters can be computed as the distance between the means of the cluster, or the distance between the farthest away points (this would be \emph{complete distance}), or maybe the average distance between any two set of points in one cluster and another etc. The computational complexity will go up, but it might provide better results, as for example, for a cluster of $3$ points, their mean $(x + y + z)/3$ is different the mean of the mean of the first and the third $((x + y) / 2 + z) / 2$.

[dendograms with different setups]

Hierarchical clustering gives an ideea of the relationships between variables/observations. This however, might be unstable (change a few points => maybe massive differences, missing values might throw things off => distance function should take this into consideration), dependent on distance and merging strategy (so we need to choose them according to our problem), changes in scale of variables affect the resulting dendogram, not obvious how to choose the cut (this is a problem with many techniques however). It is deterministic however. Useful for exploration of data (in exploratory questions). 

Very pretty dendrograms: http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79

One form of clustering is hierarchical clustering. Hierarchical clustering starts with the our sample. It tries to build a dendogram for the data: a hierarchical structure which describes nearness relationships. It first starts by computing the distances from each observation to each other observation. The two closest observations are selected to form the first cluster. The two are removed from the sample and their mean, the cluster representative is introduced. We therefore have N-1 observations in the sample. The process continues N-2 times, joining the two closest observations into a cluster. A binary tree structure becomes apparent from this process. Computing the initial distances has cost $\Theta(N*(N-1)/2 * T_{\text{sim}})$ where $T_\text{sim}$ is the time needed to compute a similarity metric, and $T_{\text{sim}} = f(d) = \Theta(d)$ in most cases (Euclidean, Pearson etc.). Selecting the greatest similarity can be done in $\Theta(N*(N-1)/2)$ operations. Joining the two most similar observations takes $\Theta(d)$ time. All similarities to the new observation must be recomputed, and this takes $\Theta(N-i)$ time (at each iteration, the "active surface" of observations needeing to be included in a cluster drops by 1 (remove two observations and include one)). Thus, the total time is $T(N) = \Theta(N*(N-1)/2 * T_{\text{sim}}) + \Theta(sum[i=1:N-1](N-i)) = \Theta(N^2 * T_{\text{sim}}) + \Theta(N^2) = \Theta(N^2 * T_{\text{sim}})$.

The dendrogram can also be drawn. Inter observation distances can be used as guides for positioning the graph nodes.

As with the filtering problem, if you transpose the data matrix, you get interesting results. In this case, it is the way in which different features tend to appear together. Features close in the dendogram tend to be correlated, or to appear together, while features far apart, tend to appear alternatively. Useful for example, in text mining, to see which words appear together, or in basket analysis to see which products are bought together.

We can attach a bit string to every clustered members. The bitstring is generated starting from the root with an empty bitstring, going until the desired vector, and then adding a $0$ bit if we do a left-branch and a $1$ bit if we do a right-branch. Actual clusters can then be built as sets of vectors with the same prefix, instead of just by distance.

\subsubsection{K-Means Clustering}

KMeans clustering is a form of clustering which works best for continuous data. It assumes the existance of the distance function $d$. It starts with the sample and a fixed number of clusters (given in advance). Each cluster is defined by a centroid, and observations are assigned to the cluster whose centroid they are closest to. The initial centroids can be selected randomly from $\mathbb{X}$, or as a random sample from the input sample or by using other, specific knowledge. Every point is then assigned to one initial cluster. Then, for a number of iterations, supposing we have our clusters and a number of points assigned to them. We recompute the cluster centroids by averaging the observations assigned to the clusters. We then reassign points to the new centroids. This process continues until the computational budget has ran out, or a ``local minima'' is encountered and no more changes from one cluster to another take place. Since, in general, the initial clusters have some sort of randomness in them, running the algorithm several times produces different results, therefore it is not deterministic. Multiple runs could be performed, and then an average taken by looking at clusters from different runs with many points in common. If, during the run, a cluster runs out of points, it can be droped, or reinitialized with the point which is farthest away from its assigned cluster.

The number of clusters is the hard part. It can be picked by intuition, cross-validation/information theory (minimize a measure of reconstruction subject to complexity constraints - pick the corner of a $J$ curve). The algorithm is not deterministic however, and multiple runs of it are sometimes used and the results averaged.

Another form of clustering is K-Means clustering. This method also starts with the sample and K - a nonnegative integer, which represents the number of clusters we wish to build. Then, K "centroids" are initialized. These can be spread randomly through the domain or, better yet, selected from the sample. Then, all the observations are assigned to a centroid, based on their similarity to them - that is, an observation is assigned to the centroid most "similar" to it. The centroids are recalculated as the means of the observations assigned to them. The process then continues in this fashion of assignment and centroid recalculation until no observation is changed from a centroid to another, that is, until the system has converged. If at any one time, a centroid has no observation attached, then that centroid can be dropped (the algorithm can continue, as, in the most catastrophic case, at least one centroid must remain) or reinitialized to the observation most far from its assigned centroid. The returned value is the list of centroids and associated observations, which form the clusters we are looking for.

\subsubsection{Brown Clustering}

Used in NLP tasks. Works on a corpus of words (or any vectors, in principle). Can produce clusters as well as hierarchical clustering. Clusters ``words'' and is useful in tasks like Named Entity Recognition. Intuition: similar words appear in similar contexts. More precisely: similar words have similar distributions of words to their immediate left and right.

For the clustering approach, we have the set $\mathcal{V}$ of all the words in the corpus and we wish to build a partition function $C \colon \mathcal{V} \rightarrow \hcrange{1}{k}$. We have two types of parameters $e$ and $q$, which are similar to HMM parameters, where $e(w \given c)$ is the probability of seeing word $e$ given cluster $c$ and $q(c_1 \given c_2)$ is the probability of seeing class $c_1$ if the previous class was $c_2$. The probability of our corpus is $p(w_1 w_2 \cdots w_t) = \prod_{i=1}^n p(w_i \given p_{w_{i-1}}) = \prod_{i=1}^n e(w_i \given C(w_i)) q(C(w_i) \given C(w_{i-1}))$. $C$, $e$ and $q$ must be ``learned''. We can look, more convinently, of course, at the logarithm of $p$. The class for the start word $\star$ is $0$, by convention.

We have our log-likelihood, for which we'll choose $C$, $e$ and $q$ such that $p$ is maximized. If we have a $C$, $e$ and $p$ can be computed using the ML estimates. The log-likelihood is:
\begin{align*}
\text{Qual}(C) & = \sum_{i=1}^n \log e(w_i \given C(w_i)) q(C(w_i) \given C(w_{i-1})) \\
& \sum_{c=1}^k \sum_{c'=1}^k p(c,c') \log p(c,c') / (p(c)p(c')) + G
\end{align*}
Here $G$ is a constant and $p(c,c') = n(c,c') / \sum_{c,c'} n(c,c')$ and $p(c) = n(c) / \sum_c n(c)$ where $n(c)$ is the number of times class $c$ occurs in the corpus, $n(c,c')$ is the number of times $c'$ is seen following $c$, under the function $C$, and these are ML estimates of the associated probabilities. The full derivation of the expression is not shown however. It is the mutual information (of what?).

How to optimize this? We start with $\abs{V}$ clusters. Each word has its own cluster. Our aim is to find $k$ final clusters. We run $\abs{V} - k$ merge steps. At each step we can consider all pair-wise merges. At each step we pick two cluster $c_i$ and $c_j$ and merge them into a single cluster as to maximize the clustering quality for $C'$. We consider the new merged words as a unit. The computation of $C$ does not change, however. The naive cost is $O(\abs{V}^5)$. An improved algorithm gives $O(\abs{V}^3)$, but it is still to bad. We can produce a full hierarchy by the same method on the last $k$ clusters.

A second algorithm is as follows. Take the top $m$ most frequent words, put each into its own cluster, $c_1, \dots, c_m$. This is the ``seed''. Then, for the rest of $\abs{V} - m$ words we have left, create a new cluster, $c_{m+1}$ for the $i^{\text{th}}$ most frequent word. We now have $m+1$ clusters. Then, choose two clusters from $c_1, \dots, c_{m+1}$ to be merged, in the same manner as before. We're back to $m$ clusters. After this, perform $m-1$ final merges, to create a full hierarchy. The running time is $O(\abs{V}m^2 + n)$ where $n$ is corpus length.

define {K-Means} {
  [standard discussion here]
  How to choose K:
    Eyball test (does it work?)
    "Elbow method" - plot cost vs K.
    Add another term in objective to penalize large K (AIC/BIC)
    call(BayesianMagick)
  VQ via k-Means - use mean to describe points in the cluster.
  Cannot generate non-convex clusters - we have a Voronoi tesselation with convex "polygons".
}

KMeans: An algorithm for clustering. We have $k$ clusters, defined by centers $m_1,\dots,m_k$. Each observation is a member of a single cluster. We have $R \in \mathbb{R}^{\hctimes{N}{k}}$ where $R_{ij} = 1$ if observation $i$ is assigned to cluster $j$ and $0$ otherwise, with $\sum_{j=1}^k R_{ij} = 1$. We wish to build $R$ and $m$ such that $J[R,m] = 1/2 \sum_{i=1}^N \sum_{j=1}^n R_{ij} \norm{x_i - m_j}^2$ is minimized. Hard to do in general. We adopt a two-phase approach. Step $1$: keep $m$ fixed and select $R$ to mimize $J$. We have $R_{ij} = 1$ if $m_j$ is the closest center to $x_i$ and $0$ otherwise (comes out of optimization proof). Step $2$: keep $R$ fixed and select $m$ to minimize $m$. We have $m_j = \sum_{i=1}^N R_{ij} x_i / \sum_{i=1}^N R_{ij}$ (comes out of optimization proof). We alternate these stepes until ``convergence'': no point is reassigned to another cluster. Intuitively, at each iteration, we first assign observations to their nearest centroids, then we recompute centroids as means of the assigned points. We have to select initial centroids: random points, random observations, farthest observations. We have to deal with clusters with no observations: drop, reinitialize, error. Works for other metrics as well.

define {Gaussian Mixture Models} {
  [standard discussion]
  Parameter is K
  Expectation Maximization for GMMs:
    Initialize mean and covariance
    E-Step: find the probability that the point $i$ belongs to cluster $c$ $P(x_i \given x_i \in C_j)$.
    M-Step: for each cluster computer mixture weight, mean and variance for new model.
  Soft version of K-Means. K-Means is a GMM with spherical variance and it taken to zero.
}

define {Hierarchical Agglomerative Clustering} {
  [standard discussion here]
  [maybe use Approximate NN to quickly find neighbours until problem becomes OK]
  Need to define a distance.
  Linkage criterion: how to define the distance between clusters of points:
    min distance between two clusters
    max distance between two clusters
  Cluster quality:
    The dunn index
    Does the clustering help in the downstream task.
}

- Clustering:
  - can perform clustering of observations or of features.
  - Decisions:
    - center/standardize or not?
    - choice of disimmilarity measure.
    - choice of linkage, if used.
    - choice of cut-point for hierarchical or K for K-Means.
    - use several options to find the "nicest" one.
  - not very robust to perturbations - changes in the dataset, insertions or removals produce substantially differnt clusters many times. Maybe do it several times to see how unrobust they are.

- KMeans Clustering:
  - [standard discussion]
  - we have \f{k} clusters which form a partition of the input space. We wish to seek the partition which minimizes the within cluster variance - that is, we have homogenous clusters. This measure is \f{\arg\min_{C_1,\dots,C_k} {\sum_i^k W(C_i)}, where \f{W(C_i) = 1/N_i \sum_{x,y \in C_i} \norm{x - y}} is the within cluster variance and it is the "average" sum of the inter-pair Euclidean distances.
  - we can rewrite \f{W(C_i) = 2 \sum_{x \in C_i} \norm{x - \mu_i}}, which gives more intuition for the algoritm.
  - there are almost \f{k^N} clustering assignments [be more exact] - brute force doesn't cut it.
  - the presented algorithm has local minima, but that's often good enough.
  - because the initial clusters are random, the algorithm must be run several times, and the best solution according to our metric selected.

- Hierarchical Clustering:
  - [standard discussion]
  - this is bottom-up or agglomerative clustering.
  - for any two observations, we can look at the point in the tree where they merge. if they are far apart they will merge high in the tree, otherwise low. this is the only way to draw conclusions about similarity. if two leaf nodes merge, but high in the dendrogram, then they might be quite dissimilar.
  - to create clusters, we perform a "cut" in the dendrogram, which makes clusters out of all roots of branches intersecting the cut.
  - linkage is important here. different clustering depending on differnt linkages.

- For many methods it is useful to perform a standardization of each feature. This fights effects such as different units of measurement and gives a better picture of the data. However, if the features are already in the same unit, then standardization might not be appropriate. Also, if data is some sort of frequency count, standardization might be good, to account for different type of frequencies for different objects.

- Linkage:
  - dissimilarity between groups of observations.
  - complete:
    - maximum distance between two observations from each group.
  - single:
    - minimum distance between two observations from each group.
    - results in "trailing dendrograms" as clusters are extended one-by-one.
  - average:
    - mean distance between two observations from each group.
  - centroid:
    - distance between centroids (means) of each group.
    - can result in inversions in dendrogram  - distance between two clusters might be smaller than any distance in the group, causing the join to be placed lower in the dendrogram than the clusters.
