=== The Most Basic Of Estimates ===

define {Difference Between Means} {
  Statistics is often interested in differences between means. Mean of a subset of a sample (say, the control) minus mean of another subset (the experiment).
  The statistic to compute it is, obviously $M_{12} = M_1 - M_2$ where $M_1$ and $M_2$ are the means from the first population and the second populations, respectively, which can be estimated as in the mean.

  The sampling distribution then has mean $E[M_{12}] = E[M_1] - E[M_2] = M_1 - M_2$ (what we expected). The variance is $V[M_{12}] = \sigma_1^2 / N_! + \sigma_2^2 / N_2$ (cf variance sum rule). For equal variance and sample size we have $SD[M_{12}] = \sqrt{2\sigma^2/n}$. 

  If the population variances are known, then confidence intervals can be computed in the classical sense: $\alpha_{1,2} = (M_1 - M_2) \pm Z_q (\sigma_{1-2})$. When the two variances are now known, then we have issues. In order to build a confidence interval, certain assumptions are needed: the two populations have the same variance, they are normally distributed and each value is sampled independently. While one can kinda break the first two assumptions, the third is crucial.
  If the two samples have the same number of elements: estimate $s_1$ and $s_2$. Then, compute the average of these two as $(s_1 + s_2) / 2$ and consider this the better estimate of the population mean. Then, the estimate of the population difference between means variance is $s_{1-2} = \sqrt{2s^2 / N}$. We then have $\alpha_{1,2} = (M_1 - M_2) \pm t_q s_{1-2}$, where $t_q$ is the value necessary to make the symmetrical interval around $0$ in the Student t distribution with $n - 1 + n - 1 = 2n - 2$ degrees of freedom contain $q$ of the probability mass. If the two samples have different number of elements, we compute $SSE = [\sum(x^1 - m_1)^2 + \sum(x^2 - m_2)^2] / (n_1 + n_2 - 2)$ as an estimate of the populations variance. Then we have $s_{1-2} = \sqrt{2SSE / n_h(n_1,n_2)}$.
}

define {Proportion} {
  Statistics is often interested in the proportion an event happens relative to when it does not happen. That is, the "probability" of that event happening.
  The statistic is then $p_E = \abs{E occurs} / \abs{D}$. The estimator is simply $p_E = \sum_{i=1}^N (x_i is E) / N$. It is a sort of mean, therefore the properties will be somewhat similar to that of the mean. $x_i is E$ can only take value $0$ or $1$ though.

  The sampling distribution is binomial $1/n Bin[p,n]$. The mean of this distribution is $1/n E[Bin[p,nn]] = 1/n pn = p$ - what we wanted and $SD[1/n Bin[p,n]] = p(1-p)/\sqrt{n}$. This is approximatively normal for large $N$ and $p$ not close to $0$ or $1$ (cannonical assumptions for approximating a binomial by a normal;).

  A confidence interval can be obtained as well for this, but the normal approximation is used for it, and we get a $t$ type interal as well, to which we add $\pm 0.5/N$ because of the approximation to the binomial (and we thus get wider intervals).
}

define {Skew For Proportions} {
  When the number of $0$s or $1$s is very small relative to the sample size we have a very skewed Bernoulli distribution.
  Minimum counts should be $10$, but it depends on the sample size ofcourse. Otherwise specified $np \le 10$ and $n(1-p) \le 10$. The success-failure condition.
  For hypothesis tests, we use the null value to verif the success-failure condition.
}

define {Finding A Sample Size For A Certain Margin Of Error For Proportions} {
  Same as for ref{Finding A Sample Size For A Certain Margin Of Error For Means}, but if we don't have a good approx, we can use $p = 0.5$, since this provides the largest margins in confidence interval computations, so, it will need the largest sample to attain a certain level (which will make all other choices better).
}

define {The Pooled Estimate} {
  When doing tests of the form $H_0: p_1 = p_2$, we can use the pooled estimate $\hat{p} = n_1 / (n_1 + n_2) \hat{p}_1 + n_2 / (n_1 + n_2) \hat{p}_2$ for SD/SError computations and in any confidence interval and hypothesis tests, instead of common knowledge or $0.5$.
}

define {Mean-To-Variance Ratio} {
  A statistic of the form $\mu / \sigma$, where $\mu$ and $\sigma$ are computed using the typical processes.
  The distribution of $\mu/\sigma$ with $\sigma$ known follows the Normal distribution of the mean. When $\sigma$ is not known and has to be estimated, then the distribution is Student t with $df = N-1$ degrees of freedom. It makes sense, when we compute one statistic, it can have a larger value because we happened to be larger or smaller, or because the computed $s$ was smaller. Thus, the probability of actually being above the interval would be over-estimated if we just used the Normal distribution. We needs to use the Student t one which has fatter tails, larger variance and less probability mass in the interal we care for.
  Used to ask questions like, what is the probability that a mean computed from a sample of size $N$ is $l s$ standard deviations from the population mean. Here $s$ is computed from the sample. We are interested in $P(\abs{m / s} \leq l)$. 
}

=== A Very Simple Model ===

define {A Very Simple Model} {
  Many statistical situations deal with means and variances. The model for this situations might be $Y = M + \epsilon$ where $M$ is a fixed but unknown mean and $\epsilon$ is a zero-centered, usually normal noise term.
}

