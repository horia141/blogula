=== Nearest Neighbor Methods ===

\subsubsection{kNN}

k Nearest Neighbours (kNN) is a simple method for classification or regression. It looks at the $k$ nearest neighbours of an observation $x$ and returns the mode of the distribution of outcomes, or the mean of the outcomes, for numerical data. The problem is then the choice of $k$. Using too small a value will leave the estimator vulnerable to the noise in the data, while using a greater vale will smooth out the actual information and patterns from the data. Similarity between two observations can be measured using any one of the metrics introduces in the first sections, about collaborative filtering.

The algoritm is computationally expensive. Adding new training observations is done easily, and this is a major advantage of this algorithm.

To compensate for the problem of considering results for observations that might be far away from the target, we need to weight each result by a value dependent on the distance, similar to how we did for user-based collaborative filtering. Three weighting functions are commonly used: the inverse, the substraction and the gaussian. Each takes a distance as a parameter. The inverse is of the form $1/(d+\epsilon)$ where $\epsilon$ is a regularizzation parameter added so that really close points don't score through the roof. The substraction is $max(0,c-d)$ where $c$ is the maximum allowed distance. A problem with this is that it doesn't make any predictions for $d >= c$ as the weights are 0. Observations not in the vecinity of other observations will not get any result. Finally, the gaussian is the well known gaussian. The parameters $\epsilon,c \text{ and } \sigma$ have to be chosen - by cross-validation maybe? Weighting has to normalize.

kNN suffers from the curse of dimensionality and, as all distance based methods do, from difference in scales between variables. To correct for the former you must switch algorithms, but to correct for the former, there are several approaches. One is to scale each feature by a known-value. This might be determined empirically. This, however is not a good practice, in general. Enter optimization ... we have a cost function: the classification score or mse for regression (or cross-validation score, whatever suites your fancy) and $d$ variables,: the scales for each value. We can employ any one of the appropriate algorithms in Optimization to get good results. Using this, you also see which variables are important, and which are not. There are several ways to find this out, in practice, but this is a general method and works well when computation time is not an issue.

Using kNN you can estimate the probability density function of the output conditioned on the input $P(y|x)$. The way to do this is to first compute the $k$ nearest neighbours of the observation and then do a histogram or a weighted histogram of the prices we see. A larger $k$ is useful for this. Any density estimation method can work here though, and density estimation by gaussian addition is pretty nifty. The author built the CDF and built every interval of the form $[0,i \delta]$, finding the probability of an item having a price in that range (obviously different for outcomes which can be negative), and stopping when said probability was $1$. The probability was computed as the sum of the weights of items in that price range over the total sum of weights.

kNN has several disadvantages: making predictions is very computationally intensive because the distance to every point has to be computed. Also, for high-dimensional data, distance loses its meaning, and it can be difficult to determine the appropriate weights or wether some variables should be eliminated. Optimization can help, but it's hard to perform computationally.

There are some advantages, though. First of all, it's dead simple to understand. Also, it's easy to interpret results and you can get an insight into the data by seeing which other obervations contribute to a result. Also, by weighting you can get an intuition of what variables are important. Finally, for the numeric example, the distribution over outputs is useful and can yield interesting insights into the data (if unimodal then the model is OK, we just have noise (more or less), if multimodla then we might be hidden variables).

Nearest Neighbour: An algorithm for binary/multiclass classification and regression. We assume the space $\mathbb{X}$ has a distance $d$. We need to store the whole training set. We have a paramter $k$. For a new point $x$, we classify it as the class of its closest $k$ neighbours, or the one which appears the most. For regression, we use the mean of the $k$ neighbours. We could weight values by a distance dependent weight. We need to store the whole trianing set $\Theta(dN)$. Training complexity is $\Theta(1)$. Usage complexity is $\Theta(T_d N + N\log{N})$. $k$ selected by cross-validation. Distance need not be symmetric (like in string edit distance). Estimates can be very noisy when the data is very noisy for small $k$. Use Random Projections to speed up from $d$ to $k$ in a controllable manner. Use KD-Trees for $\Theta(\log{N})$ queries. Use Locality Sensitive Hashing to reduce the set of similar points. In Euclidean space compute fast distances.

- k-NN:
  - Classical discussion.
  - Choosing \f{k}.
  - It gets close to the optimal/Bayes decision boundry.

define {Nearest Neighbour} {
  [typical discussion here]
  The effective number of parameters is $N/k$ (consider clusters of $k$ points as an extreme case, where we would predict their mean), which is much higher than $d$.
  Choosing $k$ is difficult than in other situations, and actually choosing the best one is done through cross-validation.
  For regression with the squared loss, this is a direct estimate of the regression function $f^\star(x) = E_{Y \given X} [Y \given X = y]$, given as $\hat{f}^\star(x) = Avg(y_i \given (x_i,y_i) \in N_k(x))$. Approximating both the mean by an average of a sample and the condition on a single point to a region of the space near the point.
  For large training sample size $N$ , the points in the neighborhood are likely to be close to $x$, and as $k$ gets large the average will get more stable. Under mild regularity conditions on $P(X,Y)$ and if as $N,k \rightarrow +\infty$ we have $k/N \rightarrow 0$ (which usually happens since $k$ is fixed), then $\hat{f}^\star \rightarrow f^\star$.
  Often, we don't have large samples, and the asympotics don't get to kick in. Furthermore, the covergence rate is dependent on dimension. Higher $d$ cause very slow convergene - the curse of dimensonality strikes again.
  For classification with the $0-1$ loss, this is a direct estimate of the Bayes classifier $f^\star(x) = \arg\max_c P(x \given X = x)$, given as $\hat{f}^\star(x) = Majority(c_i \given (x_i, c_i) \in N_k(x))$. Approximating both the mean by an average of a sample and the condition on a single point to a region of the space near the point.
  The model is not biased, but unstable.
}

* Nearest neighbour classifier/regression: a very simple (geometric, local, nonparamteric) model for classification/regression. It estimates the result as the (possibly inverse distance weighted) majority vote/average of the $k$ closest points to a given test point.
  Assumes feature space is a metric space.
  Has translation, rotation (rotating whole data does not affect neighbourhood relationships), orthogonal linear transformation invariance (such transforms don't alter the norm of vectors). No scaling/variance invariance (it affects the relative difference between points to have an axis $5$ times larger and one $10$ times smaller, for example), and, by extension, not invertible linear transformation.