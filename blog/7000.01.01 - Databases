The most common format of database is the relational one, which views data as organized into relations, which consist of a relation name, a set of attributes (called the schema or header) and a set of tuples (organized as rows of a table), with one element for each attribute. Frequently, each attribute also has a type, which restricts the elements we can have corresponding to it. Example: Links(From,To) - the table of webpage links in a webcrawler.

Operations on a relation, \f{R} and \f{S}:
* \def{Selection} Given a predicate \f{C}, \f{\sigma[C](R)} is the relation with the same schema as \f{R} and with only those tuples of \f{R} which satisfy \f{C}.
* \def{Projection} Given a subset of attributes \f{X}, \f{\pi[X](R)} is the relation with schema \f{X} and with all the tuples of \f{R}, but with components not in \f{X} removed (and duplicates removed, of course).
* \def{Set Operations} If \f{R} and \f{S} have the same schema we can do the classical set operations of union, intersection and difference on them.
* \def{Natural Join} If \f{R} and \f{S} have at least one attribute in common, \f{R \Join S} is the relation with schema \f{S_R \cup S_S}, and whose tuples are formed by considering the carthesian product of \f{R} and \f{S}, and including only those pairs of tuples which have equal elements for the common attributes, and concatenated to fit the new schema.
* \def{Grouping and Aggregation} Given two subset of attributes \f{X_1} and \f{X_2} and a summary procedure \f{\phi}, \f{\gamma[X_1,\phi(X_2)](R)} is a relation with schema \f{X_1 \cup X_2}, with tuples obtained by first grouping all the tuples of \f{R}, after distinct values from \f{X_1}, and applying \f{\phi} to the values for \f{X_2}. We get one tuple for each distinct value from \f{X_1}.

Relational data-model: organization of data into collections of two-dimensional tables called "relations". A relation in this sense is a matrix like / tabular structure, with the first row consisting "attributes" (column headers), while the next rows contain the actual values. A relation is thus a set of tuples and a tuple describing each column. Order of rows is thus not important. Order of columns is, and we must permute whole columns and the attributes in order to keep things consistent. In contrast to set-theoretic relations, but we rarely have to do it. Each row is a tuple and represents a basic fact. No two rows of the same table may have identical values in all columns. The row describing each column is called the scheme of the relation. One could say some columns for the domain of the relation, while other columns are its range (for a more set-theoretic approach).

A collection of relations is called a database. Set set of schemes for the various relations of the database is called the scheme/schema of the database.

Operations on relations:
* \f{insert(t, R)}: insert tuple \f{t} into relation \f{R} - with set semantics (if it is already present we don't add it).
* \f{delete(X, R)}: delete tuples described by \f{X} from \f{R} - \f{X} is a tuple with components which can either be a value or a \f{\star} (any value in a tested tuple will do).
* \f{lookup(X, R)}: retrieve all tuples described by \f{X} from \f{R} - \f{X} is a tuple with components which can either be a value or a \f{\star} (any value in a tested tuple will do).

Keys: we select some columns from a relation and we say that they form the domain of the relation, and, furthermore, we want to have a function - that is - each domain value is unique. These columns are said to form the key of that relation. Thus we can view relations as a set representation of a function (link to function as data model). Insert must look only at the key for uniqueness. There can be \f{2^k} sets of keys - not all are as good though. Keys are chosen in the design phase. Selecting a restricted set of attributes as keys is a further constraint on the relation. A relation without keys doesn't have duplicates, so it can have \f{\Prod_{i=1}^l \abs{A_i}} distinct elements. A relation with \f{k} keys doesn't even have duplicates in the subset of \f{k} key attributes, so it can have \f{\Prod_{i=1}^k \abs{A_i}} distinct elements. It shrinks by a factor of \f{\Prod_{i=k+1}^l \abs{A_i}} - huge.

Primary Index: BSTs, characteristic vectors, hash tables, lists (not really useful). A set of attributes is selected as the domain attributes, while the rest are the range attributes. The domain attributes are used in computing hashes and bucketing or for comparison. They might or might not be equal to the keys (usually they are when the set of keys is small - when the set is large, we have to look at what sort of queries we want to perform. Also, this increases the number of elementes hashed into the same bucket - tradeoffs-tradeoffs). Domain choice has the greatest effect on the speed of queries. It is common to use a hash table based on domain values as the "index" (a data structure that helps find tuples, given values for some components (ideally for the all the domain components)). This is the primary index structure for the relation. Primary means the location of the tuples is determined by the structure. Insert works by retrieving the domain value from the specified tuple, hashing it, seeing if the tuple is not already in the bucket and inserting it only if it is. Typical hash table. Delete works by retrieving the domain value from the specification, hashing it, and walking down the list of tuples removing all that match. Lookup works the same, only it builds a list of matching tuples. In all cases, average complexity is \f{O(L)} where \f{L} is the average length of such a list. However, if the domain value is not fully specified, we must search the whole dataset.

Secondary Index: structure used to find tuples in a relation, given values for a certain attribute or attributes, but is not used to position the tuples within the overall structure. The domain for this index is a set of attributes, while the range is a list of all the buckets in the primary index where we find tuples with the same attribute. We can use hash tables, BSTs, characteristic vectors, lists (not really useful). When we insert or delete tuples from a relation, the secondary index must be changed as well. This takes some time (searching in both the primary and secondary + deletion). If we have a fixed and known set of secondary indices, we could store pointers in a cell from the primary index back to each secondary index, so deletion is just a matter of deleting a node for which we already have a handle on (which might get nasty in BSTs, but is not that hard in hashtables). Does not help with insertion though.. In any case, it makes sense to add a secondary index only on those attributes that are likely to be lookedup many times. With secondary indexes we keep more views into the data, but a single repository of the range values in the primary index.

We can use "navigation" between relations to find out interesting information. Given an attribute \f{A_i=a} from one relation and \f{B_j=b} from another relation, whose schemas are not disjoint, we can first search for all the tuples with \f{A_i=a} in the first relation, then, for a given found tuple, scan for all tuples with common attributes in the second relation and with \f{B_i=b} as well. Assume there are \f{k} found tuples in the first relation. Then, the complexity is \f{O(n + km)}. If there are indexes (an index for \f{A_i} and indices for \f{B_j \cup \text{common attributes}}), the complexity is \f{O(k)}. This can be naturally extended to many relations. Without indices, this is \f{O(s_1 + f_1 s_2 + f_1 f_2 s_3 + \dots)} where \f{s_i} is the size of the \f{i^{\text{th}}} relation and \f{f_i} is the number of elements found after theat relation (on average). With all proper indices, this is \f{O(1 + f_1 + f_1 f_2 + \dots)}.

= Operations on relations =

== Union, Intersection, Difference ==

Have the same meaning as for sets. Restriction that schemas must be the same. These are typical set operations which have their known bounds depending on representations. If we have an index on the key of a relation, then we can insert each element and disregard duplicates which is \f{O(m)} with a hash-table index or \f{O(m\log{m})} with a balnced tree index etc.

==  Query operations on relations ==

=== Selection \f{\sigma_P(R)} ===

Given a predicate \f{P} and a relation \f{R}, return a new relation with the same schema as \f{R}, but containing only those tuples which satisfy the predicate. Implementation-wise, if the predicate is very general, we have to walk each tuple and test it and add it to a new structure if it passes, with time \f{O(nT_P)} (or \f{O(n\log{n}T_P)}) for example. If the test is an equality and we have an index, we can use the index. If it is an and of equalities, we can use one of the indexes and filter the returned elements. It is is an or, we perform all lookups and then filter the result. Range queries benefit from tree structure (or ordered structures in general). More complex queries are treated sperately, but it defenitely pays off to have specialized code depending on functions.

=== Projection \f{\pi_A(R)} ===

Given a subsequence (even disregarding order) of the sechema of \f{R} and a relation \f{R}, return a new relation with schema \f{A} and tuples formed by taking a tuple from \f{R} and dropping all those fields not in \f{A}, and rearranging them in the order indicated by \f{A}. Duplicate elements are eliminated. Implementatio-wise, either sort all tuples and remove tuples duplicates with our attributes, or create a new indexing structure and insert all tuples (stripped of extra values) and return it. Time is \f{O(n\log{n})} or \f{O(n)} (or \f{O(n\log{n})}) in the second case. No need for duplicate detection if set of attributes contain the key attributes.

=== join \f{R_1 \dbjoin R_2} with disjoint schemas ===

Given two relations \f{R_1} and \f{R_2} with disjoint schemas, return a new relation with schema the concatenation of the schemas of \f{R_1} and \f{R_2} and tuples the Carthesian product of the two relations.

=== join \f{R_1 \dbjoin R_2} with nondisjoint schemas (natural join) ===

Given two relations \f{R_1} and \f{R_2} with nondisjoint schemas, return a new relation with schema the concatenation of the schemas of \f{R_1} and \f{R_2} and dropping of common elements from the second part and tuples formed as a form of restricted Carthesian product of the two relations: for each tuple in the first relation, find all tuples in the second relation which have the same values in the common attributes and form concatenated versions of these for the new relation.

=== join \f{R_1 \dbjoin_{A_1^i=A_2^i} R_2} with nondisjoint schemas ===

And a set of selected attributes from the common attributes: the same as the natural join, only for each tuple in the first relation, we find all tuples in the second relation which have the same value in the specified attributes and form concatenated versions of these for the new relation.

=== join \f{R_1 \dbjoin_{P(As,Bs)} R_2} with some predicate on the tuples ===

The most general kind of join - we do a join like the previous one, but use a predicate to determine if we should include a tuple, instead of equality.

=== Naive join ===

If there are no indices, and we have a general join, joins can be made by nested loops for time \f{O(nm)}. Sort join. If we have no indices, but we have a natural join, we put all elements together (but those coming from the first have a \f{0} flag while from the second a \f{1} flag) and sort them by the join attributes and the flag (\f{(n+m)\log{n+m}}) then walk the new list and carthesian product groups with equal \f{b}, which is \f{O(nq+m)} if \f{q} is the average number of elements per distinct value of common attribute \f{b}. In the worst case \f{nq} is actually \f{nm} - everyone pairs with everyone else. Index join. If we have indices on at least one, we walk the one without the index and retrieve from the one with the index. Depending on the structure it can be \f{O(nq)} or \f{O(n\log{m}q)} where \f{q} is the average number of elements per distinct value of \f{b}.

== Query Optimization ==

Relation operators form an algebra. Thus we have expression trees and we can optimize expressions: huge speed-ups occur because of these. Union, Intersection and Difference have all the rules from set theory. Join is commutative if we consider relations equal even if their columns are permuted versions of each other. Join is non-associative in general, unless we specify very lax predicates. Selection usually reduces large relations to small ones. This is efficient. We wish to perform transformations that push selections down in the expression tree, so they happen early. If we have indexes - super speed. Focus on selection with and of attribute equality. Joins on the other hand you want to have as top-level as possible, so they have few elements to work with. Some algebraic rules:
* Selection and set operations: \f{\sigma_P(R \cup S) = \sigma_P(R) \cup \sigma_P(S)} and \f{\sigma_P(R \cap S) = \sigma_P(R) \cap \sigma_P(S)} and \f{\sigma_P(R \setminus S) = \sigma_P(R) \setminus \sigma_P(S)}. Schemas are the same.
* Selection pushing: we have \f{\sigma_{P_R}(R \dbjoin S) = \sigma_{P_R}(R) \dbjoin S} where \f{P_S} is a predicate only for attributes in \f{R} - after the join we'll be left only with those tuples which satisfy \f{P_R} so why not focus on those from the start. Ditto for \f{\sigma_{P_S}(R \dbjoin S) = R \dbjoin \sigma_{P_S}(S)} where \f{P_S} is a predicate only for attributes in \f{R}
* Selection splitting: we have \f{\sigma_{P_1 \land P_2} (R) = \sigma_{P_1}(\sigma_{P_2} (R))}. Computationally we only have a constant factor improvement (good enough if \f{P_1} is complex). But this allows to split an and-predicate \f{P} into \f{P_R} and \f{P_S} and perform selection pushing \f{\sigma_{P}(R \dbjoin S) = \sigma_{P_R \land P_S}(R \dbjoin S) = \sigma_{P_R}(\sigma_{P_S} (R \dbjoin S)) = \sigma_{P_R}(R \dbjoin \sigma_{P_S}(S)) = \sigma_{P_R}(R) \dbjoin \sigma_{P_S}(S)}.
* Selection commutativity: \f{\sigma_{P_1}(\sigma_{P_2}(R)) = \sigma_{P_2}(\sigma_{P_1}(R))}.
* Projection and set operations: \f{\pi_A (R \cup S) = \pi_A(R) \cup \pi_A(S)}. Schemas are the same. But this does not hold for intersection and difference, because for intersection we may end up with more elements (removing attributes removes possibilties of mismatches), while for difference we end up with less elements (more attributes in common mean less attributes just for \f{R}).
* \f{\pi_L(R) = R} (again considering equality without regard to permutations).
* Projection pushing \f{\pi_L(R \dbjoin_{A=B} S) = \pi_L(\pi_M(R) \dbjoin \pi_N(S))} where \f{M = L\cup A} and \f{N = L \cup B}. We remove any extra attributes from \f{S} to whatever \f{L} says, but we keep the join attributes. We do the same for \f{S}.
* Ignore projection: if sometimes we have a projection of a base relation (not an expression), and we, up the expression tree  have another projection, we might as well do the combined projections up the tree, on the possibly smaller expression. This is a sort of pushing up of a projection.
