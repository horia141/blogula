Pattern: a set of objects with some recognizable property. Problems: definition and recognition. A basic kind of pattern is a set of strings.

= Finita Automata / Regular Expressions / Regular Languages =

Finite automaton / automaton: a computing process which can recognize certain string patterns. It has a finite number of states and between each state there are labeled transitions. One can associate a directed graph which such an automaton. Given a string, it starts in a unique start-state and at the first symbol in the string. Then, each computational step consists of looking at the current symbol in the string and advancing to the next symbol and to a state determined by the current symbol by following the arc labeled with the same symbol out from the current state. If no label can be found, we say to be in a rejecting/error state and stop. If we have reached one of a group of accepting states, the computation stops with a success. For succintness, one can label arcs with sets of characters. Not all of the input might be processed. Another formalism associates outputs with reaching certain states.

Simulate an automaton: given an input \f{a_1\dots a_k}, build the path through the graph of the automaton, determined by following arcs from the start to the accept/reject state, appropriately labeled. The input is the label of the path.

Deterministic automata: for any state \f{s} and any input \f{x}, there is at most one transition out of state \f{s} whose label includes \f{x}. At each point in the computation, only one action can be taken (go to next state or die). A program can easily be generated from such an automaton (maintain a state variable, read input one at a time, and on each input, switch according to the out arcs from the current state, add extra code for accept/reject or output). An input determines a single/unique path. We accept if this path ends in an accept state or reject otherwise.

Nondeterministic automata: there exists a state \f{\overline{s}}, where the sets of symbols which label the outgoing arcs are not disjoint. At this point in the computation, it is not clear which action should be taken (therefore we must take both or guess). Nondeterministic automata include deterministic ones as subcases. An input can determine many paths. We accept if any of these paths ends in an accept state, or reject otherwise.

Two automata are equivalent if they accept the same input strings. If \f{a_1\dots a_k} is a string of symbols, then there is a path labeled \f{a_1\dots a_k} from the start state of one to an accepting state iff there is a path labeled \f{a_1\dots a_k} from the start state of the other to an accepting state.

Turning a nondeterministic automaton to a deterministic one: we use the subset construction. Concept: the na starts in the start state. then it is simultaneously in all the states reachable from the start state and the first symbol. then in all the states reachable from any state in the current group of states and the second symbol etc. At any one time though, it can be in at most all of its states. We can build a da which has states labeld by subsets of states of the na. The start state is the same as the start state of the na. Then, if \f{S = \{s_1,\dots,s_k\}} is a state of the da, we can build all transitions by considering every symbol from the alphabet, say \f{x}, and adding a transition from \f{S} to a state \f{Q} which contains all states which are reachable from an state in \f{S} (\f{q_j \in Q \Leftrightarrow \exists s_i \in S \colon x \in \Pi(s_i, q_j)}). We can thus have at most \f{2^n} (\f{n} is number of states) states in the da. Since each symbol appears only once on a transition out of a state, this is indeed a na. We must prove that the two are equivalent. Prove by induction with \f{S(k)} a sequence of \f{k} symbols ends in the same set of states in the na as contained in the final state of the da. \f{S(0)} we only have the empty sequence \f{\epsilon}. This makes the na stay in the start state. Since the da has the start state consisting of only the start state of the na, we're ok. Assume \f{S(k)} holds. Let \f{a_1a_2\dots a_{k+1}} be a sequence which ends in state \f{S} of the da. The na must be in exactly the states of \f{S} at the end. At symbol \f{k}, the da is in state \f{S'} which is, by \f{S(k)} the set of states in which the na is. By our construction, when we read \f{a_{k+1}} we advance in the na from each state in \f{S'} to zero or more states reachable with \f{a_{k+1}}. But these are all the states of \f{S}, by our construction. On the other hand, there are no extra states for the na to be in after \f{a_{k+1}}, because the na must be in one of the states of \f{S'} (by \f{S(k)}) and all those states lead to \f{S'}. Therefore the statement holds and \f{S(k)} holds for all.

One can add a dead state to an automaton, to which all non-accepting states which don't have some symbols specified transition to. Such an automaton is equivalent to the original one (every accepted string of the first is accepted by the second, because the second will behave identically to the first for accepting inputs (since they end up in accepting states, all inputs up to the accepting state must have made a transition, therefore we didn't follow any transition to the dead state (the input symbol was handled)), and vice-versa, if an input is accepted by the dead state automaton, it ended up in an accepting state (which is the same as the one for the original, and the same path is followed)).

To build an deterministic automaton that accepts the conjugate of a language, swap every accepting state to a non-accepting state and vice-versa. If a string was accepted in the initial automaton, it will end in an accepting state, which is a rejecting state in the new automaton. This works the same whether we use a dead-state or not. For nfas, the result does not hold. For an accepted string with mixed paths, it will still be accepted. Only with pure paths, will it become rejected. The dfa derived will have to be such that if a dfa state is accepting, all elements are accepting. That way, when we swap the states, all dfa states which were accepting become non-accepting and vice-versa. Involves dfa states building a partition of the nfa state space first of all, and this partitions being uniformly accepting/rejecting. Quite a strong command.

When an nfa automaton uses an \f{\epsilon} as a label, it is called an \f{\epsilon}-transition automaton. An \f{\epsilon} arc is always taken, but, in the resulting label path for an input, it is eliminated. To transform an \f{\epsilon} automaton to a normal one, notice that if state \f{s} has \f{\epsilon} arc to a state \f{t}, then, for a certain input which reaches \f{s}, \f{t} will also be reached. The states are therefore equivalent. For each state we can build the \f{\epsilon}-reachability sets (solving the reachability problem on graphs, for the subgraph with only \f{\epsilon} arcs in \f{O(\abs{V}\abs{E})} time (but typically \f{O(\abs{V}^2)} time for automatas of res)). Then we compress into \f{s} all \f{\epsilon}-reachable states. More precisely, in the new automaton there is an arc between state \f{s} and \f{t} with label \f{x} if there is a state \f{k} and a path between \f{s} and \f{k} is labeled by \f{\epsilon} while the arc between \f{k} and \f{t} is labeled by \f{x}. A state is accepting if it was an accepting state in the old automaton or if it can reach an accepting state through \f{\epsilon} transitions. An input of the original automaton will have one associated path to an accepting state. Any non \f{\epsilon} arc in this path will be found in the new automaton. Then, when presented with the input, the automaton will follow this path and get to an accepting state. Similarly, if the new automaton accepts a string, all the arcs will be found in the old automaton, with possible \f{\epsilon} arcs bewteen them (because those were elminated), therefore the old automaton will still get to an accept state. The two automatons are equivalent.

== Regular Expressions ==

Regular expression: an algebraic way to define patterns. Atomic operands: a symbol, the special symbols \f{\epsilon} and \f{\emptyset}, a variable which refers to another regular expression. The values of a re are sets of strings (eg arithmetic is numbers, relational is relations) called the language \f{L(E)}. \f{L(\bold{x}) = \{\bolx{x}\}}, \f{L(\epsilon) = \{\epsilon\}}, \f{L(\emptyset) = \emptyset}. Operators: union (\f{R \given S} is \f{L(R \given S) = L(R) \cup L(S)} and is binary, associative, commutative, \f{\emptyset} is neutral element), concatenation (\f{RS} and \f{L(RS) = \hctimes{L(R)}{L(S)}} and is binary, associative, \f{\epsilon} is neutral element), closure (\f{R^\star} and \f{L(R^\star) = \epsilon \cup R \cup RR \cup \dots} and is unary). Rexps are built from a finite number of occurances of the operators (therefore the explicit need for closures). Priority is closure, concatenation, union.

Given an a word \f{a_1\dots a_k}, the re which describes all words different from it is \f{A \setminus a_1 A* \given a_1 A \setminus a_2 A* \given \dots \given a_1 a_2 \dots a_{k-1} A\setminus a_k A*} - a matching word does not start with \f{a_1} (otherwise it can do as it pleases), or if it starts with \f{a_1}, the second character is not \f{a_2} (otherwise it can do as it pleases) etc. If we are given more than one word we can do \f{A \setminus \{a_1,b_1\} A* \given a_1 A \setminus a_2 A* \given \dots \given a_1 a_2 \dots a_{k-1} A\setminus a_k A* \given b_1 A \setminus b_2 A* \given \dots \given b_1 b_2 \dots b_{k-1} A\setminus b_l A*} - a matching word does not stasrt with either \f{a_1} or \f{a_2} (otherwise it can do as it pleases), or it starts with \f{a_1} it must not match the rest of \f{a}, or if it starts with \f{b_1} it must not match the rest of \f{b}. If \f{a} and \f{b} have the same prefix, some of the cases get joined together.

Extensions to REs: character classes (\f{[abc]} or \f{[a-z]} or \f{[a-zA-Z0-9-]} (dash either first or last to be literal)), line start/end (\f{^} and dollar for line processing), escape bye \f{\\}, wildcard \f{.} (anything but newline), unary optional \f{?} (\f{R?} is \f{\epsilon \given R}), unary reperition (\f{R+} is \f{R R*}).

Two res are equivalent \f{R \equiv S} iff \f{L(R) = L(S)}.

Algebraic laws for manipulating res:
* \f{\emptyset} is identity for union and \f{\epsilon} is identity for concatenation. \f{\emptyset} is an anihilator for concatenation (\f{\empsytset R \equiv R \emptyset \equiv \emptyset} - a string in \f{\emptyset R} would have to be concatenated from a string in \f{\emptyset} and one in \f{R}. since there are no such strings in \f{\emptyset} there are no strings in \f{\emptyset R}).
* \f{R(S \given T) \equiv RS \given RT} and \f{(S\given T) R \equiv SR \given TR} - left and right distributivity of concatenation and union.
* Idempotence of union \f{R \given R \equiv R}.
* Concatenation is not commutative, unlike to how multiplication behaves.
* \f{\emptyset^\star \equiv \emptyset}, \f{RR^\star \equiv R^\star R \equiv R^+} and \f{R^+ \given \epsilon = R^\star}.

Turning a re into an automaton. We start with a re \f{R} and want to build an automaton \f{A} such that \f{L(R) = L(A)}. The following inductive construction is sufficient. We want to build an automaton with one start state, one accept state, no arcs into the start state, no arcs out of the end state, languages accepted are equal. Given a re \f{R}. If \f{R} does not contain any operators, then \f{R} is either the empty string (generate two state automaton, start state \f{1}, accepting state \f{2}, arc \f{1 \rightarrow 2} labeled by \f{\epsilon}, accept only the empty string), the null set (generate two state automaton, start state \f{1}, accepting state \f{2}, no arcs, accept nothing) or a symbol (generate two state automaton, start state \f{1}, accepting state \f{2}, arc \f{1 \rightarrow 2} labeled by \f{\epsilon}, accept only the symbol). Inductive case automatons have the properties we wanted. Assume we can build such automatons for \f{n} operator expressions. Let \f{R} be an \f{n+1} operator expression. If it is \f{R = R_1 \given R_2} then both \f{R_1} and \f{R_2} are res with less than \f{n} operators and they have associated automatons with the properties listed. Build a new automaton with a start state \f{s} and \f{\epsilon} arcs to the start states of the automatons of \f{R_1} and \f{R_2}, and an accept state with \f{\epsilon} in-arcs from the accept states of the automatons of \f{R_1} and \f{R_2}. If \f{x \in L(R)} then it is either in \f{L(R_1)} or in \f{L(R_2)} or both. The automaton will go on both \f{\epsilon} paths, one subatutomaton will accept them, by getting to its accept state, and from there, through an \f{\epsilon} transition, it will get to the final accept state. The start/accept states of the subautomatons lose their qualities. Similarly, if a string is accepted by the new automaton, it is accepted by one of the sub-automatons, therefore it is in one of those languages (or both), since any accept path will pass through the start state and corresponding accept state of one or the other of the automatons (or both). If it is \f{R = R_1 R_2} then we can build the automaton from the two subautomatons (they exist), add states \f{s} (start) and \f{t} (accepting) and an \f{\epsilon} arc from \f{s} to the start state of \f{R_1}, an \f{\epsilon} arc from the accept state of \f{R_1} to start of \f{R_2} and an arc from the accept of \f{R_2} to \f{t}. The start/accept states of the subautomatons lose their qualities. Similar arguments for equivalence hold. If \f{R = R_1^\star}, then we add states \f{s} (start) and \f{t} (accept), and an \f{\epsilon} arc from the accept state of the subautomaton to the start state of the automaton (multiple copies) and an \f{\epsilon} arc from \f{s} to \f{t}. The start/accept states of the subautomaton lose their qualities. \f{\epsilon} will be accepted by the \f{\epsilon} arc from \f{s} to \f{t}. Another string \f{q = a_1a_2\dots a_k} (where \f{a_k} are substrings in \f{L(R_1)}) triggers the desired behaviour. \f{a_i} finds us in the start state of the subatuomaton and leaves us in the accept state for it. When \f{i=k} we take the path to the accept state and we accept. Similarly, we can break a string accepted by the automaton into sublabels on where the end state of the subautomaton was and these belong to \f{R_1}. Thus, the languages are equal for all cases, and we have, by structural induction, that our construct makes the automatons equivalent. The new automaton can be \f{\epsilon}-reduced to a non-deterministic automaton without \f{\epsilon} and then to a deterministic one for implementation.

Turning an automaton into a re. Basic principle: transform automaton into something that allows res as arc labels. Remove one node at a time, until a simple automaton remains, which has a standard solution. Suppose we have a single accept state. Preprocessing: if two states are unconnected, add an arc between them labeled by \f{\emptyset}. The removal process for node \f{u} is as follows. \f{u} has predecessors \f{a_i} (with re \f{R^a_i} on the arc) and successors \f{b_j} (with re \f{R^b_j} on the arc) and a self-loop with re \f{U}. Some of the successors might also be predecessors (and we should represent them separatedly). There are also arcs between each \f{a_i} and \f{b_j} (with re \f{R_{ij}} on the arc). By eliminating node \f{u}, we are left with only the arcs between \f{a_i}s and \f{b_j}s, with updated res. The new re becomes \f{R_{ij} \given R^a_i U^\star R^b_j} (that is, the string accepted between \f{a_i} and \f{b_j} is either the string described by \f{R_{ij}} by going directly, or the string described by going from \f{a_i} to \f{u}, staying there \f{0} or more times through \f{U} and then going to \f{b_j}). Notice that if \f{U = \emptyset} then \f{U^\star = \epsilon}. Also, if \f{R_{ij} = \emptyset} we have only \f{R^a_i U^\star R^b_j} or \f{R^a_i R^b_j} (if \f{U = \emptyset} as well), as a natural consequence. We repeat the process until we are left only with the start state \f{s} and end state \f{t}. The final re is \f{R^\star_{ss} R_{st} (R_{tt} \given R_{ts} R^\star_{ss} R_{st})^\star}. If we have multiple accept states, we do the procedure for each one, ending up with two state automatons, and the result is the union of all these results.

With the subset construction for turning nfas into dfas, and turning res into nfas and nfas into res we have, for a language \f{L} that the following are equivalent:
* There is some deterministic automata that accepts it and only it.
* There is some nondeterministic automata that accepts it and only it.
* There is some regular expression \f{R} such that \f{L(R)} is our language.

= Push Down Automata / Context Free Grammars / Context Free Languages =

Context Free Grammars or grammars: a way to define patterns. Consists of a sequence of rules. Each rule consists of a head and a tail. The head is the name of a syntactic category. The tail is a list of terminals or other syntactic categories. Textually, one uses the metasymbol \f{\rightarrow} to separate the head from the tail and places names of syntactic categories between brackets. Terminals are characters or abstract symbols such as number, identifier etc. \f{\epsilon} is the empty body. Several rules can have the same head, in which case we can interpret (and even rewrite them) usin the \f{\given} notation of res. A common pattern is \f{\category{X} \rightarrow \category{X} Y \given Y} (an \f{X} is either an \f{Y} or an \f{X} followed by an \f{Y} - which expands to one or more \f{X}s) or \f{\category{X} \rightarrow Y^\plus}. Similarly \f{\category{X} \rightarrow \category{X} Y \given \epsilon} is \f{\category{X} \rightarrow Y^\star}. The right-recursive forms are also true (eg \f{\category{X} \rightarrow Y \category{X} \given Y} is \f{\category{X} \rightarrow Y^\star}. \f{Y} is a terminal. One syntactic category is treated as the start category, and its rules is where we start parsing.

For each syntactic category in a grammar we can define a lanugage \f{L(\category{S})}. This is generated as follows (For each production \f{\category{S} \rightarrow T} (where \f{T} is a terminal) we simply add \f{T} to \f{L(\category{S})}. For each rule \f{\category{S} \rightarrow X_1X_2\dots X_n}, we can generate a string by selecting a string from \f{L(\category{X_i})} and concatenating them, in order. Systematically, for this rule we have \f{\hctimes{L(\category{X_1})}{\hctimes{\dots}{L(\category{X_n})}}} (where \f{\hctimes} does concat at the end) as the strings for the rule. \f{L(\category{S})} is the union of all such sets for each rule). The language of a grammar is the lanugage of its start rule. The language is infinite and ennumerable. Another systematic approach is to first start with the empty \f{L(\category{S})}. Then, in each round, we generate new strings by following the rules and generating strings from the other categories (which could be \f{\category{S}}). We can do this for all categories in parallel, and, eventually, every string will be generated (proof by induction). If there are no base rules in recursions, then the generated language is empty. Fun facts about this generation process:
* A new string is generated at step \f{i} only if at step \f{i-1} a new string was generated. If there is no new string at step \f{i-1}, then \f{L_{i-2}(\category{S}) = L_{i-1}(\category{S})}. Then, the generation process at \f{i} will look at \f{L_{i-1} = L_{i-2}} and will generate the same strings as step \f{i-1}, which are not new, by our assumption.
* If we have begun generating only strings of length longer than a string \f{s}, and \f{s} has not been encountered, then \f{s \not\in L(\category{S})}. Again, new strings will be formed by adding something to old strings, therefore the length will only grow. If all new strings are longer than \f{s}, then even newer strings will be still longer.
* After a finite number of rounds, all new strings will be longer than \f{s}. Intuitively, all steps produce bigger and bigger strings.

To prove that \f{s \in L(\catgeory{S})} we can then apply the process until we find the string \f{s} or all newly generated strings are longer than \f{s}. The proof of this shows what rules to follow and how to expand other syntactic categories in order to produce \f{s}. This can be seen as an ordered tree, called the parse tree. The nodes are labeled by terminals or \f{\epsilon} and, are, in order, \f{s}. The interior nodes are labeled by syntactic categories, and signify that that category was expanded by a production (with tail given by its children). The yield of the tree is the string of terminals in it (labels of the trees or result of preorder traversal minus the non-terminal labels). Parse trees can be described inductively from a syntactic category (we have the set of parse-trees of a grammar) (each terminal production determines one parse tree, each complex rule determines the "natural deduction").

If \f{T} is a parse tree with root \f{\category{S}} and yield \f{s} then \f{s \in L(\category{S})} and if \f{s \in L(\category{S})} then there is a parse tree \f{T} with yield \f{s} and root \f{\category{S}}. Proof by induction on the height of the tree (one direction) and induction on the number of recursive applications of the inductive step (other direction).

A grammar where there are more than one trees for some string and with the same root label is called ambiguous. Otherwise the grammar is unambiguous. Programming languages need to have unambigous grammars, otherwise we get one program with possibly several meanings.

Recursive descent parsing of grammars: certain grammars can be parsed by building recursive descent parsers. If all recursive productions are right-recursive (boil down to \f{\category{A} \rightarrow \category{B} \category{A}}) and for a set of productions for a syntactic category, each one has an unique left-most production (either a terminal or syntactic category), then, we can build a set of functions for each syntactic category. One such function has as many branches are there are productions for the category. It tries the first production, by trying to match every term in the production. If a term is a literal, it is tested for equality with the current input symbol. If a term is a syntactic category, the appropriate function is called. If at some point, failure occurs (no match for literal, or null returned from the syntactic category function), we fail the branch for this production, and we try the branch for another production. If all branches fail, we fail the whole parsing. A global "current input counter" is mantained, incremented for matches and decremented for failures. At the same time we're parsing we build the parse tree. Each function returns an appropriate node and each a function builds the appropriate node according to the branch it is in and whatever its children returned. Proof that it works: assume \f{G} is an unambiguous grammar with special properties for rec dec parsing and \f{s} a string with a unique parse tree \f{T}. Proof by induction on the height of \f{T}. If \f{T} has height \f{1}, then it is is a root labeled by a production \f{\category{S}} and leafes labeled by terminals. There must be the associated production \f{\category{S} \rightarrow T_1\dots T_k}. The parser will start with \f{\category{S}} (the start category) and notice input \f{T_1}. There will be only one production which starts with \f{T_1}, and we process it, matching the input. Therefore, this holds. Assume we have a tree of height \f{n+1}. It has form \f{\category{S}} with children of height \f{n} at most. There is only one production which starts with the same first category as the first node in the tree, and therefore the parser will pick it. It will try all subrules, and they will succeed, by the induction hypothesis. Thus it will build the proper tree.

A recursive descent parser can be turned into a table parser, which is a more efficient/compact form. Notice that the recursive calls make havey use of the stack. The table parser represents the stack explicitly. The same constraints apply for the language. We push the start syntactic category on the stack. In general, after several steps, the top category is \f{\category{S}}. We have a table which tells us for each category and input symbol, which production to follow. For the current symbol, if the table is blank, we've failed. Otherwise, we pop the stack and push in right-to-left order, the tail of the selected production. Whenever we process a terminal, we see that it and the input match, after which we pop. Whenever we process a category, we do what we just described. This is a more compact form of the previous formulation, and we can also build a tree, but from the top-down not bottom-up. Similarly, the table must be built ahead. To complete the parse table at cell \f{(\category{S},X)} we see if symbol \f{X} is the leftmost element in the tail of a production of \f{\category{S}} or of a leftmost syntactic category in a production of \f{\category{S}} and so-forth. Notice that pops occur only when matching terminals with input symbols.

We must elmininate left-recursion from a grammar, for it to be recursive descent parsable. We must also eliminate common prefixes to rules, by building a new rule with the prefix and another rule, which has the different elements (left factoring).
* Turning a regular expression into a grammar (regular languages are a subset of context free languages). Construct: basis case of \f{x} is turned into grammar \f{\category{Q} \rightarrow x}, case \f{\epsilon} is turned into grammar \f{\category{Q} \rightarrow \epsilon} and case \f{\emptyset} is turned into grammar with no rules. If \f{R = R_1R_2} then the new grammar is the union of the grammars of \f{R_1} and \f{R_2} (with no syntactic categories in common) and start rule \f{\category{Q} \rightarrow \category{Q_1} \category{Q_2}} (where \f{\category{Q_i}} is the start state of grammar \f{i}). If \f{R = R_1 \given R_2} then the construct is \f{\category{Q} \rightarrow \category{Q_1} \given \category{Q_2}}. If \f{R = R_1^\stasr} then the construct is \f{\category{Q} \rightarrow \category{Q_1} \category{Q} \given \epsilon}. Proof by induction on number of operator applications.
* A context free language which is not regular (regular languages are a strict subset of context free languages). \f{E = \{0^11^1, 0^22^2, 0^31^3, \dots\}} - set of languages with \f{n} \f{0}s followed by \f{n} \f{1}s. Grammar \f{\category{S} \rightarrow 0\category{S}1 \given \epsilon}. Suppose there was an automaton which accepted this language. It would have, say \f{m} states. Suppose we have \f{n > m} and we feed \f{m} \f{0}s to the automaton. It starts in state \f{s_0} and gets to state \f{s_m}. There are \f{m+1} states on the path, but only \f{m} states in the automaton. Therefore there are at least \f{s_i = s_j} with \f{i \leq j}. The automaton is nondeterministic, since it has a cycle and the only input is \f{0} (when it gets to \f{s_i} it once goes through \f{s_j} on \f{0}, while the other time it goes to \f{s_m}). Now, if we instead feed \f{0^{n-i+j}} we get the same result. First \f{i} \f{0}s take us to \f{s_i}. Then, the rest of \f{m-j} \f{0}s take us to \f{s_m}. From here, the same input as in the previous case is fed to the automaton, and it reaches an accepting state. But this means \f{0^{n-i+j}1^n} is in the language accepted, but this is a contradiction. Hence, no automaton recognizes this. Since no automaton recognizes it, no regular expression does (if it did, we could obtain an automaton which recognized the language). Other non-regular languages: set of balanced parantheses (similar construction as above; also notice that \f{(^n)^n \equiv 0^n1^n} is a subset of the set of balanced parantheses, and it is not regular), \f{ww^R} (palindrome strings), \f{0^n} (where \f{n} is a perfect square - must choose \f{n = m^2} so that \f{m^2 \leq m^2 - (j-i) > m^2 - m = m(m-1) > (m-1)^2} so it can't be a perfect square), \f{0^n} (where \f{n} is prime - use the fact that prime gaps can be made abitrarily large (by the Prime Number Theorem) and we can choose \f{n} such that the difference with the previous prime is greater than the number of states in the automaton). A falacious argument is that a re of the form \f{01 \given 0^21^2 \given \dots} can describe non-regular languages. But a re must have a finite number of operators. Another falacious argument is descrbing res/automatons which accept the context-free language and many other languages (the arguments for all the previous examples fall into this category - any automaton we build will be too general).

Parsing expressions: an interesting application of grammars is in describing, in an unambiguous way, how to parse arithmetic expressions. The problem lies that expressions are specified without all the proper parantheses, so we have issues of ambiguity which lead to bad interpretations (\f{1+2*3} being \f{(1+2)*3} instead of \f{1+(2*3)}). Even if we had only one type of expression, the naive approach would generte many possible trees (even though, if the operator is associative, it would not matter). To address this, each binary opertor is either left-associative or right-associative. For example, if \f{\star} is la and we have \f{\dots x \star y \star z \dots} then \f{y} will be bound to the left \f{\star}. We also introduce priorities for operators, and partition them according to it. For example, if \f{\star} has higher priority than \f{\box} and we have \f{\dots x \box y \star z \dots} then \f{y} will be bound to the right \f{\star}. More precisely, the expression is a \f{\box}-ing of \f{\star}-terms, where each term is a \f{\star}-ing of factors. Factors are atomic objects we deal with (numbers) or paranthesised expressions.

To wit, we have a set of binary and unary operators. Binary operators are either left or right associative. Unary operators appear on the left or on the right. Every operator belongs to only one priority. And all same-priority binary operators must have the same associativity. An expression consists of the lowest priority operators applied to terms formed from higher priority operators, ordered by the associativity. This continues recursively. The rules we build are \f{\category{E} \rightarrow \category{P_l}, \category{P_l} \rightarrow \category{P_l} b^l_{1} \category{P_{l-1}} \given \dots \given \category{P_l} b^l_{q_l} \category{P_{l-1}} \given  u^l_{1} \category{P_{l-1}} \given \dots \given \category{P_{l-1}} m^l_{1} \given \dots \given \category{P_{l-1}}, \dots, \category{P_1} \rightarrow \category{P_1} b^1_{1} \category{T} \given \dots \given \category{P_1} b^1_{q_1} \category{T} \given  u^1_{1} \category{T} \given \dots \given \category{T} m^1_{1} \given \gots \given \category{T}, \category{T} \rightarrow ( \category{P_l} ) \given \category{atomic}}. For right-associative binary operators we have \f{\category{P_{i-1}} \star \category{P_i}} (but this must be consistent for a level). Using \f{l+1} symbols we defined what we wanted.
