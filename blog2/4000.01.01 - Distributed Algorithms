Classical algorithms and data structures assume a computational model which consists of a processor, memory and the timed execution of instructions. The fact that the classical system exists in space relative to users and in time on a large scale is not considered. The fact that the system might fail is not considered as well.

\def{Distributed systems}, \def{distributed algorithms} and \def{distributed data structures} study how to build and program systems comprising several independent computing units. Geographical and temporal aspects are acknowledged and attention is given to failures.

A distributed system:
* Has a set of computational units called \def{nodes}, each of which has a processor and local memory (both volatile and permanent).
* The nodes are connected by \def{links}.
* The graph of nodes and links comprises the \def{topology} of the network.
* Nodes communicate by passing messages between each other, as long as there are links between them.
* Each node has its own clock, but there is no central clock.
* There is no shared state.

Interesting aspects in a distributed system / algorithm / data structure:
* Correctness - The system must solve the problem at hand in a correct manner.
* Time complexity - The system has a small time complexity, measured as the maximum of the complexities of the programs ran at each node.
* Space complexity - The system has a small space complexity, measured as the sum of the complexities of the programs ran at each node.
* Time speedup - The decrease in execution time between the distributed algorithm and the classical one.
* Space overhead - The increase in space between the distributed algorithm and the classical one.
* Communication overhead - The system has a small number of messages sent, measured as the sum of the messages sent by each node.
* Availability - The proportion of time a system is in a functioning condition. Can also have \def{user availability} which is the proportion of time a system is in a functioning condition for a given (average user). We have \f{A = U / (U + D)} where \f{A} is availability, \f{U} is uptime and \f{D} is downtime. Availability is achieved through \def{fault tolerance}, which is the ability of a system to whitstand nodes failing.
* Latency - A measure of the time delay between initiating an action and its effects being felt. Lower latencies are better. There are physical limitations to how low we can go however, given by the speed of light \f{c}.Latency appears as:
* Query latency - The latency experienced by a user of the system, that is, what is the period of time between a user starting an action and the results of the action being visible to the user.
* Data latency - The latency between performing some change to the data (not necessarily user initiated) and that change being reflected in the workings of the system (producing different results for a user query, for example). If data never changes, there is no data latency.
* Scalability - The ability of a system to handle a growing amount of work in a capable manner, and to be enlarged to accomodate that growth.

Ideally, adding a new machine would increase the performance and capacity of the system linearly. This is not possible, because each new node adds some overhead (data needs to be copied, coordination needs to happen etc). Studying distributed algorithms offers efficient solutions to specific problems, and guidance on what is possible and not.

Distributed programming deals in large part with the consequences of distribution. There is a tension between the reality that there are may nodes and the ease of use of systems that work like a single node.

The eight fallacies of distributed computing, as identified by \ref{L Peter Deutsch} and others at Sun Microsystems:
* The network is reliable.
* Latency is zero.
* Bandwidth is infinite.
* The network is secure.
* Topology doesn't change.
* There is one administrator.
* Transport cost is zero.
* The network is homogenous.

The speedup of \f{p} processors over \f{1} is \f{S = T_1 / T_p}. We would like this to be \f{p} - adding more processors increases the processing power linearly. In real life, this does not scale liniarly, however. Efficency, measures how the real life case measures to the theoretical best case and is defined as \f{E = S/p = T_1/(pT_p)}. It goes down as the number of processors goes up, however, instead of staying constant (theoretical best case). In real life, the trick is to try to solve larger and larger problems, on which the limitations of communication don't show. No point in using \f{1} million processors to search a \f{1} million byte file.

A scalable algorithm is one in which \f{E} is an increasing function of \f{n/p}, where \f{n} is "problem size" - as the size of the problem goes up wrt the number of processors, you get better and better efficiency.

Paradigms of parallel programming:
* \def{shared memory} - different computational work on the same data - need locking and does not scale with the number of processors. Partitions work.
* \def{message passing} - same work on different data - needs to communicate results/data after performing work so they cand do more work. Does not scale with communication but easier to scale. Partitions data.

= Models for distributed systems =

Given a data on which we wish to perform some computation, the \def{central dogma of distributed systems} is that the data and the computation performed must be split between several nodes. There are two forms of splitting:
* Partitioning - Split the data between the \f{n} nodes. Need not be an actual partition though. In general this is application specific. Performance increases by limiting the amount of data which needs to be inspected. Availability increases by isolating failures to subsets of the data.
* Replication - Keep a copy of the full data on each node. Increases performance by dedicating another node to the same copy of the data. Increases fault tolerance / availability. If one node fails, others will take it's place. Reduces latency, by possibly keeping data closer to a user. Will need to add some sort of way to keep the multiple copies \def{consistent}.

Models of distributed systems describe the key properties of a distributed system and the assumptions it makes about its environment. We can have:
* Timing model -  Because of physical limitations, and the fact that information can travel only so fast, each node basically experiences the world in it's own way. \def{Timing assumptions} are a shorthand for the extent to which we take reality into account.
* Synchronous - Execute in lock-step. There is a known upper bound on message transmission delay and each process has an accurate clock. Wildly unrealistic except for very specific situations (several cores on a single CPU, sharing a physical clock).
* Asynchronous - Execute at independent rates. There is no upper bound on message transmission delay and useful system-wide clocks do not exist. Real world systems are closer to asynchronous systems, rather than synchronous ones.
* Failure model
* Node failure or \def{faults} - Nodes fail independently. Resistance is called \def{fault tolerance}. Failures can be \def{crashes} (with optional \def{recovery}) or \def{adversarial} (or \def{Byzantine}).
* Link failure or \def{partitions} - Resistance is called \def{partition tolerance}. Messages might be delayed (to an unbounded degree), lost or delivered out of order. The link might drop. Spurrious messages might appear. Partitioned nodes might still be able to be accessed by clients or may be able to communicate with only a subset of the nodes, so must be dealt differently than crashed nodes.
* Consistency model - A contract between a programmer/user and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable. Categories of consistency models:
* Strong - These are capable of mantaining a single copy, while also ensuring that apparent order and visibility of updates is equivalent to a non-replicated system. Can be:
* Linearlizable - All operations appear to have executed atomically in an oder that is consistent with the global real-time ordering of operations.
* Sequential - All operations appear to have executed atomically in some order that is consistent with the order seen at individual nodes and that is equal at all nodes.
* Weak - Defined as not strong. Can be:
* Client-centric -A client / session of the system exists, and we can ensure that it never sees an older version of data, usually by caching. It might see old data if the partition it is connected to has out-of-date data, but not anomalies.
* Causal
* Eventual - If you stop changing values, then, after some undefined amount of time all replicas will agree on the same value. In the meantime, the values are inconsistent in some undefined manner. Trivially satisfiable by setting the bound to \f{+\infty}.

Strong consistency allows one to replace a single node with a group of distributed nodes and not run into any problems. Weak consistency introduces anomalies and exposes to the programmer the fact that he is dealing with a distributed system. Often these are acceptable (we either don't care about them or can deal with them in some way).
A system that makes weaker guarantees has more freedom of action, and hence potentially greater performance - but it is also potentially hard to reason about.

= The consensus problem =

\def{The consensus problem} is a fundamental problem in distributed systems and consists of having the nodes in a system agree on a value. Each node proposes a value and an algorithm must somehow choose such that we have:
* Agreement - Every correct process must agree on the same value.
* Integrity - Every correct process decides at most one value, and if it decides some value, than that value must have been proposed by some process.
* Termination - All processes eventually reach a decision.
* Validity - If all correct processes propose the same value \f{V}, then all correct processes decide \f{V}.

Examples where the consensus problem comes into play: all nodes must agree that a node with id \f{i} is the master node in the system, all nodes must agree that a particular value for the age of an Employee, all nodes must agree on who holds a lock on a certain database row etc.

== The Fischer-Lynch-Patterson (FLP) theorem ==

Assume a asynchronous system, where nodes can only fail by crashing, the network is reliable, but there are no bounds on message delay. The result of the \def{FLP theorem} says that there does not exist a deterministic algorithm for the consensus problem under the given conditions, even if at most one process fails.

Must make a tradeoff between: liveliness and safety. The system may continue to operate continuously, but it might not work correctly, or it may work correctly, but it might be indefinetly delayed. This is a hard constraint for designing distributed algorithms in this system model.

== The CAP theorem ==

The \def{CAP theorem} says that out of the following three desirable characteristics of a distributed system:
* Consistency \f{C} - All nodes see the same data at the same time.
* Availability \f{A} - Node failures do not prevent survivors from continuing to operate.
* Partition tolerance \f{P} - The system continues to operate despite message loss to to network and/or node failure.

Only two can be satisfied simultaneously. There is a nice \img{} venn diagram for this. Notice the center is empty. The consistency referred is \def{strong consistency}.

If we want consistency, then we can choose availability, which means the system cannot determine between a node and link failure, and will have to stop all updates once a fault happens, or we can choose partition tolerance, which means that the system will be able to serve users, but a minority of nodes will have to be stopped, hurting availabiliy. If we want partition tolerance, then we can choose availability, in which case we can't have consistency, since we'll need to have two separate and online subsystems, or we can have consistency, in which case availability will suffer, as we stop the nodes in the minority partition. If we want availability, then we can choose consistency or partition tolerance, as above.

There is a tension between strong consistency and performance as well in normal operation. Strong consistency requires that nodes communicate and agree on every operation, which results in bad performance.

Partition tolerance and availability are nice properties to have on their own, while consistency imposes extra computational and communication problems. If we relax the requirements to weak consistency, we have better performance and some sort of guarantee while we can get the other two properties fully.

= Parallel Programming =

== Distributed File Systems ==

When working with large amounts of data, a single hard disk might not be enough. Similarly, a single compute node might not be enought. One needs to employ clusters of compute notes, if the problem allows it. Fortnately, for most of the problems, the data is highly regular (tables) and the algorithms are highly paralelizble. Enter DFS - there are many storage nodes and one master node, which holds the file-system structure. Files are table like, and are mostly read from or appended. A client makes a request for a certain record in a certain file, to the master, and the master replies the storage node where the record is held. In reality, many records (in chuncks of 64MB) are read in one go. Also, records are held in several storage nodes, for redundancy. Moreover, there are fallbacks for the master nodes, although only one of them can operate at a certain moment.

Examples: Google File System, Hadoop File System, Big Table.

In general, if your data and algorithm access patterns are well known, and if you have enough data (and money, and time), it makes sense to use a special purpose file system, instead of standard NFS or anything.

== Map Reduce ==

Challanges of Traditional Data Warehouse Tech:
* Fault-Tolerance at scale.
* Variety of Data types: text, images, video.
* Manage data volumes without archiving.
* Parallelism was an afterthought, bolted on traditional tech.

Traditional tech could not scale, was not suited for compute-intensive deep analytics and they were very expensive.

We have parallel processing and distributed data storage.

\def{Map-reduce}: a message passing and data-parallel paradigm. It is a higher-level concept than shared memory/message passing. There are two types of tasks, mappers and reducers. There are many mappers applied to data. The results are combined by reducers. Many algorithms, especially numerical/statistical ones, can be expressed efficiently as map-reduce programs. A map task works on a set of key-value pairs and produces a set of key-value pairs, but acts on each independently. It can emit many key-value pairs as its output. A reduce task works on a set of key-value pairs produced by the mappers, but with the same key. Each mapper needs to communicate with each reducer, in order to communicate their results, or at least, the keys that actually need to be transmitted (if one reducer handles certain keys, we only send the mapper outputs with the required keys). More formally, a mapper, reads a \f{(k_1,v_1)} pair and emits \f{(k_2,v_2)} pairs, where \f{(k_2,v_2) \leftarrow \text{map}(k_1,v_1)}, a reducer receives all pairs for some \f{k_2} and combines these in some manner, where \f{k_2,f(\dots v_2 \dots) \leftarrow \text{reduce}(k_2,[\dots v_2 \dots])}. The map-reduce system is the actual hardware-software system which allows one to define map and reduce tasks, their data sources and sinks and which decides how many mapper tasks to start, how many reducer tasks to start, how to split data to mappers and how to route mapper output to reducers. It is a processing system, not a storage system, however. It can work with techically any storage system, but usually, it is a special distributed file-system like system for massive amounts of data. An SQL database, for example combines storage with processing (queries and simple processing), but it is not able to do the deep analytics we want from it. Sometimes, after the mapper phase, there is a combine phase. This is sometimes implicitly written in the map phase, as some systems don't allow for it as separate. The combine phase is a smaller reduce function, usually executed on several "local" mappers. This helps with communication overhead. This works only if the reduce function is a commutative and associative function. Some reducers also allow for reduce function itself to be a combiner.

In some systems, one does not see the input to the mappers as key-value pairs. Rather, for the sake of efficiency, we see them as "chunks" of "elements" (groups of records, if you will). Any element can belong to a single chunk and a mapper processes receives all the elements in a chunk at once, to speed things up. Similarly, keys are not actually "keys" from retrieval, since they can appear more than once.

A grouping and aggregation process, run by the map-reduce system, distributes the key-value outputs of the map phase. It knows that there will be \f{r} reduce processes, and assigns each map-outputted-key, via a hash function to one of the \f{r} processes, not before joining the data for each key.

The actual system is invoked by a user, with a specified map and reduce procedures, as well as several other parameters, and a source and destination file from the data system. It first starts a Master node on a designated computer, and a number of Worker nodes on other computers. Then, a number of map and reduce tasks are created. The map tasks can be assigned such that each task has one or more associated chunks. Fewer than this number of reduce tasks are created though (a submultiple of the number of different keys expected, as well as having to take into consideration the fact that, as will be seen further, there must exist a file for each reduce task). Each task can be in one of three main states: idle, assigned, completed. Idle tasks have been created but have not been assigned to any worker, assigned tasks are tasks in the process of being computed and completed tasks are self-explanatory. Nodes are usually split into map-nodes and reduce-nodes, and only receive tasks of one kind. Map tasks are assigned to mapper-nodes (evenly split to each mapper-node at the start, or depending on compute power). Each mapper worker node writes on its local disk, the key-values for a certain reducer, in a designated file (so there is one such file for each reducer, or at least each reducer which is assigned a key by the current mapper node). As a map task is completed, this is communicated to the master. As mapper tasks finish, reducer tasks can then start. The master tells them which mappers have files associated with them, and they only need to remotely read the files from each mapper, and then assemble the lists for each key, and process each key in turn. The resulting key-value is written to the datastore. The master periodically pings each worker. If no reply is obtained, a failure is assumed. If a mapper node fails, all the chunks/map tasks assigned to it must be reprocessed (as the reducer cannot access the associated partial files) - the task is started at another mapper. If a reducer node fails, the current task is simply restarted. If the master fails, it is game-over.

Now, a quick analysis of the actual efficiency. Suppose we have \f{D} data items which we need to process and \f{P} processors available which are to be used for both mapping and reducing. The bulk of the work to be done is in the mapping, and it takes \f{w} time to process one document (here \f{w} is typically dependent on data item size, for example \f{w = O(d\log{d})} where, \f{d} is the average size of a data item). Furthermore, suppose that each mapper emits, on average \f{\sigma} new data items, so we have \f{\sigma D} new intermediate data items generated. The processing time for one processor is \f{T_1 = wD} (again, ignoring the reduce costs). The processing time for the \f{P} processors is \f{T_P = T^w_P + T^o_P}, where \f{T^w_P} is the time required for work, across all processors, and it is \f{T^w_P = wD / P}, and \f{T^o_P} is the time required for overhead work, such as communication or synchronization, and it is made up of \f{\sigma D / P} time to store/write the data produced by each mapper and \f{\sigma D / P^2 P = \sigma D / P} time to transmit each intermediate data item to each reducer (you can view the previous equation in two ways - first, for each of the \f{P} mappers, which has generated \f{\sigma D / P} intermediate data items, on average, we have to send \f{1/P} of this data to a reducer, therefore we have \f{P \sigma D / P 1/P = P \sigma D / P^2 = \sigma D / P} - second, each reducer will read/receive \f{1/P} of all the \f{\sigma D} intermediate pairs generated, therefore we have \f{\sigma D/P} time to read this). All the overheads are also multiplied by a \f{c}, the average cost of communication in our system.

Overall, the efficiency is:
%formula{
E_{\text{MR}} & = \frac{T_1}{P T_P} \\
              & = \frac{T_1}{P \left(T^w_P + T^o_P\right)} \\
              & = \frac{wD}{P \left( \frac{wD}{P} + 2c \frac{\sigma D}{P} \right)} \\
              & = \frac{1}{1 + \frac{2c}{w} \sigma} \\
}

What this means is that the efficiency is, at a first glance, independent of \f{P}, therefore map reduce is, for the problems which can be efficiently mapped to it, a very efficient method and a scalable one. Furthermore, the lower the communication costs \f{c}, the better \f{E_{\text{MR}}} approaches \f{1}. Also, the higher the processing cost per data item, \f{w}, the better, \f{E_{\text{MR}}} approaches \f{1}. Similarly, the lower \f{\sigma}, the better, \f{E_{\text{MR}}} approaches \f{1}. This is where a hidden dependence on \f{P} or \f{D} might appear, therefore we need to control for this by using combiners and designing algorithms carefully.

Algorithms that best use map-reduce are those that operate on very large files, which are mostly read and appended to, but not modified in place. Analytic queries basically to be done on "gathered data". Web-serving on a massive scale would not be a good application, however.

A list of algorithms, expressed in terms of map and reduce functions:
* \def{Word counting} Input key is document id, input value is document as a series of words. The mapper task produces a set of key value pairs for each word in a document, with value \f{1}. The reduce task counts the number of items in the list for one word.
* \def{Word counting \#2} Input key is document id, input value is document as a series of words. The mapper task produces a set of key value pairs for each word in a document, with value equal to the number of times that word appears in the document. The reduce task sums the values for the keys it receives.
* \def{Matrix-vector multiplication} Input key is a position \f{(i,j)} and value is \f{(m_{ij},v_j)}. The mapper task computes \f{m_{ij}v_j} and outputs \f{(i,m_{ij}v_j)}. The reduce task outputs key \f{i} and sums the list of values it receives.
* \def{Matrix-vector multiplication \#2} Input key is a position \f{i} and value is \f{(m_{i \star},v)}. The mapper task computes \f{a_i = \sum_j m_{i j} v_j} and outputs \f{(i,a_i)}. The reduce task is the identity function.
* \def{Matrix-vector multiplication for huge vectors} Input key is a position \f{(i,j)} and value is \f{(m[i:i+k,j:j+k],v[j:j+k])}.
* \def{Relational algebra set selection}
* \def{Relational algebra bag selection}
* \def{Relational algebra projection}
* \def{Relational algebra set union}
* \def{Relational algebra set intersection}
* \def{Relational algebra set difference}
* \def{Relational algebra bag union}
* \def{Relational algebra bag intersection}
* \def{Relational algebra bag difference}
* \def{Relational alegbra grouping and aggregation}
* \def{Relational algebra natural join}
* \def{Relational algebra other joins}
* \def{Matrix-Matrix multiplication}
* \def{Matrix-Matrix multiplication \#2}

= Consistent Hashing =

Hashing for distributed systems. Used as sharding functions for placing data and computation on machines in a way which requires just \f{O(N/k)} data movements when we add a new machine.

Split the domain of hashing in buckets which are assigned to machines. The buckets might not be contiguous, but the assignment rule is simple: first \f{\abs{D}/k} keys go to the first machines, the next to the second etc.

When an a new machine is added, the domain of one machine is split in two and that data is split between the two machines. Only this data needs to move, not for other machines.

Similarly, when we use backups for redundancy, when a machine falls down we just need to move the associated data to another machine to recover integrity.
