= Polynomials =

A \def{monomial} is the product of a numeric \def{coefficient} and a natural power of an \def{unbound variable}. Examples: \f{4x^3}, \f{3x}, \f{5}. A \def{polynomial} is a sum of monomials. Examples: \f{3x^2 + 2x + 7/2}, \f{-2/3x^5 + x^3 + x + 2}, \f{x + 15}, \f{(x^2 + 2x)(x^3 + 4x + 3)} (not in standard form). The \def{sete of polynomials} over \f{\mathbb{R}}, or \f{\mathcal{P}[\mathbb{R}]} is the set of all polynomials which can be built with numbers from \f{\mathbb{R}}. The \def{standard form} for a polynomial is grouping all variables together with their coefficients placing the constant before the variable, and ordering these terms in decreasing order of natural power.

What we have presented so far are single variable polynomials. We can have, in general, polynomials with more than one variable, built with the same rules as before. Such polynomials are, however, not as richly structured as the single variable ones.

The \def{null polynomial}, or \f{\textbf{0}} is the polynomial consisting of the single number \f{0}. The \def{identity polynomial}, or \f{\textbf{1}} is the polynomial consisting of the single number \f{1}.

Let \f{P = a_n x^n + a_{n-1} x^{n-1} + a_1 x + a_0}, \f{Q = b_m x^m + b_{m-1} x^{m-1} + b_1 x + b_0} and \f{R = c_k x^k + \cdots + c_0} be polynomials from \f{\mathcal{P}[\mathbb{R}]}, with \f{n} and \f{m} \ref{natural numbers}.

The \def{degree} of a \f{P}, or \f{\deg{P}} is the \ref{natural number} which corresponds to the largest power an unbound variable is raised to, in the expression represented by \f{P}. \f{\deg{\textbf{0}} = -\infty}. \f{\deg{\textbf{1}} = 1} and \f{\deg{P} = n}. The \def{set of polynomials of degree n} over \f{\mathbb{R}}, or \f{\mathcal{P}^n[\mathbb{R}]} is the set of all polynomials of degree at most \f{n}. Members of this set can be represented alternatively as \f{(n+1)}-tuples, with the \f{i^\text{th}} element of tuple being equal to the coefficient of \f{x^{i-1}} in the expression represented by the polynomial. By construction, \f{\{\textbf{0}\} \subset \mathcal{P}^0[\mathbb{R}] \subset \mathcal{P}^1[\mathbb{R}] \subset \mathcal{P}^2[\mathbb{R}] \subset \dots \subset \mathcal{P}[\mathbb{R}]}.

\f{P} \def{equals} \f{Q}, or \f{P = Q} is the two polynomials have the same degrees and each corresponding coefficients are equal.

The \def{polynomial function} of \f{P}, or \f{P_f} is \f{P_f \colon \mathbb{R} \rightarrow \mathbb{R}} and \f{P_f(x) =  a_n x^n + a_{n-1} x^{n-1} + a_1 x + a_0}. This is an alternative view of the polynomial.

If \f{P} and \f{Q} be two polynomials from \f{\mathcal{P}[\mathbb{R}]} with \f{P_f = Q_f} then \f{P = Q}.

The \f{sum} of \f{P} and \f{Q}, or \f{addition} of \f{P} and \f{Q}, or \f{P + Q} is a binary operator on \f{\mathcal{P}[\mathbb{R}]} which associates with each pair \f{(P,Q)} the polynomial which is the sum of the expressions of \f{P} and \f{Q}. The \f{product} of \f{P} and \f{Q}, or \def{multiplication} of \f{P} and \f{Q}, or \f{P \star Q}, or \f{PQ} is a binary operator on \f{\mathcal{P}[\mathbb{R}]} which associates with each pair \f{(P,Q)} the polynomial which is the product of the expressions of \f{P} and \f{Q}.

Properties of polynomial addition and multiplication:
* \f{P + (Q + R) = (P + Q) + R} - associativity of polynomial addition.
* \f{P + Q = Q + P} - commutativity of polynomial addition.
* \f{P + \textbf{0} = \textbf{0} + P = P} - \f{\textbf{0}} is the identity element for polynomial addition.
* \f{\exists (-P) \in \mathcal{P}^{\deg{A}}[\mathbb{R}] \colon P + (-P) = \textbf{0}} - polynomial addition admits an additive inverse.
* \f{P(QR) = (PQ)R} - associativity of polynomial multiplication.
* \f{PQ = QR} - commutativity of polynomial multiplication.
* \f{A\textbf{1} = \textbf{1}A = A} - \f{\textbf{1}} is the identity element for polynomial multiplication.
* \f{P(Q + R) = PQ + PR} - distributivity of polynomial addition and multiplication.

The degree of the sum or product polynomial can be determined:
* \f{-\infty < \deg(P + Q) \leq \max(\deg{P},\deg{Q})}.
* \f{\deg(AB) = \deg{A} + \deg{B}}.
* If \f{P \neq \textbf{0}} and \f{Q \neq \textbf{0}} then \f{PQ \neq \textbf{0}}.

The \def{substraction} of \f{P} and \f{Q}, or \def{difference} of \f{P} and \f{Q}, or \f{P - Q} is a binary operator on \f{\mathcal{P}[\mathbb{R}]} which associates with each pair \f{(P,Q)} the sum of \f{P} and the additive inverse of \f{Q}.

Some basic algebraic manipulations of polynomials:
* \f{\textbf{0}P = \textbf{0}}.
* \f{P = Q \Rightarrow P + R = Q + R}.
* \f{PR = QR}.
* If \f{PR = QR} and \f{R \neq \textbf{0}} then \f{P = Q}.
* If \f{P - Q = Q - P} then \f{P = Q}.
* \f{-(P + Q) = (-P) + (-Q)}.
* \f{-(PQ) = -(P)Q = P(-Q)}.
* \f{-(-P) = P}.
* \f{-P = (-1)P}.
* If \f{PQ = \textbf{0}} then \f{P = \textbf{0}} or \f{Q = \textbf{0}}.
* If \f{PQ \neq \textbf{0}} then \f{P \neq \textbf{0}} and \f{Q \neq \textbf{0}}.
* If \f{P \neq \textbf{0}} and \f{Q \neq \textbf{0}} then \f{PQ \neq \textbf{0}}.
* \f{(P + Q)(R + S) = PR + PS + QR + QS}.

Factoring is the process of transforming a polynomial in standard form to a product of other, simpler polynomials. It is, in general, a hard problem, for which many tricks exist. Examples of tricks: using the distributive property in reverse, separating monomials into sums of monomials with different coefficients, adding and substracting a same monomial/polynomial so that it may be combined with others, looking for zero values and when they occur in the interaction of the several variables, looking out for conspicuous powers such as \f{x^4 + y^4} which is \f{(x^2)^2 + (y^2)^2}.

The \def{division} of \f{P} and \f{Q}, or \def{ratio} of \f{P} and \f{Q}, or \f{P / Q} is a binary operator from \f{\hctimes{\mathcal{P}[\mathbb{R}]}{\mathcal{P}[\mathbb{R}]}} to \f{\hctimes{\mathcal{P}[\mathbb{R}]}{\mathcal{P}[\mathbb{R}]}} which associates with each pair \f{(P,Q)} the polynomials \f{K} and \f{R} such that \f{P = KQ + R}. \f{K} is called the \def{quotient} and \f{R} is called the \def{remainder}. This is similar to the situation for integer numbers.

We have that \f{\exists K \in \mathcal{P}[\mathbb{R}], b \in \mathbb{R} \colon P = (x - a)K + b}, that is, there always exists unique quatient and remainder polynomials when dividing by a first degree polynomial.

We will perform strong induction on the degree of the polynomial. For \f{\deg{P} = 1}, we have \f{P = a_1x + a_0}. We can write it as \f{(x - a)a_1 + (a_0 + a_1a)} with \f{K = a_1} and \f{b = a_0 + a_1a}. Thefore, the statement holds for \f{\deg{P} = 1}. Assume that the property holds for \f{\deg{P} \in \hcrange{1}{n}}. Then for \f{\deg{P}=n+1} we have that \f{P = a_{n+1}x^{n+1} + \dots a_1x + a_0}. We can write this as \f{P = x(a_{n+1}x^n + \dots a_1) + a_0} with \f{Q = a_{n+1}x^n + \dots a_1} an \f{n^{\text{th}}} degree polynomial. We can write \f{Q} as \f{(x - a)K_Q + b_Q} which makes \f{P = x((x - a)K_Q + b_Q) + a_0 = x(x - a)K_Q + xb_Q + a_0} with \f{R = xb_Q + a_0}. We can now write \f{R} as \f{(x - a)K_R + b_R} which makes \f{P = x(x - a)K_Q + (x - a)K_R + b_R = (x - a)(xK_Q + K_R) + b_R}, which is just the form we were looking fore. Therefore the statement holds for \f{\deg{P} = n + 1}, whenever it holds for \f{\deg{P} \in \hcrange{1}{n}}. By the principle of strong induction, the statement will hold for all \f{n}.

The last statement generalizes to arbitrary polynomials. \f{\exists K,R \in \mathcal{P}[\mathbb{R}] \colon \deg{K} = \deg{P} - \deg{Q} \text{ and } -\infty < \deg{R} < \deg{P} \text{ and } P = KQ + R}. Furt
hermore \f{K} and \f{R} are unique.

The existance is given by the fact that the polynomial division algorithm (not covered here) terminates. Let us prove that they are unique. Assume, by contradiction, that there is \f{K'} and \f{R'} such that \f{\deg{K'} = \deg{P} - \deg{Q}} and \f{-\infty < \deg{R'} < \deg{Q}} and \f{P = K'Q + R'}, but \f{K' \neq K} and \f{R' \neq R}. By substracting \f{P = K'Q + R'} from \f{P = KQ + R} we get \f{\textbf{0} = (K - K')Q + (R - R')}. Now \f{R - R'} is a polynomial of degree strictly smaller than \f{\deg{Q}} and \f{K - K'}  is a polynomial of degree smaller than \f{\deg{P} - \deg{Q}}. The multiplication of \f{K - K'} with \f{Q} has degree equal to \f{\deg(K - K') + \deg{Q}} by \ref{TheDegreeSumProductPolynomial}. In order for \f{(K - K')Q} and \f{R - R'} to cancel, they must have the same degree. This cannot happen if \f{\deg(K - K') \geq 0} because \f{\deg{Q} > \deg{R - R'}}, therefore \f{\deg(K - K')} must be \f{-\infty} and \f{K - K' = \textbf{0}} and \f{K = K'}. Furthermore, this makes the product \f{(K - K')Q = \textbf{0}} therefore \f{R - R' = \textbf{0}} and \f{R = R'}. But, this is a contradiction, as the polynomials were supposed to be different. We have proven that polynomial division produces unique quotient and remainder polynomials.

\f{Q} is a \def{divisor} of \f{P}, or \f{Q \given P} if the remainder of \f{P / Q} is the null polynomial. \f{\textbf{0} \given P}. \f{\textbf{1} \given P}. \f{P \given P}. The \def{divisibility relation}, or \f{\given} is a relation on \f{\mathcal{P}[\mathbb{R}]} which associates two polynomials if the first is a divisor of the second. \f{\given} is a partial order on \f{\mathcal{P}[\mathbb{R}]}.

\def{Bezout's Theorem}. Let \f{a} be a number in \f{\mathbb{R}}. The remainder of dividing \f{A} by the polynomial \f{x - a} is \f{P_f(a)}.

We know, by \ref{ExistanceUniqueness1stQuotientRemainderPolynomials} that we can write \f{P} as \f{(x - a)K + b}, where \f{b} is the remainer polynomial of degree \f{-\infty} or \f{0}. We then have \f{P_f(a) = (a - a)K_f(a) + b = b}, which is what we wanted.

\f{a} is a \def{root} of \f{P} if \f{P_f(a) = 0}. \f{a} is a \def{root of multiplicity} \f{m} for \f{P} if \f{P = (x - a)^m g(x)}, where \f{g} has degree \f{n - m} and does not have \f{a} as a root.

Let \f{\alpha} be a number. Some results about roots:
* If \f{\alpha > 0} then it has a square root.
* If \f{\alpha > 0} and \f{n} even then it has an \f{n^{\text{th}}} root.
* If \f{n} odd then it has an \f{n^{\text{th}}} root.
* If \f{n} is odd and \f{f(x) = x^n + a_{n-1}x^{n-1} + \dots + a_0} then \f{f(x)} has a root.
* If \f{n} is even and \f{f(x) = x^n + a_{n-1}x^{n-1} + \dots + a_0} then \f{f(x)} admits an \f{y \in \mathbb{R}} such that \f{f(y) \leq f(x)} for all \f{x}.
* If \f{n} is even, \f{f(x) = x^n + a_{n-1}x^{n-1} + \dots + a_0} and \f{c \in \mathbb{R}}, then there is a number \f{m} such that \f{f(x) = c} has a solution for \f{c \leq m} and has no solution for \f{c < m}.

For part \f{1}. A first proof. Notice that \f{x^2} is a continuous function, since \f{\lim_{x \to a}f(x) = a^2} for all \f{a}, by \ref{SomeExamplesContinuity}. Therefore, by \ref{IntermediateValueTheorems} we have that on any interval \f{[0,a]} and for any \f{c \in [0,f(a)]} we can find a \f{b \in [0,a]} such that \f{f(b) = c}, or \f{b^2 = c}. Furthermore, this number is unique, becuase \f{x^2} is bijective. A second proof. Suppose, by contradiction, that \f{\alpha} was such that it did not have a square root. That is, for all numbers \f{x > 0} we have \f{x^2 - \alpha \neq 0}. But, \f{x^2 - \alpha} is a continuous function, and it takes on negative values when \f{x^2 < \alpha} and positive ones when \f{x^2 > \alpha}. Therefore, by \ref{IntermediateValueTheorems} it follows that there must be an \f{x} such that \f{x^2 - \alpha = 0}, which is a contradiction. Part \f{2} is proven in exactly the same manner. For parts \f{2} and \f{3} the proof is similar. For part \f{4}. A first proof. The basic intuition is that the odd polynomial function  \f{f(x) = x^n + a_{n-1}x^{n-1} + \dots + a_0} will have \f{\lim_{x \to +\infty} f(x) = +\infty} and \f{\lim_{x \to -\infty} f(x) = -\infty}, and therefore, by \ref{IntermediateValueTheorems}, and since \f{0 \in \ran{f}} we will have an \f{x} such that \f{f(x) = 0}. By \ref{SomeImportantPartialLimits} we have that our limits indeed hold and, therefore, the root exists. A second proof. Following the same intuition, we will show that the \f{f} has in its range a positive and a negative value. Notice that \f{x^n} is positive for \f{x > 0} and negative for \f{x < 0}. We must select \f{x_1} so large that the \f{x^n} term dominates all the possibly negative others, and \f{x_2} so small that the \f{x^n} term dominates all the possibly positive others. Rewrite \f{f(x) = x^n(1 + a_n / x + \dots + a_0 / x^n)} and notice that \f{\abs{a_{n-1} / x + \dots + a_0 / x^n} \leq \abs{a_{n-1}} / \abs{x} + \dots + \abs{a_0} / \abs{x^n}}. If we pick a large enought \f{x}, say \f{\abs{x} > 1, 2n\abs{a_{n-1}},\dots, 2n\abs{a_0}}, then we first have \f{\abs{x^k} > \abs{x}} and second \f{\abs{a_{n-k}}/\abs{x^k} < \abs{a_{n-k}}/\abs{x} < \abs{a_{n-k}}/(2n\abs{a_{n-k}}) = 1/2n}. Then, we  have \f{\abs{a_{n-1} / x + \dots + a_0 / x^n} \leq 1/2n + \dots 1/2n} or \f{\abs{a_{n-1} / x + \dots + a_0 / x^n} \leq 1/2} or \f{-1/2 \leq a_{n-1} / x + \dots + a_0 / x^n \leq 1/2} or \f{1/2 \leq 1 + a_{n-1} / x + \dots + a_0 / x^n}. Then, for any \f{x > 0} which satisfies our other constraints \f{f} will be positive, while for \f{x < 0}, \f{f} will be negative. Thus, we've proven our point.

Let \f{a_1,a_2,\dots,a_k} be roots of \f{P}. Then \f{(x - a_1)(x - a_2)\cdots(x - a_n) \given P}. Alternatively, there exists \f{g}, a polynomial of degree \f{\deg{P} - k} such that \f{P = (x - a_1)(x - a_2)\cdots(x - a_n)g}.

\f{P} has as most \f{\deg{P}} roots.

Proof \f{1}: assume, by contradiction that \f{P} has more than \f{\deg{P}} roots, and let \f{a_1, a_2, \dots, a_m} with \f{m > \deg{P}} be them. We can then write, \f{P = \Pi_{i=1}^m(x - a_i)g(x)} where \f{g} does not have \f{a_1,\dots,a_m} as roots. But the polynomial on the right has degree larger than the polynomial \f{P}, therefore, we have a contradiction.
Proof \f{2}: using Calculus, see that if \f{x_1} and \f{x_2} are two adjacent roots of \f{P}, then, by Rolle's Theorem, there is an \f{x' \in (x_1,x_2)} such that \f{P_f'(x') = 0}. Therefore, if \f{P} has \f{k} different roots, \f{P'} has at least \f{k-1} different roots. Now, we will prove by induction that we can have at most \f{n} roots. For \f{P} of degree \f{1} this is obvious, as we only have one root. Assume the statement holds for \f{P} of degree \f{n}. Then, for \f{P} of degree \f{n+1}, if we had more than \f{n+1} roots, then \f{P'}, which is a polynomial of degree \f{n}, would have, by our previous reasoning, more than \f{n} roots, which is a contradiction. Therefore \f{P} must have at most \f{n+1} roots. By induction, a polynomial of degree \f{n} has at most \f{n} roots.

A polynomial of degree \f{n} is uniquely identified by \f{n + 1} of its values. Given \f{P(a_1), P(a_2), \dots, P(a_{n+1})}, a linear system can be built, probably with unique solution, which gives us our polynomial. Think about vector spaces. A polynomial with degree \f{n} has \f{n+1} coefficients. We need, by the previous theorem, \f{n+1} values of that polynomial to correctly identify it. Somewhere here, we have a change of basis, methinks.

= Numeric Functions =

A \def{numeric function} is a a function from \f{\mathbb{R}} to \f{\mathbb{R}}. The \def{set of numeric functions} over \f{\mathbb{R}}, or, \f{\mathcal{F}[\mathbb{R}]} is the set of all functions which can be built from \f{\mathbb{R}} to \f{\mathbb{R}}. Examples: \f{f(x) = 2x + 3}, \f{f(x) = x^2}, \f{f(x) = \sin{x^2}}, \f{(x) = 2^{\sin{x}^2}}.

The \def{null numeric function}, or \f{\textbf{0}} is the \def{constant function} which maps every number to \f{0}. The \df{identity numeric function}, or \f{\textbf{1}} is the \ref{constant function} which maps every number to \f{1}.

Let \f{f}, \f{g} and \f{h} be numeric functions from \f{\mathcal{F}[\mathbb{R}]}, for the rest of this section.

\f{f} is \def{non-null} if it does not map any number to \f{0}. This is a stronger condition than \f{f \neq \textbf{0}}. We say that \f{f} has a \def{null}, if it is not non-null.

The \def{sum} of \f{f} and \f{g}, or \def{addition} of \f{f} and \f{g}, or \f{f + g} is a binary operator on \f{\mathcal{F}[\mathbb{R}]}, which associates with each pair \f{(f,g)} the function \f{f + g \colon \mathcal{T} \rightarrow \mathcal{T}} and \f{(f + g)(x) = f(x) + g(x)}. The \def{product} of \f{f} and \f{g}, or \def{multiplication} of \f{f} and \f{g}, or \f{f \star g}, or\f{fg} is a binary operator on \f{\mathcal{F}[\mathbb{R}]}, which associates with each pair \f{(f,g)} the function \f{fg \colon \mathcal{T} \rightarrow \mathcal{T}} and \f{(fg)(x) = f(x)g(x)}.

Some properties of numeric function addition and multiplication:
* \f{f + (g + h) = (f + g) + h} - associativity of numeric function addition.
* \f{f + g = g + f} - commutativity of numeric function addition.
* \f{f + \textbf{0} = \textbf{0} + f = f} - \f{\textbf{0}} is the identity element for numeric function addition.
* \f{\exists (-f) \in \mathcal{F}[\mathbb{R}] \colon f + (-f) = \textbf{0}} - numeric function addition admits an additive inverse.
* \f{f(gh) = (fg)h} - associativity of numeric function multiplication.
* \f{fg = gh} - commutativity of numeric function multiplication.
* \f{A\textbf{1} = \textbf{1}A = A} - \f{\textbf{1}} is the identity element for numeric function multiplication.
* \f{f(g + h) = fg + fh} - distributivity of numeric function addition and multiplication.

The \def{substraction} of \f{f} and \f{g}, or \f{difference} of \f{f} and \f{g}, or \f{f - g} is a binary operator on \f{\mathcal{F}[\mathbb{R}]} which associates with each pair \f{(f,g)} the sum of \f{f} and the additive inverse of \f{g}.

Basic algebraic manipulations of numerical functions:
* \f{\textbf{0}f = \textbf{0}}.
* \f{f = g \Rightarrow f + h = g + h}.
* \f{fh = gh}.
* If \f{fh = gh} and \f{h} non-null then \f{f = g}.
* If \f{f - g = g - f} then \f{f = g}.
* \f{-(f + g) = (-f) + (-g)}.
* \f{-(fg) = -(f)g = f(-g)}.
* \f{-(-f) = f}.
* \f{-f = (-1)f}.
* If \f{fg} has a null then \f{f} has a null or \f{g} has a null.
* If \f{fg} non-null then \f{f} non-null and \f{g} non null.
* If \f{f} non-null and \f{g} non-null then \f{fg} non null.
* \f{(f + g)(h + q) = fh + fq + gh + gq}.

Addition and multiplication interact with \ref{composition}:
* \f{(f + g) \circ h = f \circ h + g \circ h}.
* \f{(fg) \circ h = (f \circ h)(g \circ h)}.

\f{f} is \def{even} if \f{f(x) = f(-x)}. Even functions care only about the magnitude of their inputs. \f{f} is \def{odd} if \f{f(x) = -f(-x)}. Odd functions care about the sign of their inputs, but in a limited way.

Evenness and oddness get preserved for addition and multiplication:
* If \f{f} is odd and \f{g} is odd then \f{f + g} is odd, \f{fg} is even and \f{f \circ g} is odd.
* If \f{f} is odd and \f{g} is even then \f{f + g} is neither even nor odd, \f{fg} is odd and \f{f \circ g} is even.
* If \f{f} is even and \f{g} is odd then \f{f + g} is neither even nor odd, \f{fg} is odd and \f{f \circ g} is even.
* If \f{f} is even and \f{g} is even then \f{f + g} is even, \f{fg} is even and \f{f \circ g} is even.

The \f{even-odd decomposition} of \f{f}, or \f{(f_E,f_O)} is a unique pair of functions with \f{f = f_E + f_O}, \f{f_E} is even, \f{f_O} is odd, \f{f_E(x) = (f(x) + f(-x)) / 2} and \f{f_O(x) = (f(x) - f(-x)) / 2}.

The \def{absolute value} of \f{f}, or \f{\abs{f}} is a function which maps its argument \f{x} to \f{\abs{f(x)}}.

The \def{maximum} of \f{f} and \f{g}, or \f{\max(f,g)} is function which maps its argument \f{x} to \f{\max(f(x),g(x))}. The \def{minimum} of \f{f} and \f{g}, or \f{\min(f,g)} is a function which maps its argument \f{x} to \f{\min(f(x),g(x))}.

\f{f} is \def{positive} if it maps its arguments to positive images. \f{f} is \def{nonnegative} if it maps its arguments to nonnegative images. \f{f} is \def{negative} if it maps its arguments to negative images. \f{f} is \def{positive} if it maps its arguments to nonpositive images.

The \de{positive-negative decomposition} of \f{f}, or \f{(f^+,f^-)} is a unique pair of functions \f{(f^+,f^-)} with \f{f = f^+ + f^-}, \f{f^+} is nonnegative, \f{f^-} is nonpositive, \f{f^+ = max(f,0)} and \f{f^- = min(f,0)}.

There exist functions \f{h} and \f{g} which are nonnegative such that \f{f = h - g}. This is positive-positive decomposition of \f{f}, which is useful sometimes. It is not unique, however.

\f{f} is \def{increasing} if the ordering of images is the same as that of arguments (\f{x < y \Rightarrow f(x) < f(y)}). \f{f} is \def{nondecreasing} if the ordering of images is the same as that of arguments or if they are equal (\f{x < y \Rightarrow f(x) \leq f(y)}). \f{f} is \def{decreasing} if the ordering of images is the reverse as that of arguments (\f{x < y \Rightarrow f(x) \geq f(y)}). \f{f} is \def{nonincreasing} if the ordering of images is the reverse as that of arguments or if they are equal (\f{x < y \Rightarrow f(x) \geq f(y)}). \f{f} is \def{monotonous} if \f{f} is either increasing or decreasing. \f{f} is \def{weakly monotonous} if \f{f} is either nonincreasing or nondecreasing.

Let \f{f} be a \ref{bijective} function. Constraints on monotonicity and the inverse:
* \f{f} must be either increasing or decreasing.
* If \f{f} is increasing then \f{f^{-1}} is increasing.
* If \f{f} is decreasing then \f{f^{-1}} is decreasing.

Interval properties for nondecreasing and nonincreasing functions:
* If \f{f} is nondecreasing then there is a closed interval \f{A \subset \dom{f}} on which \f{f} is constant.
* If \f{f} is nonincreasing then there is a closed interval \f{A \subset \dom{f}} on which \f{f} is constant.

For part \f{1}, by definition, there exist \f{x,y \in \dom{f}} with \f{x < y} such that \f{f(x) \geq f(y)}. This means that for any \f{z \in [x,y]} we must have that \f{f(z) = f(x) = f(y)}, otherwise, if \f{f(z) < f(x) = f(y)} we have a contradiction because \f{f} is nondecreasing and if \f{f(z) > f(x) = f(y)} we have a contradiction because \f{f} is nondecreasing. Therefore, the interval is \f{[a,b]}.
For part \f{2}, the proof is similar to that for part \f{1}.

Let \f{f} and \f{g} be (strict) monotonic functions. Interactions between function operators and monotonicity:
* \f{f + g} has the same (strict) monotonicity as \f{f} and \f{g}.
* If \f{f} or \f{g} are positive then \f{fg} has the same (strict) monotonicity as \f{f} and \f{g}.
* \f{f \circ g} has the same (strict) monotonicity as \f{f} and \f{g}.

\f{f} is \def{periodic} if there exists a number \f{T} such that \f{f(x) = f(x + T)}. \f{T} is called the \def{period} of \f{f}.

\f{f} is \def{additive} if it maps an argument which is a sum of two terms to the sum of the images of each respective term (\f{f(x + y) = f(x) + f(y)}). \f{f} is \def{homogenous} if it maps an argument which is the product of two terms into the product of the first term and the image of the second term (\f{f(cx) = cf(x)}). \f{f} is \def{linear} if it is additive and homogenous.

Let \f{A \subseteq \dom{f}} be a subset of the domain of \f{f}.

\f{f} is \def{bounded above} on \f{A} if the set \f{f(A)} is \ref{other:bounded above}. \f{f} \def{bounded below} on \f{A} if the set \f{f(A)} is \ref{other:bounded below}.

A number \f{x} is the \def{maximum} of \f{f} if for all \f{y \in \dom{f} \subseteq \mathbb{R}} we have \f{f(x) \geq f(y)} - \f{f(x)} is the largest value \f{f} takes. A number \f{x} is the \def{minimum} of \f{f} if for all \f{y \in \dom{f} \subseteq \mathbb{R}} we have \f{f(x) \leq f(y)} - \f{f(x)} is the smallest value \f{f} takes. A number \f{x} is a \def{local maximum} of \f{f} if there exists \f{\delta > 0} such that for all \f{y \in (x - \delta,x + \delta) \subseteq \dom{f} \subseteq \mathbb{R}} we have \f{f(x) \geq f(y)} - \f{f(x)} is the largest value \f{f} takes in an interval. A point \f{x} is a \def{local minimum} of \f{f} if there exists \f{\delta > 0} such that for all \f{y \in (x - \delta,x + \delta) \subseteq \dom{f} \subseteq \mathbb{R}} we have \f{f(x) \leq f(y)} - \f{f(x)} is the smallest value \f{f} takes in an interval.

The \def{graph} of \f{f} is a set of points \f{\{(x,f(x)) \given x \in \mathbb{R}\}} in the plane \f{\mathbb{R}^2}. This can be visualized to get an understanding of the kind of mapping the functions does. The \def{epigraph} of \f{f}, or \f{A_f} is set of point \f{\{(x,y) \given y \geq f(x)\}} in the plane \f{\mathbb{R}^2}. This is the set of points above the graph of \f{f}.

The graph of an even function is symmetric around the vertical axis. This is because such functions care only about the magnitude of the argument. Example graphs can be seen in Figures \ref{figure:NumericFunctionsEven1}, \ref{figure:NumericFunctionsEven2} and \ref{figure:NumericFunctionsEven3}. The graph of an odd function is symmetric around the origin, or rather, the graph in the left side is a mirror of the graph in the right side, but with its sign changed. Example graphs can be seen in Figures \ref{figure:NumericFunctionsOdd1}, \ref{figure:NumericFunctionsOdd2} and \ref{figure:NumericFunctionsOdd3}. The graph of a bijective function intersects every horizontal line exactly once. Example graphs can be seen in Figures \ref{figure:NumericFunctionsBijective1}, \ref{figure:NumericFunctionsBijective2} and \ref{figure:NumericFunctionsBijective3}. The graph of the inverse of a numeric function is the symmetric version of the original graph around the diagonal. Example graphs can be seen in Figures \ref{figure:NumericFunctionsInverse1}, \ref{figure:NumericFunctionsInverse2} and \ref{figure:NumericFunctionsInverse3}.

There are some figure cells here which are unimplemented yet.
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = x^2}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = x.^2;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsEven1}
}
\subfigure[\f{f(x) = \abs{x}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = abs(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsEven2}
}
\subfigure[\f{f(x) = \frac{\sin{x}}{x}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = sin(x) ./ x;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsEven3}
}
\subfigure[\f{f(x) = 3x}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 3*x;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsOdd1}
}
\subfigure[\f{f(x) = \sin{x}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -2*pi:0.01:2*pi;
y = sin(x);

plot(x,y,'LineWidth',5);
axis([-2*pi 2*pi -2 4]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsOdd2}
}
\subfigure[\f{f(x) = \frac{1}{x}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 1 ./ x;

hold on;
plot(x(1:299),y(1:299),'LineWidth',5);
plot(x(301:end),y(301:end),'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsOdd3}
}
\caption{Even and odd functions.}
\end{figure}

\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = 3x + 2}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 3*x + 2;

plot(x,y,'LineWidth',5);
axis([-3 3 -3 3]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsBijective1}
}
\subfigure[\f{f(x) = x^3}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = x.^3;

plot(x,y,'LineWidth',5);
axis([-3 3 -3 3]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsBijective2}
}
\subfigure[\f{f(x) = e^x}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = exp(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -3 3]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsBijective3}
}
\subfigure[\f{f(x) = 3x + 2} \& \f{f^{-1}(x) = \frac{1}{3}x - \frac{2}{3}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 3*x + 2;
xi = -3:0.01:3;
yi = 1/3*x - 2/3;
diag_x = -3:0.01:3;
diag_y = diag_x;

hold on;
plot(x,y,'LineWidth',3,'r');
plot(xi,yi,'LineWidth',5);
plot(diag_x,diag_y,'LineWidth',2,'k');
hold off;
axis([-3 3 -3 3]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsInverse1}
}
\subfigure[\f{f(x) = x^3} \& \f{f^{-1}(x) = x^{\frac{1}{3}}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = x.^3;
xi = -3:0.01:3;
yi = sign(x) .* abs(x).^(1/3);
diag_x = -3:0.01:3;
diag_y = diag_x;

hold on;
plot(x,y,'LineWidth',3,'r');
plot(xi,yi,'LineWidth',5);
plot(diag_x,diag_y,'LineWidth',2,'k');
hold off;
axis([-3 3 -3 3]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsInverse2}
}
\subfigure[\f{f(x) = e^x} \& \f{f^{-1}(x) = \log{x}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = exp(x);
xi = 0.01:0.01:3;
yi = log(xi);
diag_x = -3:0.01:3;
diag_y = diag_x;

hold on;
plot(x,y,'LineWidth',3,'r');
plot(xi,yi,'LineWidth',5);
plot(diag_x,diag_y,'LineWidth',2,'k');
hold off;
axis([-3 3 -3 3]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsInverse3}
}
\caption{Bijective functions and their inverses.}
\end{figure}
}

== Convex Functions ==

Let \f{f}, \f{g} and \f{h} be numeric functions from \f{\mathcal{F}[\mathbb{R}]}, \f{A \subseteq{\dom{f}}} be a subset of \f{f}'s domain, for the rest of this section.

\f{f} is \def{convex} on \f{A} if for all \f{a,b \in A} with \f{a < b} we have that the line segment joining \f{(a,f(a))} and \f{(b,f(b))} lies above the graph of \f{f}, or for all \f{a, b, x \in A} with \f{a < x < b} we have \f{(f(x) - f(a)) / (x - a) < (f(b) - f(a)) / (b -a )} \f{given} For all \f{a, b \in A} and \f{t \in (0,1)} with \f{a < b} we have \f{f(ta + (1-t)b) < tf(a) + (1-t)f(b)}. \f{f} is \def{concave} on \f{A} if for all \f{a,b \in A} with \f{a < b} we have that the line segment joining \f{(a,f(a))} and \f{(b,f(b))} lies below the graph of \f{f}, or for all \f{a, b, x \in A} with \f{a < x < b} we have \f{(f(x) - f(a)) / (x - a) > (f(b) - f(a)) / (b -a )} \f{given} For all \f{a, b \in A} and \f{t \in (0,1)} with \f{a < b} we have \f{f(ta + (1-t)b) > tf(a) + (1-t)f(b)}.

The definitions of convexity and concavity are equivalent:
* The three definitions of convexity for \f{f} are equivalent.
* The three definitions of concavity for \f{g} are equivalent.
* \f{-f} is concave.
* \f{-g} is convex.

For part \f{1}, it is sufficient to show that the first definition implies the second, the second implies the third and the third implies the first. First, we will show that the first definition implies the second. If the graph of \f{f} lies below the graph of the line which passes through \f{(a,f(a)} and \f{(b,f(b))} and which is given by \f{s_{a,b}(x) = (f(b) - f(a)) / (b - a) (x - a) + f(a)}, we have that \f{f(x) < (f(b) - f(a)) / (b - a) (x - a) + f(a)}, for any \f{x \in (a,b)}. This means that \f{f(x) - f(a) < (f(b) - f(a)) / (b - a) (x - a)} or \f{(f(x) - f(a)) / (x - a) < (f(b) - f(a)) / (b - a)}, since \f{x - a > 0}. This is exactly the second definition. Second, we will show that the second definition implies the third. Let \f{a,b \in A} with \f{a < b} and let \f{t \in (0,1)}. The number \f{ta + (1 - t)b} is a convex combination of \f{a} and \f{b} and lies between the two, such that \f{a < ta + (1 - t)b < b}. Therefore, we have that \f{(f(ta + (1 - t)b) - f(a)) / (ta + (1 - t)b - a) < (f(b) - f(a)) / (b - 1)} or \f{f(ta + (1 - t)b) < (f(b) - f(a)) / (b - a) (ta + (1 - t)b - a) + f(a)} (going backwards from the first part of this proof) or \f{f(ta + (1 - t)b) < (f(b) - f(a)) / (b - a) ((1-t)b - (1-t)a) + f(a)} or \f{f(ta + (1-t)b) < (f(b) - f(a)) (1 - t) + f(a)} or \f{f(ta + (1-t)b) < tf(a) + (1 - t)f(b)}. This is exactly the third definition. Third, we will show that the third definition imples the first. Let \f{a,b \in A} with \f{a < b}. The line which passes through \f{(a,f(a))} and \f{(b,f(b))} is \f{s_{a,b}(x) = (f(b) - f(a)) / (b - a)(x - a) + f(a)}. We want to prove that for all \f{x \in (a,b)} we have \f{f(x) < s_{a,b}(x)}. If we express \f{x} as a convex combination of \f{a} and \f{b}, or, with \f{t \in (0,1)} as \f{ta + (1 - t)b}, we get that \f{s_{a,b}(ta + (1-t)b) = tf(a) + (1-t)f(b)} and we have that \f{f(ta + (1-t)b) < tf(a) + (1-t)f(b)} or \f{f(x) < s_{a,b}(x)}. This is exactly the first definition.
For part \f{2}, the proof is similar to that for part \f{1}.
For part \f{3}, we must prove that for all \f{a,b, x \in A} with \f{a < x < b} we have \f{((-f)(x) - (-f)(a)) / (x - a) > ((-f)(b) - (-f)(a)) / (b - a)}. Let \f{a, b, x \in A} with \f{a < x < b}. We have \f{(f(x) - f(a)) / (x - a) < (f(b) - f(a)) / (b - a)}. If we multiply the left and right sides by \f{-1} we get \f{-(f(x) - f(a)) / (x - a) > -(f(x) - f(a)) / (x - a)} or \f{(-f(x) - (-f(a))) / (x - a) > (-f(b) - (-f(a))) / (b - a)} or \f{((-f)(x) - (-f)(a)) / (x - a) > ((-f)(b) - (-f)(a)) / (b - a)}, which is what we wanted. Therefore, \f{-f} is concave.
For part \f{4}, the proof is similar to that for part \f{3}.

\f{f} is \def{weakly convex} on \f{A} if for all \f{a,b \in A} with \f{a < b} we have that the line segment joining \f{(a,f(a))} and \f{(b,f(b))} lies above the graph of \f{f} or is equal to the graph of \f{f}, or for all \f{a, b, x \in A} with \f{a < x < b} we have \f{(f(x) - f(a)) / (x - a) \leq (f(b) - f(a)) / (b -a )} \f{given} For all \f{a, b \in A} and \f{t \in (0,1)} with \f{a < b} we have \f{f(ta + (1-t)b) \leq tf(a) + (1-t)f(b)}. \f{f} is \def{weakly concave} on \f{A} if for all \f{a,b \in A} with \f{a < b} we have that the line segment joining \f{(a,f(a))} and \f{(b,f(b))} lies below the graph of \f{f} or is equal to the graph of \f{f}, or for all \f{a, b, x \in A} with \f{a < x < b} we have \f{(f(x) - f(a)) / (x - a) \geq (f(b) - f(a)) / (b -a )} \f{given} For all \f{a, b \in A} and \f{t \in (0,1)} with \f{a < b} we have \f{f(ta + (1-t)b) \geq tf(a) + (1-t)f(b)}.

Similar results as for convex and concave functions:
* The three definitions of weak convexity for \f{f} are equivalent.
* The three definitions of weak concavity for \f{g} are equivalent.
* \f{-f} is weak concave.
* \f{-g} is weak convex.

If \f{f} is increasing then \f{f \circ g} is convex - If \f{f(x) = x^2} and \f{g(x) = x^2 - 1}, \f{f \circ g} is no longer convex, so we need the assumption that \f{f} is increasing. The same applies to \ref{weak convexity}.

We have to prove that for all \f{x,y \in A} and \f{t \in (0,1)} we have \f{f(g(tx + (1 - t)y)) < tf(g(x)) + (1-t)f(g(y))}. Because \f{g} is convex we have that \f{g(tx + (1 - t)y) < tg(x) + (1 - t)g(y)}. Because \f{f} is increasing we have \f{f(g(tx + (1-t)y)) < f(tg(x) + (1 - t)g(y))}. Becauser \f{f} is convex we have \f{f(tg(x) + (1-t)g(y)) < tf(g(x)) + (1-t)f(g(y))}. Therefore, we have \f{f(g(tx + (1-t)y)) < tf(g(x)) + (1-t)f(g(y))}, which is what we wanted.

If \f{f} is weakly convex and if the graph of \f{f} does not contain any straight lines then \f{f} is convex.

Assume that the graph of \f{f} does not contain any straight line but that \f{f} is not convex, by contradiction. Therefore, there exists \f{a,b,c \in \dom{f}} such that \f{(f(b) - f(a)) / (b - a) = (f(c) - f(a)) / (c - a)}. By a geometric argument [draw a picture] we have that \f{(f(b) - f(a)) / (b - a) = (f(c) - f(b)) / (c - b)} as well. Since there are no straight lines in the graph of \f{f}, there must exist an \f{x \in (b,c)} such that \f{(f(x) - f(a)) / (x - a) < (f(c) - f(b)) / (c - b)} or \f{(f(x) - f(a)) / (x - a) < (f(b) - f(a)) / (b - a)}. But, because \f{a < b < x} we must also have \f{(f(b) - f(a)) / (b - a) \leq (f(x) - f(a)) / (x - a)}. But this is a contradiction. Therefore, \f{f} must be convex.

If \f{f} is weakly convex and \f{(x_1,y_1), (x_2,y_2) \in A_f} then \f{A_f} contains the line segment betweeen \f{(x_1,y_1)} and \f{(x_2,y_2)}.

Let \f{y \in (x_1,x_2)}. We want to prove that \f{s_{x_1,x_2}(y) \geq f(y)}, where \f{s_{x_1,x_2}(y) = (y_2 - y_1) / (x_2 - x_1) (y - x_1) + y_1} is the line segment between \f{(x_1,y_1)} and \f{(x_2,y_2)}. We have that \f{y_1 \geq f(x_1)} and \f{y_2 \geq f(x_2)}. This means that the line segment between \f{(x_1,y_1)} and \f{(x_2,y_2)} and \f{(x_1,f(x_1))} and \f{(x_2,f(x_2))} either do not intersect, intersect at only one end, or are equal. In any case we have \f{s_{x_1,x_2}(y) \geq s_{x_1,x_2}'(y)} (where \f{s_{x_1,x_2}} is the line segment between \f{(x_1,f(x_1))} and \f{(x_2,f(x_2))}). Because \f{f} is convex, then the line segment between \f{(x,f(x_1))} and \f{(y,f(x_2))} lies above or is equal to \f{f}, therefore \f{s_{x_1,x_2}(y) \geq f(y)}, which means that \f{(y,s_{x_1,x_2}(y)) \in A_f}, which is what we wanted to prove.

It is intuitive to see how certain modifications affect the graph of functions. Figure \ref{figure:NumericFunctionsModifications} shows several common types of modifications.

Another set of commented figures:
\comment{
\begin{figure}
\subfigure[\f{f(x) = 2x^2 + x} - original] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*x.^2 + x;

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(x) + c} - cod. shift (\f{c = 3})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*x.^2 + x + 3;

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(x) + c} - cod. shift (\f{c = -3})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*x.^2 + x - 3;

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(x + c)} - dom. shift (\f{c = 1})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*(x + 1).^2 + (x + 1);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(x + c)} - dom. shift (\f{c = -1})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*(x - 1).^2 + (x - 1);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{cf(x)} - cod. scale (\f{c = 2})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*(2*x.^2 + x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{cf(x)} - cod. scale (\f{c = \frac{1}{2}})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 0.5*(2*x.^2 + x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{cf(x)} - cod. scale (\f{c = -2})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = -2*(2*x.^2 + x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(cx)} - dom. scale (\f{c = 2})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*(2*x).^2 + (2*x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(cx)} - dom. scale (\f{c = \frac{1}{2}})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*(0.5*x).^2 + (0.5*x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(cx)} - dom. scale (\f{c = -2})] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*(-2*x).^2 + (-2*x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{\abs{f(x)}} - cod. absolute value] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = abs(2*x.^2 + x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f(\abs{x})} - dom. absolute value] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = 2*abs(x).^2 + abs(x);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f^+(x)} - positive part] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = max(2*x.^2 + x,0);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\subfigure[\f{f^-(x)} - negative part] {
== drawing-beg [width=0.3\textwidth] ==
x = -3:0.01:3;
y = min(2*x.^2 + x,0);

hold on;
plot(x,y,'LineWidth',5);
axis([-3 3 -5 5]);
grid on;
== drawing-end ==
}
\caption{The effect of different modifications to functions.}
\label{fig:NumericFunctionsModifications}
\end{figure}
}

= Well Known Functions =

== Constant ==

Let \f{c} be a number.

\def{Constant functions} are functions of the form \f{f(x) = c}. Example graphs can be seen in Figures \ref{figure:NumericFunctionsConstant1},\ref{figure:NumericFunctionsConstant2} and \ref{figure:NumericFunctionsConstant3}.

A set of commentet figures:
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = 1}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = ones(length(x),1);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsConstant1}
}
\subfigure[\f{f(x) = -1}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = -1*ones(length(x),1);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsConstant2}
}
\subfigure[\f{f(x) = 2}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 2*ones(length(x),1);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsConstant3}
}
\caption{Different types of constant functions.}
\end{figure}
}

Let \f{f(x) = c} be a constant function and \f{[a,b]} an interval.
* \f{f} is even.
* If \f{c = 0} then \f{f} is odd.
* If \f{c > 0} then \f{f} is positive.
* If \f{c < 0} then \f{f} is negative.
* \f{f} is upper bounded by \f{c}, lower bounded by \f{c} and has \f{c} as a maximia and minima.
* For all \f{y \in \mathbb{R}} we have \f{\lim_y f(x) = c}.
* \f{\lim_{+\infty} f(x) = c}.
* \f{\lim_{-\infty} f(x) = c}.
* \f{f} is continuous everywhere.
* \f{f} is uniformly continuous on \f{\mathbb{R}}.
* \f{f} is differentiable everywhere.
* \f{f'(x) = 0}.
* \f{f} is both weak-convex and weak-concave.
* \f{\int_a^b f = c(b - a)} is integrable on \f{[a,b]}.

For part \f{1}, we have that \f{f(-x) = c = f(x)}, therefore \f{f} is even.
For part \f{2}, we have that \f{f(-x) = 0 = -f(x)}, therefore \f{f} is odd.
For part \f{3}, we have that for all \f{x \in \mathbb{R}} we have \f{f(x) = c > 0}, therefore \f{f} is positive.
For part \f{4}, the proof is similar to that for part \f{3}.
For part \f{5}, the proof is obvious.
For part \f{6}, let \f{\epsilon > 0}. We can choose any \f{\delta} because for all \f{x \in (y - \delta,y + \delta)\setminus\{y\}} we have \f{\abs{f(x) - c} = \abs{c - c} = 0 < \epsilon}. Therefore, according to the definition, we have \f{\lim_y f(x) = c}.
For part \f{7}, let \f{\epsilon > 0}. We can choose any \f{N} because for all \f{x > N} we have \f{\abs{f(x) - c} = \abs{c - c} = 0 < \epsilon}. Therefore, according to the definition, we have \f{\lim_{+\infty} f(x) = c}.
For part \f{8}, the proof is similar to that for part \f{7}.
For part \f{9}, since for all \f{y \in \mathbb{R}} we have \f{\lim_y f(x) = f(y) = c} we have that \f{f} is continuous everywhere.
For part \f{10}, let \f{\epsilon > 0}. We can choose any \f{\delta > 0} such that for all \f{x,y \in \mathbb{R}} with \f{\abs{x - y} < \delta} we have \f{\abs{f(x) - f(y)} = \abs{c - c} = 0 < \epsilon}. Therefore, \f{f} is uniformly continuous on \f{\mathbb{R}}.
For part \f{11}, let \f{y \in \mathbb{R}}. The limit \f{\lim^h_0(f(y + h) - f(y)) / h = \lim^h_0 (c - c) / h = \lim^h_0 0 = 0} obviously exists, therefore \f{f} is differentiable everywhere.
For part \f{12}, we have from part \f{11} that \f{f'} exists for all points in the domain and \f{f'(x) = 0} as well.
For part \f{13}, let \f{x,y,z \in \mathbb{R}} with \f{x < y < z}. We have \f{(f(y) - f(x)) / (y - x) \leq (f(z) - f(x)) / (z - x)} or \f{0 / (y - x) \leq 0 / (z - z)} or \f{0 \leq 0} which is true, therefore \f{f} is weak-convex. Similarly, \f{f} is weak-concave.
For part \f{14}, consider a partition \f{P = \{p_1,\dots,p_n\}}. We have \f{L[f,P] = \sum_{i=1}^n m_i (p_i - p_{i-1}) = \sum_{i=1}^n c (p_i - p_{i-1}) = c (b - a)}. Similarly we have \f{U[f,P] = c (b - a)}. Since this holds for any partition, we have \f{\int_a^b f = c(b-a)}.

== Linear ==

\section{Linear}

Let \f{a} and \f{b} be numbers with \f{a \neq 0}.

\def{Linear functions} are functions of the form \f{f(x) = ax + b}. The name comes from the fact that these functions have a line as their graph, and not because they have the linearity property. Example graphs can be seen in figures \ref{figure:NumericFunctionsLinear1}, \ref{figure:NumericFunctionsLinear2} and \ref{figure:NumericFunctionsLinear3}.

Constant functions are a special case of linear functions (with \f{a = 0}).

A set of commented figures:
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = 2x + 1}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 2*x + 1;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsLinear1}
}
\subfigure[\f{f(x) = -x + 2}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = -x + 2;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsLinear2}
}
\subfigure[\f{f(x) = 2x - 3}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 2*x - 1;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsLinear3}
}
\caption{Different types of linear functions.}
\end{figure}
}

Let \f{f(x) = ax + b} be a linear function. Then:
* If \f{b = 0} then \f{f} is odd.
* If \f{a > 0} then \f{f} is increasing.
* If \f{a < 0} then \f{f} is decreasing.
* If \f{b = 0} then \f{f} is linear.
* \f{x = -b/a} is a root for \f{f}.
* For all \f{y \in \mathbb{R}} we have \f{\lim_y f(x) = ay + b}.
* If \f{a > 0} then \f{\lim_{+\infty} f(x) = +\infty}.
* If \f{a < 0} then \f{\lim_{+\infty} f(x) = -\infty}.
* If \f{a > 0} then \f{\lim_{-\infty} f(x) = -\infty}.
* If \f{a < 0} then \f{\lim_{-\infty} f(x) = +\infty}.
* \f{f} is continuous everywhere.
* \f{f} is uniformly continuous on \f{\mathbb{R}}.
* \f{f} is differentiable everywhere.
* \f{f'(x) = a}.
* \f{f} is bijective.
* \f{f^{-1}(x) = (1/a) x - (b/a)}.
* \f{f} is both weak-convex and weak-concave.

For part \f{1}, we have that \f{f(-x) = a(-x) = -ax = -f(x)}, therefore \f{f} is odd.
For part \f{2}, let \f{x,y \in \mathbb{R}} with \f{x < y}. Then \f{f(x) < f(y)} or \f{ax + b < ay + b} or \f{ax < ay}. Since \f{a > 0} this results in \f{x < y}, which is true. Therefore, \f{f} is increasing.
For part \f{3}, the proof is similar to that for part \f{2}.
For part \f{4}, let \f{x,y,c \in \mathbb{R}}. We have that \f{f(x + y) = a(x + y) = ax + ay = f(x) + f(y)}, therefore \f{f} is additive and \f{f(cx) = acx = cax = cf(x)}, therefore \f{f} is homogenous. In the end \f{f} is linear.
For part \f{5}, notice that \f{f(-b/a) = a(-b/a) + b = -b + b = 0}, therefore \f{x = -b/a} is a root for \f{f}.
For part \f{6}, consider first the case of \f{\overline{f}(x) = x}. Let \f{\epsilon > 0}. We can choose \f{\delta < \epsilon}. Then, we will have for all \f{x} with \f{0 < \abs{x - y} < \delta < \epsilon} that \f{\abs{\overline{f}(x) - y} = \abs{x - y}} which is strictly smaller than \f{\delta} and strictly smaller than \f{\epsilon}. Therefore, according to the definition, we have \f{\lim_y \overline{f}(x) = y}. Since we're dealing with finite limits, we have that \f{\lim_y f(x) = \lim_y a\overline{f}(x) + b = ay + b}, which is what we wanted. Therefore, we have \f{\lim_y f(x) = ay + b}.
For part \f{7}, let \f{M > 0}. We have to find an \f{N} such that for all \f{x > N} we have \f{f(x) > M}. If we choose \f{N = (1/a) M - (b/a)} then for \f{x > N} means \f{x > (1/a) M - (b/a)} or \f{ax > M - b}, since \f{a > 0}, or \f{ax + b > M} or \f{f(x) > M}, which is what we wanted. Therefore, \f{\lim_{+infty}f(x) = +\infty}.
For parts \f{8}, \f{9} and \f{10} the proof is similar to that for part \f{7}.
For part \f{11}, since for all \f{y \in \mathbb{R}} we have that \f{\lim_y f(x) = ay + b = f(y)} we have that \f{f} is continuous everywhere.
For part \f{12}, let \f{\epsilon > 0}. We have to choose \f{\delta > 0} such that for all \f{x, y \in \mathbb{R}} with \f{\abs{x - y} < \delta} we have \f{\abs{f(x) - f(y)} < \epsilon}. Let \f{\delta = \epsilon/\abs{a} > 0}. Then we have \f{\abs{f(x) - f(y)} = \abs{ax + b - ay - b} = \abs{a}\abs{x - y}}. But, since \f{\abs{x - y} < \epsilon/\abs{a}} we get \f{\abs{a}\abs{x - y} < \abs{a}\epsilon/\abs{a}} or \f{\abs{a}\abs{x - y} < \epsilon}, therefore \f{\abs{f(x) - f(y)} < \epsilon}, which is what we wanted. Therefore, \f{f} is uniformly continous on \f{\mathbb{R}}.
For part \f{13}, let \f{y \in \mathbb{R}}. The limit \f{\lim^h_0(f(y + h) - f(y)) / h = \lim^h_0 (a(y + h) - b - ay - h) / h = \lim^h_0 (ay + ah - ay) / h = \lim^h_0 ah / h = \lim^h_0 a = a} obviously exits, therefore \f{f} is differentiable everywhere.
For part \f{14}, we have from part \f{13} that \f{f'} exists for all points in the domain and \f{f'(x) = a} as well.
For part \f{15}, let \f{x, y \in \mathbb{R}}. Then if \f{f(x) = f(y)} we have \f{ax + b = ay + b} or \f{ax = ay} or \f{x = y}, since \f{a \neq 0}, which means \f{f} is injective. Similarly, for each \f{y \in \mathbb{R}} we can find an \f{x = (1/a) y - (b/a)} such that \f{f(x) = y}, therefore \f{f} is surjective.
For part \f{16}, see the proof of part \f{15}.
For part \f{17}, let \f{x, y, z \in \mathbb{R}} with \f{x < y < z}. We have \f{(f(y) - f(x)) / (y - x) \leq (f(z) - f(x)) / (z - x)} or \f{(ay + b - ax - b) / (y - x) \leq (az + b - ax - b) / (z - x)} or \f{a \leq a}, which is true, therefore \f{f} is weak-convex. Similarly, \f{f} is weak-concave.

== Quadratic ==

\section{Quadratic}

Let \f{a, b \text{ and } c} be numbers with \f{a \neq 0}.

\def{Quadratic functions} are functions of the form \f{f(x) = ax^2 + bx + c}. Example graphs can be seen in figures \ref{figure:NumericFunctionsQuadratic1}, \ref{figure:NumericFunctionsQuadratic2} and \ref{figure:NumericFunctionsQuadratic3}.

Linear functions are a special case of quadratic functions (with \f{a = 0}).

Another set of commented figures:
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = 2x^2 + 3x + 1}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 2*x.^2 + 3*x + 1;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsQuadratic1}
}
\subfigure[\f{f(x) = -x^2 + 3x + 2}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = -x.^2 + 3*x + 2;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsQuadratic2}
}
\subfigure[\f{f(x) = x^2 + 1}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = x.^2 + 1;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsQuadratic3}
}
\caption{Different types of quadratic functions.}
\end{figure}
}

Let \f{f(x) = ax^2 + bx + c} be a quadratic function. Then
* If \f{b = 0} then \f{f} is even.
* [talk about conditions for positive/negative/etc.]
* If \f{a > 0} then \f{f} is decreasing on \f{(-\infty,minimum]} and increasing on \f{[minimum,+\infty)}.
* If \f{a < 0} then \f{f} is increasing on \f{(-\infty,minimum]} and decreasing on \f{[minimum,+\infty)}.
* [Talk about roots here]
* [talk about lower bounds/maxima when \f{a > 0} and upper bounds/minima when \f{a < 0}]
* For all \f{y \in \mathbb{R}} we have \f{\lim_y f(x) = ay^2 + by + c}.
* If \f{a > 0} then \f{\lim_{+\infty} f(x) = +\infty}.
* If \f{a < 0} then \f{\lim_{+\infty} f(x) = -\infty}.
* If \f{a > 0} then \f{\lim_{-\infty} f(x) = +\infty}.
* If \f{a < 0} then \f{\lim_{-\infty} f(x) = -\infty}.
* \f{f} is continuous everywhere.
* \f{f} is uniformly continuous on \f{\mathbb{R}}.
* \f{f} is differentiable everywhere.
* \f{f'(x) = 2ax}.
* \f{f"(x) = 2a}.
* The restriction of \f{f} on \f{[minimum,+\infty)} is bijective.
* \f{f^{-1}(x) = (1/a) x - (b/a)} for the restriction.
* If \f{a > 0} then \f{f} is convex.
* If \f{a < 0} then \f{f} is concave.

== Polynomial ==

Let \f{a_n,a_{n-1},\dots,a_1 \text{ and } a_0} be numbers with \f{a_n \neq 0}.

\def{Polynomial functions} are functions of the form \f{f(x) = a_nx^n + a_{n-1}x^{n-1} + \dots + a_1x + a_0} with \f{a_n \neq 0}. These are the functions attached to polynomials. Example graphs can be seen in figures \ref{figure:NumericFunctionsPolynomial1}, \ref{figure:NumericFunctionsPolynomial2} and \ref{figure:NumericFunctionsPolynomial3}.

Quadratic functions are a special case of polynomial functions (with \f{n = 2}).

Pictures of polynomials:
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = x^3}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = x.^3;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsPolynomial1}
}
\subfigure[\f{f(x) = \frac{1}{10}x^3 + 3x + 2}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = 0.1 * x.^3 + 3*x + 2;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsPolynomial2}
}
\subfigure[\f{f(x) = -x^4 + 3x^2 + \frac{2}{7}x + 1}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = -x.^4 + 3*x.^2 + 2/7*x + 1;

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsPolynomial3}
}
\caption{Different types of polynomial functions.}
\end{figure}
}

Let \f{f(x) = x^n} be a \ref{monomial} function. Then:
* If \f{n} is even then \f{f} is even.
* If \f{n} is odd then \f{f} is odd.
* If \f{n} is even then \f{f} is decreasing on \f{(-\infty,0]} and increasing on \f{[0,+\infty)}.
* If \f{n} is odd then \f{f} is increasing.
* \f{x = 0} is the single root for \f{f}.
* If \f{n} is even then \f{0} is a lower bound and the minimum of \f{f}.
* If \f{n} is odd then \f{0} is an upper bound and the maximum of \f{f} on \f{(-\infty,0]} and a lower bound and the minimum of \f{f} on \f{[0,+\infty)}.
* For all \f{y \in \mathbb{R}} we have \f{\lim_y f(x) = y^n}.
* If \f{n} is even then \f{\lim_{+\infty} f(x) = +\infty}.
* If \f{n} is even then \f{\lim_{-\infty} f(x) = +\infty}.
* If \f{n} is odd then \f{\lim_{+\infty} f(x) = +\infty}.
* If \f{n} is odd then \f{\lim_{-\infty} f(x) = -\infty}.
* \f{f} is continuoues everywhere.
* Let \f{a < b} be numbers then \f{f} is uniformly continuous on \f{(a,b)}.
* \f{f} is infinitely differentiable everywhere.
* \f{f^{(k)}(x) = \Pi_{i=0}^{k}(n-i) x^{n-k}} for \f{k \leq n} and \f{f^{(k)}(x) = 0} for \f{k > n}.
* \f{f'(x) = nx^{n-1}}.
* \f{f^{(n)}(x) = n!}.
* If \f{n} is even then the restriction of \f{f} on \f{(-\infty,0]} or \f{[0,+\infty)} is bijective.
* If \f{n} is even then \f{f^{-1}} for \f{[0,+\infty)} is \f{f^{-1} = \sqrt[n]{x}}.
* If \f{n} is odd then \f{f} is bijective.
* If \f{n} is odd then \f{f^{-1} = \sqrt[n]{\abs{x}}}.
* If \f{n} is even then \f{f} is convex.
* If \f{n} is odd then \f{f} is concave on \f{(-\infty,0]} and convex and \f{[0,+\infty)}.

For part \f{1}, let \f{k \in \mathbb{N}} such that \f{n = 2k}. We have that \f{f(-x) = (-x)^n = (-x)^{2k} = ((-x)^2)^k = x^{2k} = x^n = f(x)}, therefore \f{f} is even.
For part \f{2}, let \f{k \in \mathbb{N}} such that \f{n = 2k+1}. We have that \f{f(-x) = (-x)^n = (-x)^{2k+1} = (-x)((-x)^2)^k = (-x)x^{2k} = -(x^{2k+1}) = -f(x)}, therefore \f{f} is odd.
For part \f{3}, let \f{x, y \in (-\infty,0]} and \f{x < y}. Then \f{x = -\abs{x}}, \f{y = -\abs{y}}, \f{\abs{x} > \abs{y}} and we have \f{f(x) > f(y)} or \f{x^n > y^n} or \f{(-1)^n\abs{x}^n > (-1)^n\abs{y}^n}. Since \f{(-1)^n = 1} because \f{n} is even, we get \f{\abs{x}^n > \abs{y}^n}, since \f{\abs{x} > \abs{y}} (see BasicObjects). For \f{x, y \in [0,+\infty)} the proof is similar.
For part \f{4}, the proof is similar to that for part \f{3}.
For part \f{5}, we have that \f{0^n = 0}, therefore \f{0} is a root for \f{f}. Now, assume \f{y \neq 0} is also a root for \f{f}. But then \f{y^n = 0} or \f{yyy \dots y = 0} which means (see BasicObjects) that one of the factors must be \f{0}, or \f{y = 0}, which is a contradiction. Therefore, only \f{0} is a root for \f{f}.
For part \f{6}, we have \f{f(x) = \abs{x}^n}. Since, for \f{x \neq 0} we have \f{\abs{x} > 0}, we also have \f{\abs{x}^n > 0^n} or \f{f(x) > 0}, therefore \f{0} is a lower bound for \f{f}. Since \f{f(0) = 0}, then \f{0} is also the infimum/minimum of \f{f} (lower bounds greater than \f{0} are out of the question).
For part \f{7}, the proof is similar to that for part \f{6}.
For part \f{8}, we have \f{2} proofs. Proof \f{1}: consider that \f{\lim_y x = y}, by \thmref{NumericFunctionsLinearProperties}. Then (see Calculus), we have \f{\lim_y x^n = (\lim_y x)^n = y^n}. Proof \f{2}: let \f{\epsilon > 0}. We have to find \f{\delta > 0} such that for all \f{x} with \f{0 < \abs{x - y} < \delta} we have \f{\abs{x^n - y^n} < \epsilon}. We can rewrite the last inequality as \f{\abs{x - y}\abs{x^{n-1} + x^{n-2}y + \dots + xy^{n-2} + y^{n-1}} < \epsilon}. We have to bound the left and right terms somehow. If we let \f{\abs{x - y} < 1} then we have \f{\abs{x} - \abs{y} < 1} or \f{\abs{x} < \abs{y} + 1}. Returning to the previous term, we have, \f{\abs{x - y}\abs{x^{n-1} + x^{n-2}y + \dots + xy^{n-2} + y^{n-1}} < \abs{x^{n-1} + x^{n-2}y + \dots + xy^{n-2} + y^{n-1}} < \abs{(\abs{y}+1)^{n-1} + (\abs{y}+1)^{n-2}y + \dots + (\abs{y}+1)y^{n-2} + y^{n-2}}}. If we now make \f{\abs{x - y} < \epsilon / \abs{(\abs{y}+1)^{n-1} + (\abs{y}+1)^{n-2}y + \dots + (\abs{y}+1)y^{n-2} + y^{n-2}}} we have that \f{\abs{x^n - y^n} < \epsilon / \abs{(\abs{y}+1)^{n-1} + (\abs{y}+1)^{n-2}y + \dots + (\abs{y}+1)y^{n-2} + y^{n-2}} * \abs{(\abs{y}+1)^{n-1} + (\abs{y}+1)^{n-2}y + \dots + (\abs{y}+1)y^{n-2} + y^{n-2}} = \epsilon} or \f{\abs{x^n - y^n} < \epsilon}, which is what we wanted. We must therefore choose \f{\delta = \min(1,\epsilon / \abs{(\abs{y}+1)^{n-1} + (\abs{y}+1)^{n-2}y + \dots + (\abs{y}+1)y^{n-2} + y^{n-2}})} for our \f{\epsilon > 0} and \f{y}.
For part \f{9}, let \f{M > 0 }. We must choose \f{N > 0} such that for all \f{x > N} we have \f{f(x) > M} or \f{x^n > M}. But, since \f{f} is increasing, any \f{N} with \f{f(N) > M} will suffice, as for all \f{x > N} we will have \f{f(x) > f(N) > M}, which is what we wanted.
For parts \f{10}, \f{11} and \f{12}, the proofs are similar to that for part \f{9}.
For part \f{13}, since for all \f{y \in \mathbb{R}} we have \f{\lim_y f(x) = f(y) = y^n} we have that \f{f} is continuous everywhere.
For part \f{14}, let \f{\epsilon > 0}. First, suppose \f{0 < a < b}. We have to choose \f{\delta > 0} such that for all \f{x,y \in (a,b)} with \f{\abs{x - y} < \delta} we have \f{\abs{f(x) - f(y)} < \epsilon} or \f{\abs{x^n - y^n} < \epsilon}. We have \f{\abs{x^n - y^n} = \abs{x - y}\abs{x^{n-1} + \dots + y^{n-1}} < \abs{x - y}\abs{nb^n} = \abs{x - y} (nb^n)}. If we choose \f{\delta = \epsilon / (nb^n)} we get \f{\abs{x^n - y^n} < \epsilon / (nb^n) (nb^n) = \epsilon}, which is what we wanted. The other cases are dealt with similarly, but majoring by \f{\Delta = \abs{b - a}} instead of \f{b}, where required.
For parts \f{15}, \f{16}, \f{17} and \f{18} there are two proofs. Proof \f{1}: let \f{y \in \mathbb{R}}. The limit \f{\lim^h_0 (f(y + h) - f(y)) / h = \lim^h_0 ((y + h)^n - y^n) / h = \lim^h_0 (y^n + \sum_{i=1}^n {{n}\choose{i}} h^i y^{n-i} - y^n) / h = \lim^h_0 ({{n}\choose{1}} y^{n-1} h + \dots {{n}\choose{n-1}} y h^{n-1} + h^n) / h = \lim^h_0 (n y^{n-1} + {{n}\choose{2}} h y^{n-2} + \dots + {{n}\choose{n-1}} y h^{n-2} + h^{n-1}) = ny^{n-1}}, since \f{\lim^h_0 \alpha h^i = 0} and all limits exist and we have an addition of limits which exist (see Calculus). Thus \f{f} is differentiable and \f{f'(x) = ny^{n-1}}. \f{f'} is itself obviously differentiable, having the same form as \f{f}, and, by a simple induction, we obtain the general form of part \f{16} and the specialized forms of parts \f{17} and \f{18}. Proof \f{2}: let \f{y \in \mathbb{R}}. We will prove that \f{f'(x) = nx^{n-1}} by induction on \f{n}. For \f{n = 1}, we have that \f{f(x) = x} and, by \thmref{NumericFunctionsLinearProperties} we have that \f{f'(x) = 1}, which is what we wanted. Assume now that \f{f'(x) = nx^{n-1}} for \f{n}. We will prove we can deduce the statement for \f{n+1}. We have \f{f(x) = x^{n+1}} and let \f{\overline{f}(x) = x^n} such that \f{f(x) = x\overline{f}(x)}. Then, we have \f{f'(x) = (x\overline{f})'(x) = (x)'(x) \overline{f}(x) + x \overline{f}'(x) = \overline{f}(x) + x \overline{f}'(x) = x^n + x n x^{n-1} = (n + 1)x^n}, which is again, what we wanted. Therefore, by the principle of induction we have that our statement holds. The argument for higher-order derivatives then continues as in the first part.

* For all \f{y \in \mathbb{R}} we have \f{\lim_y f(x) = a_ny^n + \dots + a_1y + a_0}.
* If \f{n} is even and \f{a_n > 0} then \f{\lim_{+\infty} f(x) = +\infty}.
* If \f{n} is even and \f{a_n < 0} then \f{\lim_{+\infty} f(x) = -\infty}.
* If \f{n} is even and \f{a_n > 0} then \f{\lim_{-\infty} f(x) = +\infty}.
* If \f{n} is even and \f{a_n < 0} then \f{\lim_{-\infty} f(x) = -\infty}.
* If \f{n} is odd and \f{a_n > 0} then \f{\lim_{+\infty} f(x) = +\infty}.
* If \f{n} is odd and \f{a_n < 0} then \f{\lim_{+\infty} f(x) = -\infty}.
* If \f{n} is odd and \f{a_n > 0} then \f{\lim_{-\infty} f(x) = -\infty}.
* If \f{n} is odd and \f{a_n < 0} then \f{\lim_{-\infty} f(x) = +\infty}.
* \f{f} is continuoues everywhere.
* Let \f{a < b} be numbers then \f{f} is uniformly continuous on \f{(a,b)}.
* \f{f} is infinitely differentiable everywhere.
* \f{f^{(k)}(x) = \sum_{i=0}^n a_i \Pi_{j=0}^{k} max(0,(i-j)) x^{\max(0,{i-j})}} for \f{k \leq n} and \f{f^{(k)}(x) = 0} for \f{k > n}.
* \f{f'(x) = a_nnx^{n-1} + \dots + 2a_2x + a_1}.

For part \f{1}, since \f{f} is the sum of monomials, and by the properties of additions of limits (see Calculus), it follows that this property holds.
For part \f{2}, we can write \f{f(x) = a_nx^n (1 + (a_{n-1}/a_n)(1/x) + \dots + (a_1/a_n)(1/x^{n-1}) + (a_0/a_n)(1/x^n))}. We know from \thmref{NumericFunctionsReciprocalProperties} that \f{\lim_{+\infty}1/x^k = 0}. Since all limits exist, we  have that \f{\lim_{+\infty} f(x) = \lim_{+\infty} a_n x^n = a_n \lim_{+\infty} x^n}. By \thmref{NumericFunctionsMonomialProperties} we know that \f{\lim_{+\infty} x^n = +\infty}, and since \f{a_n > 0} we have \f{\lim_{+\infty}f(x) = +\infty}.
For parts \f{3}, \f{4}, \f{5}, \f{6}, \f{7}, \f{8} and \f{9}, the proof is similar to that for part \f{2}.
For part \f{10}, since \f{f} is the sum of functions which are continuous everywhere, by \thmref{NumericFunctionsMonomialProperties}, it follows that \f{f} is continuous everywhere (see Calculus).
For part \f{11}, since \f{f} is the sum of functions which are uniformly continous on \f{(a,b)}, by \thmref{NumericFunctionsMonomialProperties}, it follows that \f{f} is uniformly continuous on \f{(a,b)} (see Calculus).
For parts \f{12}, \f{13} and \f{14}, the results follow immediately from the fact that \f{f} is a sum of monomial functions.

Let \f{f} be a polynomial function. Then
* If \f{a} is a root of \f{f} of order \f{m} then \f{a} is a root for \f{f^{(1)}, f^{(2)}, \dots, f^{(m-1)}}.
* If \f{a} and \f{b} are consecutive roots of \f{f}, then there exists \f{x \in (a,b)} such that \f{f'(x) = 0} - the roots of the derivative of the polynomial lie between the roots of the polynomial.

For part \f{1}, consider we can write \f{f = (x - a)^m g}, where \f{a} is not a root for \f{g}. Assume \f{n = 1}. We will use an argument by induction. Then, we have \f{f' = m(x - a)^{m-1} g + (x - a)^m g'}. Then, \f{f'(a) = m(a - a)^{m-1}g(a) + (a - a)^m g'(a) = 0}, therefore \f{a} is a root for \f{f'}. Therefore, the statement holds for \f{n = 1}. Assume now it holds for \f{n < m - 1}. We then have \f{f^{(n+1)}(x) = (f^{(n)}(x))'}. Since \f{a} is a root for \f{f^{(n)}} we have \f{f^{(n)}(x) = (x - a)^{m-n}g_n(x)}, where \f{g_n(x)} does not have \f{a} as a root. Then, we have \f{f^{(n+1)}(x) = (m-n)(x - a)^{m-n-1}g_n(x)  + (x - a)^{m-n}g_n'(x)}. If we apply \f{f^{(n+1)}(a)} we get \f{0}. Therefore, by the Principle of Induction, we have that our statement holds.
For part \f{2}, proof \f{1}: use Rolle's Theorem. proof \f{2}: when the orders of \f{a} and \f{b} are both \f{1}, we have that \f{f(x) = (x - a)(x - b)g(x)} and \f{f'(x) = (x - a)g(x) + (x - b)g(x) + (x - a)(x - b)g'(x)}. Here \f{g(x)} is a polynomial function of degree \f{\deg{f} - 2} which does not have \f{a} and \f{b} as roots. Now, \f{f(a) = 0} and \f{f(b) = 0}. On the interval \f{(a,b)}, \f{f} must have the same sign, else, by the \thmref{IntermediateValueTheorem}, we have a \f{0} crossing, which is a contradiction, as \f{a} and \f{b} are consecutive roots. Since \f{(x - a)(x - b)} is negative for \f{x \in (a,b)}, then \f{g(x)} has the opposite sign of \f{f} on \f{(a,b)}. Furthermore, \f{g(a)} and \f{g(b)} have the same sign, as otherwise, by the \thmref{IntermediateValueTheorem}, we'd have a \f{0} crossing, which weould be another root for \f{f}, which is a contradiction, as \f{a} and \f{b} are consecutive roots. Since this is the case, notice that \f{f'(a) = (a - b)g(a)} and \f{f'(b) = (b - a)g(b)}. Since \f{g(a)} and \f{g(b)} have the same sign, then \f{(a - b)g(a)} and \f{(b -a)g(b)} have opposite signs. Since \f{f'} is continuous, we have by \thmref{IntermediateValueTheorem} that \f{f'} has a \f{0} crossing. Therefore, there exists an \f{x \in (a,b)} with \f{f'(x) = 0}. If \f{a} and \f{b} have higher orders, then we consider \f{h(x) = f'(x) / ((x-a)^{m-1}(x - b)^{k-1})}, and the proof is similar as the first part.

Since \f{f} has at most \f{n} roots, \f{f'} has at most \f{n-1} roots, therefore \f{f} has at most \f{n-1} critical points. \f{f} then has at most \f{n} regions of increase and decrease (defined by the \f{n-1} critical points). The signs of these can be found out by inspecting \f{n} points (say the midpoint of every interval defined by the \f{n-1} critical points, as well as some reasonable points in the first and last intervals, which start in \f{-\infty} and end in \f{+\infty}, respectively).

Let \f{f} be a polynomial function of degree \f{n} and \f{g} be a polynomial function of degree \f{m}. Then \f{f} and \f{g} intersect at most \f{\max(n,m)} times.

\f{f} and \f{g} intersect at a point \f{\overline{x}} for which \f{f(\overline{x}) = g(\overline{x})} or \f{f(\overline{x}) - g(\overline{x}) = 0}. Since \f{f - g} is a polynomial, then \f{\overline{x}} is a root of this polynomial. The sum/difference of two polynomials has degree \f{\max(n,m)} at most \thmref{TheDegreeSumProductPolynomial}, therefore it has at most \f{\max(n,m)} roots \thmref{MaximumNumberRoots}, therefore the two polynomials intersect at most \f{\max(n,m)} times.

== Rational ==

Let \f{a_n,a_{n-1},\dots,a_1 \text{ and } a_0} be numbers with \f{a_n \neq 0} and \f{b_m,b_{m-1},\dots,b_1 \text{ and } b_0} be numbers with \f{b_m \neq 0}.

\def{Rational function} are functions of the form \f{f(x) = (a_nx^n + a_{n-1}x^{n-1} + \dots + a_1x + a_0) / (b_mx^m + b_{m-1}x^{m-1} + \dots + b_1x + b_0)}. These are functions which are the ratio of two polynomial functions. Example graphs can be seen in figures \ref{figure:NumericFunctionsRational1}, \ref{figure:NumericFunctionsRational2} and \ref{figure:NumericFunctionsRational3}.

Polynomial functions are a special case of rational functions (with \f{m = 0}).

Pictures of rational functions:
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = \frac{x^2}{x^2 + 2}}] {
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = x.^2 ./ (x .^ 2 + 2);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsRational1}
}
\subfigure[\f{f(x) = \frac{\frac{1}{2}x^3 + 3x + 2}{x^2 + 2x + 3}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = (0.5 * x.^3 + 3*x + 2) ./ (x .^ 2 + 2*x + 3);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsRational2}
}
\subfigure[\f{f(x) = \frac{-x^4 + 3x^2 + \frac{2}{7}x + 1}{x^3 + 2x + 4}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = (-x.^4 + 3*x.^2 + 2/7*x + 1) ./ (x.^3 + 2*x + 4);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsRational3}
}
\caption{Different types of rational functions.}
\end{figure}
}

Let \f{f(x) = 1/x^n} be a reciprocal function with \f{\dom{f} = \mathbb{R} \setminus \{0\}}.
* If \f{n} is even then \f{f} is even.
* If \f{n} is odd then \f{f} is odd.
* If \f{n} is even then \f{f} is increasing on \f{(-\infty,0)} and decreasing on \f{(0,+\infty)}.
* If \f{n} is odd then \f{f} is decreasing on \f{(-\infty,0)} and decreasing on \f{(0,+\infty)}.
* If \f{n} is even then \f{0} is a lower bound of \f{f}.
* If \f{n} is odd then \f{0} is a upper bound of \f{f} on \f{(-\infty,0)} and a lower bound on \f{f} on \f{(0,+\infty)}.
* For all \f{y \in \mathbb{R}\setminus \{0\}} we have \f{\lim_y f(x) = 1/y^n}.
* \f{\lim_{+\infty} f(x) = 0}.
* \f{\lim_{-\infty} f(x) = 0}.
* If \f{n} is even then \f{\lim_{0+} = +\infty}.
* If \f{n} is even then \f{\lim_{0-} = +\infty}.
* If \f{n} is even then \f{\lim_{0+} = +\infty}.
* If \f{n} is odd then \f{\lim_{0-} = -\infty}.
* \f{f} is continuous everywhere except at \f{0}.
* Let \f{a < b < 0} or \f{0 < a < b} be numbers then \f{f} is uniformly continous on \f{(a,b)}.
* \f{f} is infinitely differentiable everywhere except at \f{0}.
* \f{f^{(k)}(x) = (-1)^k\Pi_{i=0}^k(n+i) x^{-(n+k)}}.
* \f{f'(x) = -n/x^{n+1}}.
* If \f{n} is even then the restriction of \f{f} on \f{(-\infty,0)} or \f{(0,+\infty)} is bijective.
* If \f{n} is even then \f{f^{-1}} for \f{(0,+\infty)} is \f{f^{-1} = \sqrt[-n]{x}}.
* If \f{n} is odd then \f{f} is bijective.
* If \f{n} is odd then \f{f^{-1} = \sqrt[-n]{\abs{x}}}.
* If \f{n} is even then \f{f} is convex on \f{(-\infty,0)} and \f{(0,+\infty)}.
* If \f{n} is odd then \f{f} is concave on \f{(-\infty,0)} and convex and \f{(0,+\infty)}.

The graph of a rational function is determined by the numerator polynomial and the denominator polynomial. Suppose \f{f} is in reduced form / standard form, that is \f{f = P/Q}, where \f{P} and \f{Q} have no roots in common. Then \f{f} is not defined for the roots of \f{Q}. When plotting the graph of \f{f}, we must first identify these points. If there are \f{m} such roots, then there will be \f{m + 1} intervals we will treat separatedly. Suppose \f{b_i} and \f{b_{i+1}} are two consecutive roots of \f{Q}. On \f{[b_i,b_{i+1}]}, \f{Q} is either always positive or always negative, and always non-null, therefore division makes sense. We then look at all the roots of \f{P} in the interval \f{[b_i,b_{i+1}]}, let them be \f{a^i_1, \dots, a^i_{k_i}} where \f{k_i \geq 0}. These will be the zero crossings. We then proceed to identify critical points of \f{P} and inflection points etc., because on \f{[b_i,b_{i+1}]} \f{f} will have the same critical points and inflection points as \f{P}. We can sketch out the minima and maxima and inflection points for the intervals. Now, for \f{+\infty} and \f{-\infty}, one has to look at the degree of \f{P} and \f{Q}. If \f{\deg{P} > \deg{Q}}, then \f{f} goes to inftinity as well. If \f{\deg{P} < \deg{Q}}, then \f{f} goes to \f{0}. If \f{\deg{P} = \deg{Q}} then \f{f} goes to \f{a_n / a_m}, like in the theorem \thmref{NumericFunctionsRationalProperties}. Now, for the actual roots of \f{Q}, it depends on how \f{P} and \f{Q} behave, but they should go to either \f{+\infty} or \f{-\infty} towards the root, on the left and the right, depending on how it is.

== Rational Power ==

Pictures of rational power functions:
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = \sqrt{x}}] {
== drawing-beg [width=0.30\textwidth] ==
x = 0:0.01:3;
y = x.^(1/2);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsNumeric2}
}
\caption{Different types of rational power functions.}
\end{figure}
}

== Trigonometric ==


Pictures of trigonometric functions:
\comment{
\subfigure[\f{f(x) = \sin{x}}] {
== drawing-beg [width=0.30\textwidth] ==
x = -2*pi:0.01:2*pi;
y = sin(x);

plot(x,y,'LineWidth',5);
axis([-2*pi 2*pi -2 4]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsNumeric1}
}
\caption{Different types of trigonometric functions.}
\end{figure}
}

== Exponential And Logarithmic ==

Pictures of logarithmic and exponential functions.
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{f(x) = \log{X}}] {
== drawing-beg [width=0.30\textwidth] ==
x = 0:0.01:3;
y = log(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsNumeric3}
}
\caption{Different types of logarithm functions.}
\end{figure}
}

== Dirrichlet Type Functions ==

The function \f{f(x) = x^2} if \f{x} rational and \f{f(x) = 0} otherwise is differentiable at \f{0} and has \f{f'(0) = 0}. For \f{f(x) = p/q \given 0}. Every irrational is a global mimima and every rational is a local maxima. \f{f(x) = x \given 0} every irrational is a maxima if \f{x > 0} and minima if \f{x < 0}. \f{f(x) = 1/n \given 0} every \f{1/2} is a global maxima and every other point is a global minima. Such functions do not have area as we have \f{\sup\{L[f,P] \given P \text{ is a partition of } [a,b]\} < \inf\{U[f,P] \given P \text{ is a partition of } [a,b]\}} (proof).

== Absolute Value ==

The \def{absolute value function}, or \f{\abs{\star}} is a function equal to the absolute value of its argument - \f{\abs{\star}(x) = \abs{x}}. This is a functional form of the absolute value of a number. All the properties of the absolute value are kept, but we can use tools from the study of numeric functions and Calculus to obtain extra information. An example graph can be seen in figure \ref{figure:NumericFunctionsAbsoluteValue}.

Picture of the absolute value function:
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{\abs{\star}(x) = \abs{x}}]{
== drawing-beg [width=0.30\textwidth] ==
x = -3:0.01:3;
y = abs(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsAbsoluteValue}
}
\caption{The absolute value function.}
\end{figure}
}

Properties of the absolute value function:
* \f{\abs{\star}} is even.
* \f{\abs{\star}} is positive for \f{x \neq 0} and \f{0} for \f{x = 0}.
* \f{\abs{\star}} is decreasing on \f{(-\infty,0]} and increasing on \f{[0,+\infty)}.
* \f{\abs{\star}} is lower-bounded by \f{0}.
* \f{x = 0} is a root for \f{\abs{\star}}.
* For all \f{y \in \mathbb{R}} we have \f{\lim_y \abs{\star}(x) = \abs{y}}.
* \f{\lim_{+\infty} \abs{\star}(x) = +\infty}.
* \f{\lim_{-\infty} \abs{\star}(x) = +\infty}.
* \f{\abs{\star}} is continuous everywhere.
* \f{\abs{\star}} is uniformly continuous on \f{\mathbb{R}}.
* \f{\abs{\star}} is differentiable on \f{\mathbb{R} \setminus \{0\}}.
* \f{\abs{\star}'(x) = 1} for \f{x > 0} and \f{\abs{\star}'(x) = -1} for \f{x < 0}.
* \f{\abs{\star}} is weak-convex.

== Sign Function ==

The \def{sign function}, or \f{\sigma} is a function equal to the \f{1} if the argument is positive, \f{-1} if the argument is negative and \f{0} if the argument is \f{0}. An example graph can be seen in figure \ref{figure:NumericFunctionsSign}.

Figures
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{\sigma(x)}]{
== drawing-beg [width=0.30\textwidth] ==

hold on;
plot(-3:0.01:0,-1*ones(1,length(-3:0.01:0)),'LineWidth',5);
plot(0,0,'LineWidth',5);
plot(0:0.01:+3,+1*ones(1,length(0:0.01:+3)),'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsSign}
}
\caption{The sign function.}
\end{figure}
}

Properties of the sign function:
* \f{\sigma} is odd.
* \f{\sigma} is positive for \f{x > 0}, negative for \f{x < 0} and \f{0} for \f{x = 0}.
* \f{\sigma} is upper-bounded by \f{1}, lower-bounded by \f{-1}, has \f{1} as a maximum and \f{-1} as a minimum.
* \f{x = 0} is a root for \f{\sigma}.
* For all \f{y \in \mathbb{R} \setminus \{0\}} we have \f{\lim_y \sigma(x) = \sigma{y}}.
* \f{\lim_{0^+}\sigma(x) = 1}.
* \f{\lim_{0^+}\sigma(x) = -1}.
* \f{\lim_{+\infty} f(x) = 1}.
* \f{\lim_{-\infty} f(x) = -1}.
* \f{\sigma} is continuous on \f{\mathbb{R} \setminus \{0\}}.
* \f{\sigma} is left-continuous everywhere.
* \f{\sigma} is right-continuous everywhere.
* \f{\sigma} is uniformly continuous on \f{\mathbb{R} \setminus \{0\}}.
* \f{\sigma} is differentiable on \f{\mathbb{R} \setminus \{0\}}.
* \f{\sigma'(x) = 0} for \f{x \neq 0}.
* \f{\sigma} is weak-convex on \f{(-\infty,0)} and \f{(0,+\infty)} separatedly.

== Ceil, Floor, Round, FMod ==

The \def{ceil function}, or \f{\ceil{\star}} is a function equal to the smallest integer larger than its argument. An example graph can be seen in figure \ref{figure:NumericFunctionsCeil}. The \def{floor function}, or \f{\floor{\star}}s is a function equal to the largest integer smaller than its argument. An example graph can be seen in figure \ref{figure:NumericFunctionsFloor}. The \def{round function}, or \f{\round{\star}} is a function equal to the closest integer to its argument. An example graph can be seen in figure \ref{figure:NumericFunctionsRound}. The \def{FMod function}, or \f{\fmod{\star}} is a function equal to the difference between its argument and the floor of its argument - \f{\fmod{x} = x - \floor{x}}. An example graph can be seen in figure \ref{figure:NumericFunctionsFMod}.

Figures
\comment{
\begin{figure}[h!]
\centering
\subfigure[\f{\ceil{x}}]{
== drawing-beg [width=0.45\textwidth] ==
x = -3:0.01:3;
y = ceil(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsCeil}
}
\subfigure[\f{\floor{x}}]{
== drawing-beg [width=0.45\textwidth] ==
x = -3:0.01:3;
y = floor(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsFloor}
}
\subfigure[\f{\round{x}}]{
== drawing-beg [width=0.45\textwidth] ==
x = -3:0.01:3;
y = round(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsRound}
}
\subfigure[\f{\fmod{x}}]{
== drawing-beg [width=0.45\textwidth] ==
x = -3:0.01:3;
y = x - floor(x);

plot(x,y,'LineWidth',5);
axis([-3 3 -2 6]);
grid on;
== drawing-end ==
\label{fig:NumericFunctionsFMod}
}
\caption{The ceil, floor, round and fmod function.}
\end{figure}
}

Properties of the ceil function:
* Let \f{k \in \mathbb{Z}} then \f{\ceil{\star}} is constant on \f{(k,k+1]}.
* \f{\ceil{\star}} is nondecreasing.
* Let \f{y \in \mathbb{R}\setminus\mathbb{Z}} then \f{\lim_y \ceil{x} = \ceil{y}}.
* Let \f{y \in \mathbb{R}} then \f{\lim_{y^-} \ceil{x} = \ceil{y}}.
* \f{\lim_{+\infty} \ceil{x} = +\infty}.
* \f{\lim_{-\infty} \ceil{x} = -\infty}.
* \f{\ceil{\star}} is continuous on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\ceil{\star}} is left-continuous everywhere.
* \f{\ceil{\star}} is uniformly continous on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\ceil{\star}} is differentiable on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\ceil{\star}} is left-differentiable everywhere.
* \f{\ceil{\star}'(x) = 0} for \f{x \in \mathbb{Z} \setminus \mathbb{Z}}.
* Let \f{k \in \mathbb{Z}} then \f{\ceil{\star}} is weak-convex on \f{(k,k+1]}.

Properties of the floor function.
* Let \f{k \in \mathbb{Z}} then \f{\floor{\star}} is constant on \f{[k,k+1)}.
* \f{\floor{\star}} is nondecreasing.
* Let \f{y \in \mathbb{R}\setminus\mathbb{Z}} then \f{\lim_y \floor{x} = \floor{y}}.
* Let \f{y \in \mathbb{R}} then \f{\lim_{y^-} \floor{x} = \floor{y}}.
* \f{\lim_{+\infty} \floor{x} = +\infty}.
* \f{\lim_{-\infty} \floor{x} = -\infty}.
* \f{\floor{\star}} is continuous on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\floor{\star}} is left-continuous everywhere.
* \f{\floor{\star}} is uniformly continous on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\floor{\star}} is differentiable on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\floor{\star}} is left-differentiable everywhere.
* \f{\floor{\star}'(x) = 0} for \f{x \in \mathbb{Z} \setminus \mathbb{Z}}.
* Let \f{k \in \mathbb{Z}} then \f{\floor{\star}} is weak-convex on \f{[k,k+1)}.

Properties of the round function.
* Let \f{k \in \mathbb{Z}} then \f{\round{\star}} is constant on \f{(k-1/2,k+1/2]}.
* \f{\round{\star}} is nondecreasing.
* Let \f{y \in \mathbb{R}\setminus\mathbb{Z}} then \f{\lim_y \round{x} = \round{y}}.
* Let \f{y \in \mathbb{R}} then \f{\lim_{y^-} \round{x} = \round{y}}.
* \f{\lim_{+\infty} \round{x} = +\infty}.
* \f{\lim_{-\infty} \round{x} = -\infty}.
* \f{\round{\star}} is continuous on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\round{\star}} is left-continuous everywhere.
* \f{\round{\star}} is uniformly continous on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\round{\star}} is differentiable on \f{\mathbb{R} \setminus \mathbb{Z}}.
* \f{\round{\star}} is left-differentiable everywhere.
* \f{\round{\star}'(x) = 0} for \f{x \in \mathbb{Z} \setminus \mathbb{Z}}.
* Let \f{k \in \mathbb{Z}} then \f{\round{\star}} is weak-convex on \f{(k-1/2,k+1/2]}.

= Combinatorics =

Counting Assignments: we have a list of \f{n} items and a set \f{V} of \f{k} values. An assignment is a list of \f{n} values \f{(v_1,\dots,v_n)} where \f{v_i \in V} and \f{v_i} is associated with item \f{i}. There are \f{k^n} such assignments (prove by induction and intuition (assign any value from \f{V} to element \f{1} and we have the same choice for \f{i-1} etc.)). Number of functions from \f{\hcrange{1}{n}} to \f{V} with no other constraints. For example, a computer with \f{n} bit memory addresses can address \f{2^n} different memory locations.

Counting Orderings: we have a set of \f{n} items \f{X}. An ordering is a list which contains all the values in \f{X} exactly once. There are \f{\Pi(n) = n!} such orderings (prove by induction and intuition (assign any index to element \f{1} and we can assign only \f{n-1} values for \f{i-1} etc.)). Number of bijective functions from \f{\hcrange{1}{n}} to a set of equal size. It is a restriction of counting assignments, where we require \f{n=k} and each element to be unique.

Counting Partial Orderings / Arrangements: we have a set of \f{n} items \f{X} and a number \f{k}. A partial ordering is a list of length \f{k} which contains values from \f{X} exactly once. Not all values are included. There are \f{\Pi(n,k) = n!/(n-k)! = n(n-1)\cdots(n-k+1)} such orderings (prove by induction and intuition (assign any index to element \f{1} and we can assign only \f{n-1} values for \f{i-1} etc. until we reach element \f{k} at which point we stop)). Number of injective function from \f{\hcrange{1}{k}} to \f{\hcrange{1}{n}}. It is a generalization on counting partial orderings, where we have less elements in the orderings than the full set of items.

Counting Combinations: we have a set of \f{n} items \f{X} and a number \f{k}. A combination is a set of \f{k} distinct elements from \f{X}. There are \f{\choose{m}{k} = \Pi(m,k)/\Pi(k) = n!/(k!(n-k)!)} such orderings (prove by induction and intuition (proceed as with arrangements, then notice that a certain set of length \f{k} appears as \f{\Pi(k)} different permutations in the set of arrangements. Furthermore, no other set generates a given arrangement, so by dividing by \f{\Pi(k)} we're not doing any under/overcounting). Number of ways to choose sets of \f{k} elements from a set of \f{n} elements. It is a version of counting arrangements where order does not count.
* It is also known that \f{\choose{n}{0} = 1}, \f{\choose{n}{n} = 1} and \f{\choose{n}{k} = \choose{n-1}{k} + \choose{n-1}{k-1}}. Intuitively, in order to choose \f{k} out of \f{n}, we can not choose the first element, and then choose \f{k} out of the rest of \f{n-1} elements, or choose the first element and choose \f{k-1} elements out of the rest \f{n-1} elements. A cool visualization is Pascal's Triangle [more about this here]. Furthermore, this can be proven by double induction, or by induction on \f{n\min{k,n-k}}.
* Using the previous form and induction, \f{\choose{n}{k}} is always an integer. Physically intuitive, but hard to deduce from the factorial formula.
* If we compute \f{\choose{n}{k}} by factorial computation, the complexity is \f{O(n)} (exponential in storage requirements). We can speed things up quite a lot if \f{k} is small (\f{n-k} is large, and we divide \f{n!} by \f{(n-k)!} beforehand, so we only have \f{O(k)} complexity) or if \f{k} is large (we divide \f{n!} by \f{k!} be forhand, and \f{n-k} is small, so we only have \f{O(n-k)} exomplexity). We do have either overflow issues or floating point issues if we do multiplications and divisions interleaved.
* If we compute \f{\choose{n}{k}} by the recursive form, we can proove it's complexity is upper bounded by \f{O(2^n)} (simple recurrence relation \f{T(1) = a}, \f{T(n) = a + T(n-1) + T(n-2)}. A tight (but hard to prove bound) is \f{O(2^n/\sqrt{n})}.
* Simple fact: \f{\choose{n}{k} = \choose{n}{n-k}}. Intuitively, if we have a subset of size \f{k}, then there is a corresponding subset of size \f{n-k} which was not included. For each subset counted we then have a corresponding subset of size \f{n-k}. There are no other subsets of size \f{n-k} (otherwise we would have a corresponding subset of size \f{k}), therefore \f{\schoose{n}{n-k} = \choose{n}{k}}.
* If we keep \f{n} fixed and large, then \f{\choose{n}{k}} is a function of \f{k}. It has some nice properties: it is a roughly symmetric bell-shaped curve centered around \f{n/2}, the maximum heigt at the center \f{\choose{n}{n/2} \approxto 2^n / \sqrt{\pi n/2}}, the spread/variance is \f{\sqrt{n}} and outside this range, the values fall off sharply.
* We also get the binomial coefficients: \f{(x+y)^n = \sum_{i=0}^n \choose{n}{i} x^iy^{n-i}}. The expansion has \f{2^n} terms, each a sequence of \f{x}s and \f{y}y multiplied. We can view this as a binary vector (\f{x} stands for \f{1} and \f{y} for \f{0}) which encodes if certain elements from a \f{n} element set are included. There are \f{\choose{n}{i}} such sets of size \f{i} and therefore the same amount of terms with \f{i} \f{x}s. Thanks to the properties of multiplication, all these get grouped in a term \f{\choose{n}{i}x^iy^{n-i}}.
* We have that \f{2^n = \sum_{i=0}^n \choose{n}{i}}. Use \f{(1+1)^n} and binomial expansion. This means \f{\choose{n}{i} < 2^n}. Furthermore, since \f{\choose{n}{n/2}} and around, we get close to \f{2^n}, other values can't be that big - we get a concentration of mass near the center.

Counting Orderings with Repetitions: we have a set of \f{n} items \f{X} and a partition of \f{X} into \f{l} groups of sizes \f{m_1, \dots, m_l}. An ordering with repetitions is a list of length \f{n} which contains all the values in \f{X} exactly once, but, for counting purposes, we consider elements belonging to the same partition class to be indistinguishable. The number of such orderings is \f{n! / \Prod_{i=1}^n m_i!} (prove by induction and intuition (there are \f{n!} orderings; given an ordering, the elements from the first class, for example, appear in certain positions in a certain order; but all other orderings which contain the elements in the same positions, but in a different order are equivalent; there are \f{m_1!} such orderings, and they are all equivalent. Thus, we compress this set of orderings turning groups of \f{m_1!} into \f{1} and we need to divide \f{n!} by \f{m_1!} etc. notice that given two orderings they are either in the same equivalence class (they have elements from \f{P_1} in the same position) or they don't, therefore there is no over/under-counting)). A generalization of orderings, with multi-sets.

Counting Distribution of Indistinguishable Objects to Bins: we have a set of \f{n} items \f{X} which we wish to distribute into \f{k} bins. The number of such distributions is \f{\choose{n+k-1}{n} = \choose{n+k-1}{k-1}} (prove by a very nice coding: we use binary words of size \f{n+k-1}. The number of \f{0}s until the first \f{1} represent the number of objects in the first bin, the number of \f{0}s till the second \f{1} represent the number of objects in the second bin etc. We have to choose \f{k} positions from \f{\hcrange{1}{n+k-1}} to serve as positions for the \f{1}s, while the other positions are \f{0}s. This is then the problem of counting combintions of \f{k} items from \f{n+k-1} items. A mirrored argument works for selecting positions of \f{0}s). If we have a constraint that each bin should contain at least one object, then we subtract \f{k} from the number of objects (we consider \f{k} objects reserved to be used in each bin) and we get \f{\choose{n-1}{n-k} = \choose{n-1}{k-1}}.

Counting Distribution of Indistinguishable Objects to Bins: we have a set of \f{n} items \f{X} and a partition of \f{X} into \f{l} groups of size \f{m_1, \dots, m_l} which we wish to distribute into \f{k} bins, but, again, we consider elements beloning to the same partition class to be indistinguishable. The number of such distributions is \f{(n+k-1)!/((k-1)!\Prod_{i=1}^l i_k!)}. We again use a smart coding. We add one more class consisting of \f{\star} - separator symbols. We know from counting orderings with repetitions how many such orderings we generate (our formula). We count all the symbols before the first \f{\star} as belonging to the first bin etc.

Chessboard moving problem: suppose we have an \f{\hctimes{n}{n}} grid. We start at \f{(1,1)} (lower-left) and wish to get at \f{(n,n)} (upper-right) by right or up movements. There are \f{\choose{2(n-1)}{(n-1)}} paths. Use a bit vector for each movement, coded with \f{1} for right and \f{0} for up. There must be \f{n-1} ups and \f{n-1} rights, therefore this is a combinations problem - selecting from \f{2(n-1)} positions, those that are right.

General approaches to counting problems:
* Express a count as a sequence of choices, each of which refines the description of a particular member of the class.
* Express a count as a difference of counts, between a more general class of counts (of which our count is a part of) and all the counts which don't belong in our count.
* Express a count as a sum of counts for subcases.

Pidgeonhole principle: if \f{m+1} pidgeons fly into \f{m} pidgeonholes, there must be at least one hole with two pidgeon. Works for finite sets only.
