Margin: Classification concept. Two sets of points with \f{+1} and \f{-1} labels. A plane described by \f{(w,b)}. \f{+1} corresponds to the positive side of the plane (where \f{\inner{x , w} + b > 0}). \f{-1} corresponds to the negative side of the plane. The margin of \f{(x,y)} relative to \f{(w,b)} is \f{\gamma(x,y) = y(\langle w , x \rangle + b)} - the distance from \f{x} to the plane, modulated by the label of the point. If the point has label \f{+1} and is on the positive side of the plane, the margin is positive. If it is on the negative side of the plane, the margin is negative. If the point has label \f{-1} and is on the negative side of the plane, the margin is positive. If it is on the positve side of the plane, the margin is negative. The margin of the whole dataset is \f{\gamma(\textbf{X},\textbf{Y}) = \min_{i \in \hcrange{1}{N}} \gamma(x_i,y_i)}. The margin can be negative, but if it is positive, we can linearly separate the two classes.

= Maximum Margin Classifier =

a separating hyperplane \f{\Theta} has the property that \f{\Theta^T X > 0} for positive labeled \f{X} or \f{\Theta^T X < 0} for negative labeled \f{X}. Then we have \f{Y (\Theta^T X) > 0} in general. if a classifier which has that \f{y_i (\Theta^T x_i) > 0} for all points in the training set, then we can use this for classification. if one exists, then an infinity of them exist, in general. what is best? the margin is defined as the minimum distance of a traning observation to the plane. This distance is measured by \f{\abs{\Theta^T x} = y\Theta^T x / \norm_2{\Theta}}. Therefore the margin is \f{m = \min_{x \in X_T} \y\Theta^T x}. This will be positive if the data is linearly separable.

a maximum margin classifier is the one which maximizes the margin on the dataset relative to it. So it is \f{\Theta^\star = \arg \max_{\Theta} m(\Theta) = \arg \max_{\Theta} \min_{x,y} y\Theta^T x} subject to \f{y\Theta^T x > 0} for all \f{x,y}. hope that a good performance on the training set will be good on the test set as well. tend to overfit for high \f{p}. a support vector is a training observation which is at distance \f{m} from the plane - the closest then. There can be multiple such vectors, at least one on each side of the plane. the plane depends only on them. any other movement of observations outside the margin band don't affect them. this also casuses overfitting problem. the problem can be posed as \f{\Theta^\star = \arg \max_{\Theta} M} subject to \f{\norm_2{\Theta} = 1} and \f{y_i\Theta^T x \geq M} for all \f{i}. There are many hyperplane with this setup, but we want ours to be of norm \f{1} - this is a unique plane. can't handle non-serparable case at all. may have high variance because of very ridig dependence on training set / support vectors.

= Support Vector Classifier =

allows some violation of the margin (so observations can be on the wrong side of it), which helps with high variability, as well as on the wrong side of the plane, which helps in the non-separable case. the problem becomes \f{\Theta^\star = \arg\max_{\Theta,\Epsilon} M} subject to \f{\norm_2{\Theta} = 1} and \f{y_{\Theta^ x \geq M}} for all \f{i} and \f{\epsilon_i \geq 0} and \f{\sum_{\epsilon_i} < C}. Each training observation has a slack variable \f{\epsilon} which controls how much it can vary, but all can vary up to a constant \f{C}. If \f{\epsilon_i = 0} we have nothing, if it is between \f{0} and \f{1} we get cross-margin stuff, if it is larger than \f{1} we get cross-plane stuff. Obviously, if \f{C < 1} we can't have non-linear separability, but we do get lower variance. In general we'd like to have \f{C > 1}. If \f{C = 0} we have the maximum margin classifier.

No more than \f{C} observations can be on the wrong side of the hyperplane, because doing so implies \f{\epsilon_i > 1}. This sets an upper bound of \f{C} at \f{N} (all observations are on the wrong side), but more realistically at \f{N/2} (if more than that are misclassified, maybe we should reverse the plane). \f{C} controls bias-variance tradeoff and should be selected via cross-validation. apparently, only vectors on the margin or which violate it affect the result (support vectors), symmetric with the maximum margin classifier. large \f{C} implies many observations can influence the plane, therefore we have a lower variance, but a higher bias. result in high margin. small \f{C} implies fewer observations can influence the plane, therefore we have a higher variance, but a lower bias. result in low margin. insensitive to points far away from the decision boundry - good.

= Support Vector Machine =

one approach for non-linearity is to do a basis expansion like with other tools. the SVM comes into its own with kernel expansion, since you can write the result of a support vector classifier \f{Y = \beta_0 + \sum_{i=1}^N \alpha_i \inner{x_i}{X}}, where \f{\alpha_i} is non-null for support vectors only. this is an inner-product form and solving for \f{\alpha} implies only using inner products so we can use kernels.

= Kernelization =

Kernelization: If the algorithm has decision function dependent on an inner product of \f{x} with parameters, like \f{f(x) = f'(\langle ... , x \rangle)}, if we do projections into feature spaces, then, if the feature space is a Reproducing Kernel Hilbert Space, we have that \f{\langle \Phi(y), \Phi(x) \rangle} can be replaced by \f{K(y,x)}. Examples: polynomial (\f{K(y,x) = \langle x,y \rangle^d}) or RBF (\f{K(y,x) = e^{-\gamma \norm{x - y}^2}}). Convex tools can be used for optimization. Useful for all tasks actually.

Kernelization: if a classifier/regressor uses inner products, than we can replace them with kernels, which are functions with the same properties as inner products, but which can express nonlinearities. Implicitly, a mapping to a higher dimensional space happens, and then an inner product is done in that space, but kernels efficiently capture both projection and inner product.

examples of kenels:
* linear
* polynomial
* radial
* logistic
* tanh
* string

