Many models consist of a single feature. A number of descriptive methods can be used to analyze and characterize them. Furthermore, these can be used to analyze each feature of more complex models independently.

We are given a data where each member of the sample is described by just one quantity, a quantitative variable. What can we say? The minimum and the maximum are the smallest and the largest values we have in the sample. The \def{median} is the value selected such that half of the sample has values lower than it and half of it has higher. The \def{first quartile} is the value selected such that one quarter of the sample has value lower than it and three quarters of the sample are larger. The \def{third quartile} is the value selected such that three quarters of the sample has value lower than it and one quarters of the sample are larger. An \def{box plot} (see later) is a graphical summary of the sample using all this information. Algorithmically, we can sort the data, and find the minimum, the maximum, the median (position \f{n/2}), first quartile (position \f{n/4}) and the third quartile (position \f{3n/4}). The running time is \f{O(n\log{n})}. If \f{n/2} is not integer, we can take the number half-way between the \f{\floor{n/2}} and \f{\ceil{n/2}}. Similar for quartiles. The \def{median} is an \def{robust statistic} - it is robust to exterme observations.

The minimum, first quartile, median, third quartile and maximum are called the \f{5}-number summary.

Means, variances, standard deviations or any statistic which depends on field properties of the domain, work only for numeric quantiative data. Modes, medians, quartiles, minima, maxima or any statistic which depends on order properties of the domain, work for ordinal data as well as numeric data.

If \f{X} is a RVar, we have three important parameters. The \def{mean} or \def{expected value} of \f{X}, denoted by \f{E[X]} or \f{\mu_X}, measures the center of a distribution. The \def{variance} of \f{X}, denoted by \f{\text{Var}[X]} or \f{\sigma^2_X}, measures how spread out a distribution is and is in (units of \f{X})\f{^2}. The \def{standard deviation} of \f{X}, denoted by \f{\text{SD}[X]} or \f{\sigma_X}, measures how spread out a distribution is, is equal to \f{\sqrt{\text{Var[X]}}} and is in (uints of \f{X}).

= Measures Of Shape =

Interesting properties of the shape of a distribution:
* symmetric
* unimodal - has a single "peak".
* positive skew - the "tail" in the right direction extends further than the left direction.
* negative skew - the "tail" in the left direction extends further than the right direction.
* bimodal - has two "peaks".
* multimodal - has several "peaks".
* leptokurtic - more elements in the "tails". a property of kurtosis.
* platykurtic - less elements in the "tails". a property of kurtosis.

= The Histogram =

The full shape of the data is characterized by the \def{histogram}, which is an estimator of the distribution of the data. We're looking for how many peaks (modes) we have, wether the histogram is simmetric, wheter there is a left- or right tail (a propensity for seeing smaller or larger than the median values) which means left-or-right skewerd, gaps (which might indicate outliers (or always??)), the general spread of the distribution. The \def{mode} is the most ``probable'' value (the bin with the highest frequency) for unimodal distributions. Unimodal distributions are the most common. In this case, if it is symmetric, the mode, median and mean are in roughly the same position. In a left-skewed distribution, the mean is smaller than the median which is smaller than the mode. In a right-skewed distribution, the mode is smaller than the median which is smaller than the mean.

= Measures Of Centrality =

The median, mean and trimmed mean are measures of centre of the data.

A statistic which measures the "center" or where the most probability mass of a distribution is placed.

Ideas:
* Point of balance of the distribution - there is as much mass on the left as there is on the right of this point.
* Point of minimum sum of absolute deviance from the distribution (or every point in the dataset).
* Point of minimum sum of squared deviances from the distribution (or every point in the dataset).
* Makes sense for ratio scales only.

== The Mean ==

The \def{mean} is the sum of the data values divided by the number of data values. If our data is \f{x_1,x_2,\dots,x_n} we have \f{\overline{x} = \sum_{i=1}^n x_i / N}. The mean is more succeptible to outliers - it is not an \def{robust statistic}. One can compute the \f{x\%} trimmed mean, which is a mean computed from the data, sans the \f{x\%} smallest and largest samples.

A measure of the central tendency defined as \f{M = \sum {x_i} / N} or \f{M = \int_D xP(x) dx}.

It is the point that minimizes the sum of squared deviances central tendency.

A statistic for computing the mean is \f{M = \sum {x_i} / N}. The sampling distribution is \f{N[\mu, \sigma/\sqrt{N}]} where \f{\mu} is the actual mean of the population and \f{\sigma} is the standard deviation of the population distribution. [Obtain both results from linearity of expectation and of variance under independence]. By the CLT, the actual distribution is normal. This means that the value we are computing is very likely to be close to the actual mean, as most of the mass of a normal distribution is at most two SDs from the mean (which is the actual parameter we want to estimate here) and the SD itself goes down with the root of the number of observations.

The statistic for the mean is not biased. We have \f{E[M] = \mu}.

A condifence interval statistic can be computed in two ways, depending on if we have the population variance (mostly in very constricted cases, pedagogic even) or if we don't.

If we have the variance, and we wish to build a \f{q \in [0,1]} level confidence interval, we simply compute the mean and build the interval as \f{\alpha_1 = M - Z_q \sigma} and \f{\alpha_2 = M + Z_q \sigma}, where \f{Z_q} is the \f{Z} score which gives the \f{q} percentile in \f{N[0,1]} and \f{Z_q \sigma} is the domain value corresponding to the \f{q} percentile in our setup. Considering that a similar interval around \f{\mu} means that we'll have the sample mean in it with probability \f{q} (or \f{100q} of the time) it is easy to see how building the condiference interval this way gives us the condifence interval interpretation of having the true population mean \f{100q} of the time. If we don't know the variance, we must estimate it from the data and the distribution we're considering will be Student t with \f{N-1} degrees of freedom. Then, we replace \f{Z_q} by \f{T_q} which is the value in the domain of the Student t distribution which causes \f{q} of the mass to be in an interval. Replace \f{\sigma} by \f{s}. For small \f{N} this value will be much larger than the Z score, reflecting the variability of small samples. For large samples (\f{N > 100}) they are very similar, and one can use the Normal approximation.

If the \f{X_i} are IID Bernoulli variables, then we can build a confidence interval by selecting \f{\epilon_n = \sqrt{1/{2n} \log{2/\alpha}}} such that \f{\prob{\abs{\mu - p} \geq \epsilon_n} \leq \alpha} by \ref{Probability Theory:Classical Inequalities:Hoeffding:Bernoulli case}{Hoeffding's Inequality}. It is not an exact construction (as we get in the normal case - is it better than the normal approximation to the Binomial). Notice that we overestimate the interval - it's bigger, because we select it such that the probability of being outside it is smaller than our desired confidence level, not equal to it.

== Median ==

A measure of the central tendency defined as the point \f{M} such that \f{P(X \leq M) = P(X \geq M)}.
It is the point that balances the distribution. The median is the \f{50}th percentile. It can be computed by sorting the array and returing the middle element (\f{N} odd) or the average of the middle elements (\f{N} even). This is \f{O(N\log{N})}. There is also a \f{O(N)} median finding algorithm. [details later]

== Mode ==

A measure of the central tendency defined as the point of highest probability \f{M = \arg\max_{x \in D} P(X)}. It is the point that minimizes the sum of squared deviances central tendency. If the domain is continuous, or discrete with too many values, a histogram is used. The center of the bin with the most elements is selected as the mode.

== Trimean ==

A measure of central tedency equal to the average of the \f{25, 50, 75}th percentiles.

== Geometric Mean ==

A measure of central tehdency equal to the \f{N}th root of the product of all numbers in the dataset/ distribution. Use logarithms to compute it faster. Used to average data that is "rates" - the rate of growth at different points in time (when we express each value as \f{1.x} for \f{x\%%} increase and \f{1-0.x} for \f{x\%%} decrease.

== Trimmed Mean ==

A measure of central tendency which removes the top \f{X\%} and the bottom \f{X\%} of the data in terms of percentiles, and computes means better. Adds resistance to outliers. \f{X = 5} usually. A hybrid of mean and median (the latter uses \f{X = 49} roughly).

== Distribution Shape Effect On Measures Of Central Tendency ==

If the distribution is symmetric, the mean, median, trimean and trimmed mean are equal.

If the distribution is also unimodal, the median is equal to them as well.

If the distribution is unimodal:
* If there is a positive skew: The mean will be higher than the median. The trimean and trimmed mean will fall between the mean and median, typically. The mode will usually be smaller than the median and mean.
* The geometric mean is lower than all measures except the mode.

= Measures of Variability =

A statistic which measures how spreadout the probability mass of a distribution is. Makes sense for ratio scales only.

The range, the IQR, the mean of absolute deviations, the variance and the standard deviataion are measures of the spread of the data.

== Range ==

The maximum minus the minimum is the \def{range} of the data. The \def{inter-quartile range} (IQR) is the third quartile minus the first quartile. This means that half of the data sits between the IQR. The \def{deviation} of an observation relative to the mean is \f{x_1 - \overline{x}}. It measures how distant one observation is from the ``center'' of the data, as measured by the mean. The sum of deviations is \f{0} however, and is not very good. The \def{mean of absolute deviances} (MAD) is \f{\sum_{i=1}^n \abs{x_i - \overline{x}} / n}. It is hard to work with. The \def{variance} is \f{\sum_{i=1}^n (x_i - \overline{x})^2 / (n-1)}. The \def{standard deviation} is the square root of the variance. We divide by \f{n-1} in order to obtain an \def{unbiased estimator}, that is, one that tends to the real value of the process which generated the data, as more and more data become available. Intuitively, a single value tells us nothing about the spread of the data, it is only the remaining \f{n-1} values that do. When \f{n} is very large, there isn't a big difference. The range is not robust (it changes very much when we remove outliers). The IQR is robust (because the quartiles are robust). The variance and the standard deviation are not robust (depending on the mean).

A measure of variability equal to the largest value in the domain minus the largest value. Alternatively, equal to the largest value with positive probability mass non-zero minus the smallest value with probability mass non-zero. In any case, it might be infinite.

To compute this one usually finds the smallest and largest elements and substract them in \f{O(N)} time.

== Interquartile Range ==

A measure of variability equal to the \f{75}th percentile minus the \f{25}th percentile. These two numbers always exist. Equal to the H-Spread of the Box Plot.

Gives the range of the domain which contains \f{50\%} of the probability mass.

Related is the semi-interquartile range - which is equal to half of the interquartile range. In a symmetric distribution, the median +/- this is \f{50\%} of the distribution.

== Variance ==

A measure of variability equal to the average squared distance of every point in the domain to the mean of the domain. It is \f{\sigma^2 = E[(X - E[X])^2] = \int_D (x - E[X])^2 P(x) dx}.

For a dataset this is computed as \f{\sigma^2 = 1/N \sum (x_i - \mu)}.

If a dataset is a sample this is computed as \f{\s^2 = 1/(N-1) \sum (x_i - m)} - the previous formula underestimates the variance, because it would use the sample mean, itself an estimator. What happens is that we loose a "degree of freedom" when we estimate the mean. A signle point does not have a variance. If we have \f{N} points, and we select one point as the "base" and vary the position of all \f{N-1} points relative to this then the variance "changes". Hard to explain.

We also have \f{V[X] = E[X^2] - E[X]^2}.

A positive number always.

The sample statistic is not biased, and we have \f{E[s^2] = \sigma^2}. The population version is biased though, and we have \f{E[\sigma^2] = (N-1)/N \sigma^2}.

== Standard Deviation ==

The square root of the Variance.

Useful for normal or nearly normal distributions mostly (since it has a very clear mapping to the distribution shape and probability mass between standard deviations).

Also useful because it is in the "units" of the domain, rather than squared units.

A positive number always.

= Measures Of Skew =

A statistic which measures how skewed to the left or right a distribution is. Makes sense for ratio scales only.

== Parson's Index ==

A measure of the skew of a distribution equal to \f{3(Mean - Median) / StandardDeviation}. For positive skewed distributions this will usually be positive since the mean is larger than the median. Opposite for negative skew distributions.

== Third Moment About The Mean ==

A measure of the skew of a distribution equal to \f{\sum (X - \mu)^3 / \sigma^3}.

== Kurtosis ==

A measure of the skew of a distribution equal to \f{\sum (X - \mu)^4 / \sigma^4 - 3}. \f{3} is substracted to make the kurtosis of the normal distribution \f{0}.

= Z-Scores and Standardization =

Z Score of an observation \f{x} \f{Z(x)}: at how many standard deviations above (positive) or below (negative) the mean the observation is situated. Assume that \f{x \drawnfrom X}. Then \f{Z(x) = (x - \mean{X}) / \sd{X}} - the difference between \f{x} and the mean, divided by the standard deviation. If \f{X \distas \mathcal{N}[\mu,\sigma]} then \f{Z(X) \distas \mathcal{N}[0,1]}. The \f{Z} score is usually estimated by using the estimated mean and standard deviation.

An observation is more unusual than another if the magnitude of its Z score is larger than the magnitude of the others Z score. In a typical unimodal distribution, the probability of seeing such an observation is less than seeing a less unusual one.

It is, in many situations, better do standardize our variables so that \f{E[X] = 0} and \f{\text{Var}[X] = 1}. The way to achieve this is to do \f{(X - E[X]) / \text{SD}[X]}. This is most of the time not possible directly, therefore we use the estimates of the mean and standard deviation.

If we know \f{E[X]} then \f{\hat{X} \sim \mathcal{N}[E[X],\hat{\text{Var}}[\hat{X}]]} then \f{(\hat{X} - E[X]) / \text{SD}[\hat{X}] \sim t_{n-2}}, that is it follows the Student \f{t} distribution with \f{n - 2} degrees of freedom.

= Percentile =

Percentile of an observation \f{x} \f{P_\%(x)}: the percentage of observations with value lower than \f{x}. Assume that \f{x \drawnfrom X}. Then \f{P_\%(x) = P_X(X < x) = F_X(x)}. Alternatively we can use "lowar than or equal" and get \f{P_\%(x) = P_X(X \leq x) = F_X(x)} as well. The two definitions can lead to dramatically different results, esp where there is little data.

It is useful to compare percentiles instead of values to see who has a better score etc.

\f{i^{\text{th}}} percentile of a variable \f{X}: the domain value \f{q} such that \f{P_\%(x) = i}. It is the observation which would have assigned percentile \f{i}. The function \f{Q \colon [0,1] \rightarrow \mathbb{R}} assigns a domain value to each distinct percentile. We have \f{Q = P_\%^{-1}} or \f{Q = F_X^{-1}}. Again we have the problem of the two definitions.

Each observation has a rank - the number of observations smaller than or equal to it. Sort the observations. The element at position \f{i} in the resulting array has rank \f{i} - there are \f{i} elements smaller than or equal to it. Ranks can be turned into percentiles. For definition 1 of a percentile, given a percentile \f{p} (as a probability) then the rank is \f{r = \floor{p (N + 1)}}. \f{N+1} because we are considering the "spaces between observations" as the points we are interested in - where the percentile "is". The element at rank \f{r} is the required percentile. Notice that for \f{p = 1}, there is no element which is larger than all elements in the dataset, therefore an error is encountered. For definition 2 of a percentile, we simply compute the same rank \f{r} as before, but access element at position \f{r+1} in the sorted array. The better solution is to do linar interpolation. Compute \f{r} and find elements \f{x = a[r]} and \f{y = a[r+1]}. Then, compute \f{f = \frac{p (N+1)}} - the proportion of the rank towards \f{r} rather than \f{r+1}. The computed number is \f{t = x + f (y - x)} - the convex combination between \f{x} and \f{y}. We also have \f{x \leq y}, naturally. This is the computational definition of percentile we'll use. If \f{f} is \f{0} we obtain the natural result. The percentile of a dataset might be a number not necessarily in the dataset. Complexity is \f{O(N\log{N})} - dominated by sorting. There is an \f{O(N)} percentile finding algorithm. Good for finding few percentiles. If you want to find more, better sort first.

Percentile of an observation \f{x} \f{P_\%(x)}: the percentage of observations with value lower than \f{x}. Assume that \f{x \drawnfrom X}. Then \f{P_\%(x) = P_X(X < x) = F_X(x)}. This is usually estimated as the number of observations smaller than the current one in a sample.

\f{i^{\text{th}}} percentile of a variable \f{X}: the domain value \f{q} such that \f{P_\%(x) = i}. It is the observation which would have assigned percentile \f{i}. The function \f{Q \colon [0,1] \rightarrow \mathbb{R}} assigns a domain value to each distinct percentile. We have \f{Q = P_\%^{-1}} or \f{Q = F_X^{-1}}.

There is a one-to-one and monotonously increasing mapping between percentiles and Z scores. Given a Z score we can compute a percentile. Given a percentile we can compute a Z score (since there is one mean and any point on the real axis is assigned a single Z score and each point on the real axis also has a unique percentile - this breaks down however when the distribution assigns whole intervals probability \f{0} - every point in that interval has the same percentile, but a different Z score). This makes it easier to compute the percentile for certain distributions. The mapping from Z scores to percentiles is used, and only Z scores are computed.

= Effect Of A Linear Transformation =

If \f{X} is a random variable and \f{Y = aX + b} is a linear transformation of \f{X} then:
* The mean of \f{Y} is \f{E[Y] = aE[X] + b} - the mean of \f{X} transformed.
* The median of \f{Y} is \f{med[Y] = amed[Y] + b} - the median transformed - since the median is a point in the domain, it gets mapped to another point - no reason for the probabilities to change though.
* The variance of \f{Y} is is \f{V[Y] = a^2V[Y]} - the variance changes quadratically with the spread factor (to be expected) and does not depend on \f{b} - \f{b} only shifts the distribution, so it has no effect onthe variance, since this is around the mean, and the mean shifts by \f{b} as well.
* The standard deviation of \f{Y} is \f{S[Y] = aS[Y]} - expected, as well.

= Empirical Rule =

The \def{Empirical Rule}. For symmetric, unimodal and bell-shaped distributions, which occur a lot in practice, \f{68\%} of the data occur between \f{\overline{x} - s.d} and \f{\overline{x} + s.d}, \f{95\%} of the data occur between \f{\overline{x} - 2s.d} and \f{\overline{x} + 2s.d} and \f{99.7\%} of the data occur beteween \f{\overline{x} - 3s.d} and \f{\overline{x} + 3s.d}. The rule works surprisingly well for other distributions as well.
