Directed Graph: a set of nodes \f{V} (vertices) and a binary relation \f{A} on \f{V} (the set of arcs). There is a nice visualiztion as points in the plane connected by arcs. If \f{(a,b) \in A} then we can write \f{a \leftarrow b}. \f{a} is the tail and \f{b} is the head of the arc. \f{a} is a predecessor of \f{b} and \f{b} a successor of \f{a}. If \f{a \rightarrow a} we have a loop. As with trees, we can label the nodes of graphs. Don't confuse names with labels (which must be unique). 

A path is a list of nodes \f{(v_1,\dots,v_k)} such that we have \f{v_i \rightarrow v_{i+1}}. The length of the path is \f{k-1} - the number of arcs. Paths of length \f{0} are allowed (single nodes).

A cycle is a path for which \f{v_1 = v_k}. Cycles of length \f{0} are not allowed (single nodes). A cycling of the elements in the cycle produces an equivalent cycle.

A simple cycle is a cycle where each node appears only once, except the first and last which are identical.

If there is a cycle in a graph then there is at least one simple cycle: consider a cycle \f{(v_1,\dots,v_{k-1},v_1)}. There can be another \f{v_1} in the cycle \f{(v_1,\dots,v_{i-1},v_1,v_{i+1},\dots,v_{k-1},v_1)} or there might be another node \f{u} \f{(v_1,\dots,u,\dots,u,\dots,v_{k-1},v_1)}. In the first case we can form a smaller cycle \f{(v_1,v_{i+1},\dots,v_{k-1},v_1)} by dropping all before \f{v_{i-1}}, while in the second case we can drop everything between the \f{u} and have \f{(v_1,\dots,u,\dots,v_{k-1},v_1)} and still have a valid cycle, but of smaller langth. We repeat this process until we are left with a simple cycle.

Cyclic graph: a graph with one or more cycles (or with a simple cycle cf the previous theorem). Acyclic graph: a graph with no cycles.

An acyclic path (simple path?) is a path without nodes repeating, every node appears only once. Like in the previous argument, we can reduce a path to an acyclic path by replacing \f{(v_1,\dots,u,\dots,u,\dots,v_k)} with \f{(v_1,\dots,u,\dots,v_k)} and still have a valid path.

A graph can have up to \f{\abs{V}^2} arcs (size of carthesian product of \f{V} with itself, of which \f{A} is a subset) and as few as \f{0} arcs. If we count out self loops we have \f{\abs{V}(\abs{V}-1)} arcs. If the graph is acyclic it can have at most \f{\abs{V}-1} arcs, in which case it is a tree (it cannot have \f{\abs{V}} arcs, because we could then build a cycle. So \f{\abs{V}} is an upper bound. We can build a graph with \f{\abs{V}-1} arcs \f{v_1 \rightarrow v_2, \dots, v_{\abs{V}-1} \rightarrow v_{\abs{V}}} so this is an example of the upper bound).

The relation \f{\mathcal{R}} on the cycles of a graph which relates two equal length cycles if they are cyclic equivalents is an equivalence relation.

The relation \f{\mathcal{R}} on the nodes of a graph which relates two nodes if they are equal or if there is a cycle containing them is an equivalence relation (transitivity is trickier - \f{(v,\dots,u,\dots,v)} and \f{(u,\dots,z,\dots,u)} are two such cycles, then we can form \f{(v,d\ots,u,\dots,z,\dots,u,\dots,v)}).

The in-degree of a node is the number of arcs having that node as the head (coming into the node). future: number of times appears in the adjaceny list of some nodes and sum of column in adjacency matrix. The out-degree of a node is the number of arcs having that node as the tail (coming out of the node). future: Length of adjaceny list and sum of row in adjacency matrix. The in-degree of a graph is the maximum in-degree of any node and the out-degree of a node is the maximum out-degree of any node.

Representation of directed graphs with \f{\abs{V}} nodes.
* Adjacency lists: generalized characteristic vectors. We have a vector of length \f{\abs{V}}, indexed by node name, and with a list of nodes to which there is an arc from the current node as contents. Useful for graphs with few arcs - space is \f{O(\abs{V} + \abs{E})}. Checking if two nodes are adjacent is proportional to the number of successors (half of them on average or even \f{\abs{E}/\abs{V}}). Listing all successors is fast (proportional to number of successors - best we can find (even just return the pointer to the list in practice)). Finding predecessors involves looking at all the lists which is \f{O(\abs{V} + \abs{E})} (loss). Labels can be added as an extra piece of information in the characteristic vector. We can also add labels to arcs as an extra piece of information in the node structure for the linked lists.
* Adjacency matrix: a \f{\hctimes{\abs{V}}{\hctimes{V}}} matrix which holds \f{T} at position \f{(i,j)} if there is an arc from node \f{i} to node \f{j}, otherwise \f{F}. Row \f{i} is dedicated to connections for node \f{i}. Useful for graphs with many arcs - space is \f{O(\abs{V}^2)}. Checking if two nodes are adjacent is constant. Listing all successors is slot \f{O(\abs{V})}. Finding predecessors involves looking at column \f{i} which is \f{O(\abs{V})} (win).
* If \f{\abs{E} < \abs{E}^2/2} (in a rough approximation) we have a sparse case and prefer adjacency lists from the memory perspective. Other operations influence our decision though. Labels can be added as an extra array of information for each node. We can also add labels to arcs by using matrix of the label type instead of the adjacency matrix, and allowing one value from the label type to be invalid (signal no-arc).
* The adjacency matrix is a characteristic vector of characteristic vectors, whereas the adjacency list is a characteristic vector of lists.

Undirected Graph: a set of nodes \f{V} (vertices) and a set of two element subsets of \f{V} named \f{E} (the set of edges). A simplification of directed graphs where if there is an arc \f{a \rightarrow b} there is also the arc \f{b \rightarrow a}. Alternatively the associated relation \f{E} is symmetric. Some definitions allow self-edges while others do not. If \f{\{a,b\} \in E} then \f{a} and \f{b} are adjacent or neighbours.

A path is defined as for directed graphs. Length as well.

A simple cycle is a path of length three or more (four nodes or more) that has unique nodes excepting the first and last nodes. Since one can walk an edge in both directions, we have to exclude trival cases like \f{(a,b,a)} where \f{\{a,b\} \in E} or a palindrom path etc. Nonsimple cycles are not useful in such graphs. The concept of equivalent cycles is extended to include the path being reversed and various cyclic shifts of the revsersed path.

An undirected graph has \f{\abs{V}(\abs{V}-1)/2 = \choose{\abs{V}}{2}} edges at most (number of subsets of size \f{2} from \f{\abs{V}} and we don't have self loops). If the graph is acyclic it can have at most \f{\abs{V}-1} arcs, in which case it is a tree (it cannot have \f{\abs{V}} edges, because we could then build a cycle. So \f{\abs{V}} is an upper bound. We can build a graph with \f{\abs{V}-1} arcs \f{\{v_1, v_2\}, \dots, \{v_{\abs{V}-1}, v_{\abs{V}}\}} so this is an example of the upper bound).

The relation \f{\mathcal{R}} on the cycles of a graph which relates two equal length cycles if they are cyclic equivalents is an equivalence relation.

The degree of a node is the number of neighbours of a node. The degree of the graph \f{D(G)} is the maximum degree of any node.

Euler's Theorem: The sum of the degree of all the nodes is twice the number of edges. (proof by induction on \f{\abs{V}}, base has one node, no edges, it holds. If \f{\abs{V} > 1} we have a distinct node \f{i} connected by \f{k} edges to \f{k} nodes from an \f{\abs{V}-1} node subgraph with \f{L = \abs{V}-k} edges. The sum of edges is the sum of edges from the \f{\abs{V}-1} node subgraph plus \f{k} for node \f{i} plus \f{k} times \f{1} for each node with which \f{i} is connected to in the \f{\abs{V}-1} node subgraph, yielding \f{2\abs{V}} which is what we wanted).

Representation of undirected graphs with \f{\abs{V}} nodes.
* Adjacency lists: same discussion as for directed graphs only if \f{\{i,j\} \in E} then \f{i} appears in \f{j} adjacency list and vice-versa (the pair appears twice) - we're OK with that.
* Adjacency matrix: same discussion as for directed graphs only the matrix is symmetric.
* Labels are treated as for directed graphs.

Connected component in an undirected graph: a subset of the nodes of a graph, which are pairwise reachable, but any other node is not reachable (they are a maximal group of pairwise reachable nodes). The set of connected components is a partition of the node space and it is an equivalence relation.

Connected undirected graph: one where all nodes are in a single connected component. A component with \f{k} nodes has at least \f{n-1} edges. \f{S(1)} has \f{k=1} and \f{0} edges, so it holds. Assume \f{S(1),\dots,S(n-1)} holds. Let \f{q} be a node in the component. It has \f{l} neighbours \f{n_1,\dots,n_l}. If we were to remove the \f{l} connecting edges, we would remain with \f{C_1,\dots,C_p} connected components (\f{p \geq 1}) with \f{a_1,\dots,a_p} nodes (\f{a_i \leq n-1}). We have \f{\sum{a_i} = n-1}. But also for each component the number of edges \f{t_i \leq a_i-1}. Then we have \f{\sum{t_i} \geq \sum{a_i-1} = n-1 - p}. The total number of edges is then \f{\sum{t_i} + l \leq (n-1) + (l-p)}. Since \f{l \leq p} (more neighbours than connected because in the worst case the only connection was through the removed link) we have that the number of edges is greater than \f{n-1} which is what we wanted.

Algorithm for connected components: start out with a set of \f{\abs{V}} connected components and process all edges of the graph, in order. If an edge connects two nodes not in the same component, join the two connected components. Otherwise do nothing, because nothing changes. We build \f{G_0,\dots,G_{\abs{E}}} sets of subsets of nodes this way, and \f{G_{\abs{E}}} is the partition we're looking for. Induction statement: at step \f{i} we have all the connected components of a graph with the same nodes as our original, but only edg
es \f{\hcrange{1}{i}}. If we use a disjoint set data structure for this, we have \f{O(\abs{E}\log{\abs{V}})} complexity.

Disjoint set / union-find data model: suppose we have an universe set \f{U}, which is finite, and we want to represent disjoint sets. Initially each element is its own set, and we want to determine what set an element is in and merge two sets. Can use: arrays, lists etc. Most common implementation: a tree structure. You can get a hold of the pointer to the node of each element. Each node has a pointer just to the parent, a subset label, which is valid only for the root node, and a height counter. Searching for the label of a node means going through parents until a root is reached (we have a forest actually). This is in complexity \f{O(h)} where \f{h} is the height of the respective (we must bound the largest height of any such tree). Merging two subsets means adding the root of the tree of smaller height as a child of the root of the tree with larger height. It is constant time if we have pointers to the roots, else \f{O(h)} as well. We increment the height of the added-to root node only if the two trees have the same height, otherwise the height does not increase. 
* We have that \f{h = O(\log{n})}. Statement \f{S(h)}: a tree of height \f{h} has at least \f{2^h} nodes. \f{S(0)}: a tree of height \f{0} is a single node tree with \f{2^1 = 1} nodes. Assume \f{S(h)} holds for \f{h \geq 0}. We have a tree of height \f{h+1}. It must have one subtree \f{A} at least, of height \f{h}. By \f{S(h)} this has at least \f{2^h} elements. Now, when \f{A} was added, the tree had height \f{h+1} already, therefore it had a subtree \f{B} with height \f{h} and \f{2^h} nodes by \f{S(h)}, or it was a tree of height \f{h}, which got bumped to \f{h+1} by adding the new tree. In this case, it had by itself \f{2^h} nodes by \f{S(h)}. In both cases we get that we have more than \f{2^h + 2^h = 2^{h+1}} nodes, therefore the statement holds. Thus \f{n \geq 2^h} or \f{h \leq \log{n}} or \f{h = O(\log{n})}.
* Same things apply if we join by number of elements rather than height (even maybe with every bijectiv(ish) mapping of the height).

A spanning tree of an connected undirected graph is a tree with the same nodes as the graph, but only the subset of edges with ensure the resulting graph is connected, and there are no cycles. It is a tree in the most general sense. A spanning forest is the set of spanning trees for each connected component of a unidrected but unconnected graph. If each edge has a weight we can talk about minimum spanning trees or minimum spanning forests (selects the edges with yield lower sum of weights).

Kruskal's algorithm: a derivation of the connected components algorithm. We start with all nodes forming their separate spanning forests. We then walk each edge in order of cost and, if it connects two disjoint spanning trees, we add it to the result spanning forest. If it is between two nodes in the same tree, we don't do anything. If we use a union-find data structure, this takes at most \f{O(m\log{m})} to sort the edges and \f{O(m\log{n})} to perform the main algorithm (and \f{O(m)} for the building of the spanning forest). Since \f{m \leq n^2} we have \f{\log{m} \leq 2\log{n}} and we have total time \f{O(m\log{n})}. At each edge we do searches for the two nodes. If they end up in the same component, we don't do anything, otherwise we merge. Adjacency list representation is assumed. Total space is \f{O(n)} for the output tree, \f{O(2n)} for the disjoint set data structure.
* Proof for a single component. Suppose by contradiction that the tree we obtain is not the MST one. Let \f{T} be the Kruskal tree and \f{T'} be the actual smallest cost tree. The two trees share some edges but have some differing edges. Let \f{e} be the smallest cost edge which is in \f{T} but not in \f{T'} (that is, for all edges of cost lower than \f{e} in the sorted order, either both \f{T} and \f{T'} have them or they both don'). \f{e} connects \f{(x,y)}. In \f{T'} these two must also be connected, and there is a path \f{(x,a_1,\dots,a_k,y)} with all edges in \f{T'} which connects them. Not all edges are in \f{T} though, otherwise we'd have a cycle. We claim that at least one of these edges is of higher cost than \f{e}. Suppose they were all of smaller cost. Then they would be considered by Kruskal's algorithm before \f{e}. But they appear both in \f{T} and \f{T'} since they appear before \f{e} (and \f{e} is what it is). Therefore we have two paths which connect \f{x} and \f{y} and therefore a cycle, which is a contradiction. Let \f{e'} be a higher cost edge and it connects \f{x'} to \f{y'}. But, if we remove this edge from \f{T'}, then it is no longer connected. But, if we add \f{e} to \f{T} then we restore connectivity (if \f{a -- x' - y' -- b} was a path then we now have the path \f{a -- x' -- x - y -- y' -- b}) and add no cycles (because we're a fully connected graph with \f{n-1} edges which implies we're a tree and have no cycles). But this tree is of even lower cost than \f{T'}, because the cost of \f{e} is smaller than the cost of \f{e'}. Therefore, we have a contradiction. The tree produces by Kruskal is optimal. For multiple components, we the optimal forest is the forest of optimal trees on each component, and since Kruskal produces those we're good.
* We can build a variant of this with a min-queue as a backing structure. To construct it is \f{O(m)}. Extracting all elements is \f{O(m\log{m}+m) = O(m\log{m})}. If we only want \f{m/\log{m}} components of the connected component we get \f{O(m)} time (we don't have to sort, therefore cheaper time).

Euler circuit: a path that starts and ends at the same node and has all edges exactly once.
A path has an Euler circuit iff it has an even number of edges per node (did not prove this).

Depth-First Search: a form of exploration of a directed graph. We start at a node, and visit it's first child, and that child's first child etc. At every visit, we mark the node as such. When a first child is already visited, we visit the second child etc. Intuitively, this will mark as visited all nodes which are descendants of the initial node (assume a descendant hasn't been visited at the end, then there is a path from the node to the descendant, and at some point we switch from visited to not-visited. But, when we saw the visited node, with unvisited child, our algorithm should have gone to the non-visited node, therefore we have a contradiction). We scan all nodes and start the dfs process from them, if unvisited, in order to visit every node (all nodes are visited - either they were unvisited and starting the dfs process visits them, or they were a descendent of a previous node and are already visited). The running time is \f{O(\abs{V} + \abs{E})} - we visit every node at most once (switching from unvisited to visited) and for each node we scan its edges (\f{\abs{V} + \sum_{i=1}^{\abs{V}}d_{+}(i))}).
* If we consider the edges which are followed because a child is un-visited, we end up with a depth first forest which includes every node, after the dfs algorithm has ended, which parallels the exploration process.
* The arcs of a graph can be partitioned into: tree arcs (they are included in the depth first forest), forward arcs (not in the dfs forest, \f{u \rightarrow v}, \f{v} is a proper descendent of \f{u}, but not a child), bacwards arcs (not in the dfs forest, \f{u \rightarrow v}, \f{u} is a descendent of \f{v} (\f{u=v} might happen - loop), cross arcs (not in the dfs forest, \f{u \rightarrow v}, such that neither is an ancestor of either).
* The DFS forest is ordered, as determined by the order of edges in the graph representation. As such, cross arcs can only go from left-to-right (When we DFS explore the tree we reach \f{u}, we then scan all its children. Say \f{v} has already been added and \f{u \rightarrow v} is a cross edge. No sibling of \f{u} to the right of it has been added, as well as any other node going up to the right, according to DFS, so only siblings to the left of \f{u} could have added it. Therefore \f{u \rightarrow v} is an edge to the left of the tree). This carries over to the whole forest.
* We can assign, post-order, a number to each node. In a DFS tree, the number assigned to the leftmost child subtree are smaller than the next child subtree etc, which are all smaller than the one for the root. These numbers correspond to the times dfs is active as well. Furthermore a call to a righter-child cannot begin until a call to a lefter child has ended. Whenever \f{v} is to the left of \f{p} the postorder number of \f{v} is smaller than the postorder number of \f{w} (\f{p(v) < p(w)}). If \f{u \rightarrow v} is a tree arc, then \f{p(v) < p(u)}. If \f{u \rightarrow v} is a forward arc, then \f{p(v) < p(u)}. If \f{u \righatrrow v} is a backward arc, and \f{u \neq v} then \f{p(v) > p(u)}, or in general \f{p(v) \geq p(u)}. If \f{u \rightarrow v} is a cross arc then \f{p(v) < p(u)}. Backwards arcs make a discording note, and are thus easy to identify.
* Tree edges are also easily found: they are the edges which, when doing the dfs, allow movement from one node to another.

Finding cycles in graphs: application of DFS and backward arc detection (which can be done by inspecting each edge and the postorder number assigned). A graph has a cycle iff it has a backward arc. Complexity is \f{O(\abs{V} + \abs{E})}.

Directed Acyclic Graph: A directed graph with no cycles. Only graphs that can be topological sorted.

Topological order of a DAG: an ordering of the nodes of a graph, such that a node \f{i} comes before a node \f{j} if there is no edge \f{j \rightarrow i}. In other words, all edges starting from \f{i} end in nodes which appear after \f{i}.
* Topological sort of a DAG: the process of obtaining a topological sort of the nodes of a a graph. A DFS can be used for this as well, with the post-numbering. Whenever we assign a number, we also push the node onto a stack, and at the end we pop the stack. The order of the nodes after popping is the topological sort we want. Node \f{i} in this order will have post-number \f{\abs{V} - i}. Since there are no backedges, all edges that remain are tree, forward and cross edges, which all start from \f{i} and end in nodes of lower post-order number, therefore lower in the stack and later popped. The complexity is \f{O(\abs{V} + \abs{E})}. Also, if a back-edge is detected, we know the graph is not a DAG and the topolofical sort is meaningless.
* Topological sorting corresponds to the relation topological sorting, if the graph is seen as a relation. There are more than one topological orders, and this algorithm finds one. Given a topological order, we can extend a partial order to a total order defined as \f{v_iTv_j} when \f{i \leq j} with all elements comparable and, furthermore, if \f{v_iRv_j} then \f{v_iTv_j} as well, since the order keeps that. Any pair in \f{R} will be a pair in \f{T} because of this.

Reachable set of a node in a graph: the set of nodes for which a path exists from that node to them. The DFS gives us a way to find this set: all nodes marked as visited from a DFS starting from that node are the reachable set of a node. Can be done in \f{O(\abs{V} + \abs{E})} or \f{O(\abs{V}^2 + \abs{V}\abs{E})} for all nodes.

Another algorithm for connected components of an undirected graph: transform the graph into a directed one by transform each edge \f{\{u,v\}} into arcs \f{u \rightarrow v} and \f{v \rightarrow u} and run DFS on it. Each connected component consists of a DFS tree in the DFS forest found. Proof: we run DFS and find a forest. Consider nodes \f{u} and \f{v} in one tree. Then there is a path of tree links \f{u \rightarrow a_1 \rightarrow \dots a_k \rightarrow v} where \f{k \geq 0}. But we also have the corresponding mirrored arcs. Therefore we have a path in the undirected graph and the two nodes are reachable and in the same component. Now, consider two nodes in the same component \f{u} and \f{v}. Then, when DFS is run on the transformed graph, either \f{u} or \f{v} is processed first. Suppose \f{u} is. Then, because there is a path from \f{u} to \f{v} and \f{v} is not yet processed, \f{v} must be processed later, but before \f{u} is fully processed, therefore there will be a tree path to \f{v}, making them both part of the same tree.

Shortest path from \f{s} in a DAG: Do a topological sort and obtain an order. Initialize an array of length \f{\abs{V}} to \f{+\infty}. Start from node \f{s} in the order and process all following nodes in order. All previous nodes are unreachable (otherwise there would be an edge which violates the order). For a given node \f{i}, if, when we reach it, it has cost \f{+\infty} we don't do anything, since it isn't reachable from \f{i}. Otherwise, it is reachable from \f{i}, and whatever cost it has is the minimum cost. We then look at all the edges out of \f{i}. Let \f{i \rightarrow j} and we have \f{i < j} because of the order. We replace the cost of \f{j} with \f{\min(c(i) + \pi(i \rightarrow j), c(j))}. We also record \f{b(j) = i} so we know that we went through node \f{i} to get here (and we can reconstruct the links). Proof by induction. \f{c(s)} will be \f{0}, which is the smallest cost possible. Suppose we reach node \f{i} in the list. Let \f{q_1,\dots,q_k} be the nodes which have incoming edges to \f{i}. If \f{k = 0}, then the cost is \f{+\infty} which is right. Otherwise, consider the found path \f{s \rightarrow \dots \rightarrow q_l \rightarrow i} and say there is another path \f{s \rightarrow \dots \rightarrow q_o \rightarrow i} of smallest cost. This cost is smaller than \f{c(q_o) + \pi(q_o,i)} since, by our assumption, \f{c(q_l) + \pi(q_l,i) < c(q_o) + \pi(q_o,i)}. But this means, that the path does not enter into \f{q_o} from the same node found by our algorithm (otherwise it would have the same cost). But, by the induction assumption, the edge it does enter through is not optimal, and we could replace it with the optimal path, thus obtaining a lower cost. This is a contradiction, therefore we have what we want.Notice also that no edge after \f{i} can point back to \f{i}, so we have no more updates to do. The running time is \f{O(\abs{V} + \abs{E})}. This is an instance of a Dynamic Programming algorithm. The tree form starts with the last node in the sort and looks at all incoming edges, selecting the minimum cost one according to the previous form. Can be efficiently solved in a more general case. We can find the shortest path to \f{s} by scanning the nodes before \f{s} and applying the same algorithm and the same reasoning.

Longest path from \f{s} in a DAG: Do the same thing as for the shortest path, only initialize with \f{-\infty} and use \f{\max} instead of \f{\min}. The same argument holds. Cannot be efficiently solved in a more general case. We can find the longest path to \f{s} by scanning the nodes before \f{s} and applying the same algorithm and the same reasoning.

Minimum distance in a directed/undirected graph between two nodes \f{u} and \f{v} is the minium cost of any path between \f{u} and \f{v}, where the cost is computed as the sum of the edges in the patah.

Sortest path from \f{s} in a general graph. Finding the minimum distance path from \f{s} to all other nodes in a graph. Interesting situations: positive weights, negative weights. In the first case cycles are elminated (as before) since they can only make a path worse.

Dijkstra's Algorithm: Computes the shortest path from \f{s} in a directed graph with only positive weights.

Complete graph \f{K_n}: an undirected graph with an edge between each node. It has \f{\choose{n}{2} = n(n-1)} edges.

Bipartite graph: an undirected graph with \f{V} partitioned into \f{V = A \cup B} and any edge has one node in \f{A} and one in \f{B}, and there are no edges between \f{A} and \f{A} or \f{B} and \f{B}.

Complete bipartite graph \f{K_{n,m}}: an bipartite graph where \f{\abs{A} = n} and \f{\abs{B} = m} and with an edge between each node in the first group to each node in the second group, but no other edges. It has \f{nm} edges. We also have \f{\abs{E(K_{n+m})} > \abs{E(K_{n,m})}}.

Complete directed graph: a directed graph with an arcs between each node. It has \f{n^2} arcs. If no self-loops are allowed, it has \f{n(n-1)} arcs.

Planar graph: an undirected graph for which it it possible to place the nodes on a plane and draw its edges as lines which do not intersect. A nonplanar graph is a graph which is not planar.
* \f{K_1,K_2,K_3,K_4,K_{0,\star},K_{1,\star},K_{2,\star},K_{\star,0},K_{\star,1},K_{\star,2}} are planar graphs.
* \f{K_5} and \f{K_{3,3}} are "the simplest" non-planar graphs. In general \f{K_n} with \f{n \geq 5} and \f{K_{n,m}} with \f{n \leq 3} and \f{m \leq 3} are not planar.
* Trees are planar graphs.
* Kuratowski's Theorem: Every nonplanar graph contains a copy of \f{K_5} or \f{K_{3,3}}, where we might replace an edge in the two graphs with a path in the target graph.
* It is desirable to build a planar representation of a graph, or if the graph is not planar, a representation with as few crossings as possible, in many applications.

Graph coloring: assigning a color/label from a set \f{C} to each node, so that no two nodes connected by an edge are assigned the same color. The minimum number of colors required to color a graph is the chromatic number \f{\Chi(G)}. A graph which can be colorable with no more than \f{k} colors is \f{k-colorable}.
* \f{\Chi(K_n) = n} (no two nodes may have the same color since there is an edge between them). \f{K_n} is \f{k}-colorable for \f{k \geq n}.
* If \f{G} is bipartite then \f{\Chi(G) = 2} (all nodes in \f{A} are one color and all in \f{B} the other color, all edges are between nodes of different color, by definition).
* To find exact minimal colorings, one must try all combinations. Grows exponentially.
* Finding maximally independent sets of nodes is a good heuristic for approximate minimal colorings.
* Applications include scheduling courses (edge between two courses if they have students in common) - find chromatic number of the graph to find out minimum number of time slots.
* The chromatic number of a tree is \f{2}.

Clique: a subset of the nodes of \f{G} such that there is an edge between every two nodes in the subset. A \f{k}-clique is a clique of \f{k} nodes. The size of the largest clique is the clique number \f{C_l(G)} of the graph. A maximal clique is a clique which is not a subset of some larger clique.
* A clique of size \f{k} is an embedding of \f{K_k} in the graph.
* Applications include scheduling courses (edge between two courses if the do not have students in common) - finding a maximal clique gives us the most courses we can schedule at one time. Finding all such cliques gives us a partition of courses where a class means courses with no dependencies.

\f{C_l(G) \leq D(G)} (the largest clique can at most include the node with the largest degree, so this is a bound on the clique number. If a clique had a higher number, then it would contain nodes of higher degree - a contradiction).
\f{C_l(G) \leq \Chi(G)} (the largest clique will need \f{C_l(G)} colors because it is an embedding of \f{K_{C_l(G)}}).

For a bipartite graph, every even length path that starts in \f{A} also ends in \f{A}, while every odd path ends in \f{B}.

Algorithm for telling if a graph is bipartite: hold an \f{\abs{V}} length tri-state array, initially set to \f{null}. Mark the first node as \f{A}. Suppose we have only one connected component. We look at all the edges from the \f{A} nodes and add all incident nodes as \f{B} nodes. We then look at all edges of \f{B} nodes and add all as \f{A}. We continue until we've touched all nodes. If at any time we see a mismatch, the graph is not bipartite. If we do no see a mismatch, then the graph is bipartite. We look at all edges and nodes therefore \f{O(\abs{V}+\abs{E})} is the complexity. Better thought out as a BFS of the graph. Eventually all nodes will be seen. Each level alternates the label \f{A} or \f{B}. If we find a backlink (basically) to a same level node, we bail.

In a DAG, at least one node has \f{0} indegree and at least one node has \f{0} outdegree. Proof by contradiction: walk from one node to a certain child node. You'll end up exploring every node in a connected component. At the last node in that, we still have outdegree 1, so we can go to an already visited node - cycle. Use graph transpose for indegree proof.

In a DAG, we can do a topological sort by first selecting the one node with \f{0} incoming edges. This node exists and we place it as the first node. We remove this node from G and are left with \f{G_1} which is a valid DAG. We repeat this procedure for the next node of in-degree \f{0} from \f{G_1}. We do this n times until we run out of nodes. We obtain the order \f{v_{i_1},v_{i_2},\dots,v_{i_n}}. A different procedure by Tarjan (1976) has the added benefit that when a node \f{v_{i_j}} is added, all nodes on which it depends (its ancestors) have been added to the list.

