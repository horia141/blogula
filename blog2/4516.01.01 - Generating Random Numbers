Generating random numbers is an important task for many data analysis problems.

We deal primarily with \def{pseudo-randomn number generators}, which are algorithms which produce sequences of numbers which are sufficiently random for the purposes at hand.

= Uniform Random Numbers In \f{[0,1)} Or \f{\range{0}{n}} =

= Standard Univariate Distributions =

= Arbitrary Discrete Distribution =

Problem: we want to sample from a given discrete probability distribution \f{p} on \f{\hcrange{1}{n}} defined by \f{p_1, p_2, \dots, p_i} (with \f{p_i \geq 0} and \f{\sum_{i=1}^n p_i = 1}).

Assumptions:
* Our computing model works with real-number words of finite precision (IEEE-754 doubles).
* We can add, substract, multiply, divide and compare the real-number words we work with in constant time. 
* We can generate a uniform random number in the interval \f{[0,1)} in constant time.
* We can compute the integer floor of a real number in constant time (!!!does not generalizez for arbitrary real-numbers!!!).
* We measure space complexity for everything we use above the space required to hold \f{p}.

== Algorithm For Sampling From A Uniform Distribution ==

Take the interval \f{[0,1)} and divide it into \f{n} equal buckets. Generate a random number \f{x} in \f{[0,1)} uniformly. The bucket in which the number fell is our sampled number. We want to return the number \f{i} such that \f{i/n \leq x} but \f{(i+1)/n > n}. This means that \f{i \leq nx} and furthermore \f{i = \floor{nx}}. Actual implementation is (1) \f{x \leftarrow \text{rand01}()} and (2) \f{return \floor{nx}}. No setup time or space complexity. Sampling time complexity \f{\Theta(1)} and space complexity \f{\Theta(1)}.

== Algorithm For Sampling From A Distribution Where \f{p_i} Are Rational (\f{p_i = r_i / q_i}) ==

Compute the LCM of \f{p_i} (\f{L}) and divide the interval \f{[0,1)} into \f{L} equal buckets. Each bucket is assigned to one number. We then use the first algorithm to generate an integer in \f{\hcrange{1}{L}} and return the number from \f{\hcrange{1}{n}} assigned to that bucket. We need to build an array \f{A} of size \f{L} which holds for element \f{i} the number in \f{\hcrange{1}{n}} to which the respective bucket is associated. The first \f{p_i L = r_i L / q_i} buckets (an integer) are assigned \f{1} etc.

Actual implementation is (1) \f{i \leftarrow \text{first_algorithm}(L)} and (2) \f{\text{return} A[i]}. Worst case bounds for \f{L} are \f{O(\sum_{i=1}^n q_i)} - this can get pretty huge in bad cases therefore the method is not good for most situations - only when we know \f{p} in advance. Setup time/space is \f{O(n)} in best case (uniform) and \f{O(\sum_{i=1}^n q_i)} in worst case. Sampling time/space complexity is \f{\Theta(1)}.

== Algorithm For Sampling With A Biased Coind ==

Take the interval \f{[0,1)} and divide it into \f{[0,p_1)} and \f{[p_1,1)}. Generate a random number \f{x} in \f{[0,1)} uniformely. If the number is smaller than \f{p_1} return \f{1} else return \f{2}. Actual implementation is (1) \f{x \leftarrow \text{rand01}()} and (2) \f{\text{return} x < p_1}. No setup time or space complexity. Sampling time/space complexity is \f{\Theta(1)}.

= Standard Multivariate Distributions =

== Special Multivariate Cases ==

=== Uniformly Generating Points On The Unit Sphere ===

Want to generate points uniformly distributed on the \f{n}-dimensional unit sphere. Applications - choose hyperplanes uniformly distributed.
The problem has been much studied.
The winner problem (by Muller) is - generate a \f{n}-vector with components drawn from independent standard normals and normalize it. The result is that the points are uniformly distributed on the \f{n-1} dimensional surface of the unit sphere.

This has to generate more random numbers than other methods, but it is intuitive and relatively cheap to use on modern computers.

= The Nature Of Randomness =

Understanding what is meant by "random" insofar as randomness is needed in your application is critical to correctly figuring out what kind of pseudorandom number generator you need. All pseudorandom number generators are deterministic, so it is possible to predict all future outputs of a PRNG. In the literature, there are at least 4 different notions of randomness: statistical, cryptographic, algorithmic, and physical. The first two are important for practical applications. The third, also known as Chaitin-Kolmogorov randomness, is mostly of theoretical interest. The last, of course, is not pseudorandomness but true randomness.

\def{Statistical Randomness} A pseudorandom number generator is said to be statistically random if its output passes any statistical test. Such generators may be safely used in simulations, Monte Carlo experiments, etc, since they will not skew the results of the experiment.

\def{Cryptographic Randomness} A pseudorandom number generator is said to be cryptographically secure if, by observing a polynomial-sized number of outputs, no polynomial-time predicate of the next output can be predicted with noticeably greater than random chance. If the generator outputs one bit at a time, this means that after observing the generator's output, no efficient algorithm exists to predict the value of the next output bit with success probability greater than 1/2 plus epsilon. Here "polynomial-sized" and "efficient" is relative to the size of the initial seed. This is an informal statement of Yao's next bit test. The definition involves polynomial-time computable predicates because otherwise a generator that outputs a 32-bit word at a time may output values that are unpredictable as a whole, but, for example, the low-order bit might be easily determined. Such generators may be safely used in any adversarial context, including cryptographic key / nonce generation.

\def{Chaitin-Kolmogorov Randomness} Chaitin and Kolmogorov independently came up with this characterization, also called algorithmic randomness, of random strings. It captures intuition that most people have about randomness: the bit string "0000000000000000" is often considered less random than "0100101001011101", despite the fact that both may have been the output of the same pseudorandom number generator or even the output of a source of physical randomness. Indeed, a physically random source such as a fair coin flipped 16 times would output both strings with equal probability. The algorithmic randomness of a string is the length of the shortest program which would output the string. It doesn't matter what programming language the definition is relative to, since for sufficiently long strings it doesn't matter: the difference is a "small" constant, since one could prefix the program in language B with an interpreter for language B written in language A.

\def{Physical Randomness} Many sources of "physical randomness" is random only relative to the available computation power. For example, if we were to flip a fair coin to obtain a random bit, an adversary who can train a high frame rate camera or similar sensor on the coin as it flew through the air and has enough computational power to analyze, in real time, the physics of the coin and its interactions with the hand that is about to catch it is likely to be able to predict the coin toss with greater than 1/2 + epsilon probability of success. Other sources make use of quantum mechanical uncertainty. An often used example involves measuring the junction noise of a reverse-biased zener diode (due to quantum mechanical tunneling).
