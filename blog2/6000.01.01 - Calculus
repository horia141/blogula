Unify treatment of functions so more of the theorems are "on an interval" (increasing/decreasing/convexity etc.).
Unify pasting theorems (plaement and format).

= Limits =

Let \f{f} be a \ref{numeric function} and \f{a}, \f{b} and \f{l} numbers.

\f{f} \def{approaches the limit} \f{l} at \f{a}, or \f{\lim_{a}f(x) = l} for every \f{\epsilon > 0} there is some \f{\delta > 0} such that, for all \f{x}, if \f{0 < \abs{x - a} < \delta}, then \f{\abs{f(x) - l} < \epsilon}. The existance of a limit at \f{a} tells something about the local behaviour of the function - you can get closer and closer to \f{l} in the codomain by selecting smaller and smaller intervals around \f{a} in the domain. \f{f} \def{approaches the limit} \f{l} at \f{\pm\infty}, or \f{\lim_{\pm\infty}f(x) = l} for every \f{\epsilon > 0} there is some \f{N > 0} such that, for all \f{x}, if \f{x > N} (or \f{x < N)}, then \f{\abs{f(x) - l} < \epsilon}. The existance of a \def{limit at infinity} tells something about the \def{asymptotic behaviour} of the function - you can get closer and closer to \f{l} in the codomain by selecting larger and larger (or smaller and smaller) points in the domain. \f{f} \def{approaches} \f{\pm\infty} at \f{a}, or \f{\lim_{a}f(x) = \pm\infty} if for every \f{N} there is some \f{\delta > 0} such that, for all \f{x}, if \f{0 < \abs{x - a} < \delta} then \f{f(x) > N} (or \f{f(x) < N}). The existance of an \def{infinity limit} at \f{a} tells somthing about the local behaviour of the function - you can get larger and larger (or smaller and smaller) values in the codomain by selecting smaller and smaller intervals around \f{a} in the domain. \f{f} \def{approaches} \f{\pm\infty} at \f{\pm\infty}, or \f{\lim_{\pm\infty}f(x) = \pm\infty} if every \f{N} there is some \f{M > 0} such that, for all \f{x}, if \f{x > M} (or \f{x < -M}), then \f{\abs{f(x)} > N} (or \f{\abs{f(x)} < N}). The existance of an \def{infinite limit at infinity} tells something about the asymptotic behaviour of the function - you can get larger and larger (or smaller and smaller) values in the codomain by selecting larger and larger (or smaller and smaller) points in the domain.

In general, the limit at \f{a} might not exist, or if it exists, it might be infinite, or if it is not infinite, it might be different from \f{f(a)}. In the same manner, the limit at infinity might not exist or it  might be infinite. When dealing with combinations of functions, some of which might exist, and some of which might be infinite, care must be taken.

\f{f} \def{approaches the limit} \f{l} from below at \f{a}, or \f{\lim_{a^-} f(x) = l} if for every \f{\epsilon > 0} there is some \f{\delta > 0} such that, for all \f{x}, if \f{0 < a - x < \delta}, then \f{\abs{f(x) - l} < \epsilon}. The existance of a limit at \f{a} from below tells something about the local behaviour of the function - you can get closer and closter to \f{l} in the codomain by selecting smaller and smaller intervals to the left of \f{a} in the domain. If \f{f} \def{approaches} \f{\pm\infty} from below at \f{a}, or \f{\lim_{a^-} f(x) = \pm\infty} if for every \f{N} there is some \f{\delta > 0} such that, for all \f{x}, if \f{0 < a - x < \delta}, then \f{f(x) > N} (or \f{f(x) < N}). The existance of an \def{infinity limit} at \f{a} from below tells something about the local behaviour of the function - you can get larger and larger (or smaller and smaller) values in the codomain by selecting smaller and smaller intervals to the left of \f{a} in the domain. \f{f} \def{approaches the limit} \f{l} from above at \f{a}, or \f{\lim_{a^+} f(x) = l} if for every \f{\epsilon > 0} there is some \f{\delta > 0} such that, for all \f{x}, if \f{0 < x - a < \delta}, then \f{\abs{f(x) - l} < \epsilon}. The existance of a limit at \f{a} from above tells something about the local behaviour of the function - you can get closer and closter to \f{l} in the codomain by selecting smaller and smaller intervals to the right of \f{a} in the domain. \f{f} \def{approaches} \f{\pm\infty} from below at \f{a}, or \f{\lim_{a^+} f(x) = \pm\infty} if for every \f{N} there is some \f{\delta > 0} such that, for all \f{x}, if \f{0 < x - a < \delta}, then \f{f(x) > N} (or \f{f(x) < N}). The existance of an \def{infinity limit} at \f{a} from above tells something about the local behaviour of the function - you can get larger and larger (or smaller and smaller) values in the codomain by selecting smaller and smaller intervals to the right of \f{a} in the domain.

In general, the limits from above and below at \f{a} might not exist, or if they exist, they might be infinite, or if they are not infinite, they might be different from \f{f(a)} or different from each other.

We will use \f{\lim_{a^\star}} to refer to any of the limits \f{\lim_{a}}, \f{\lim_{a^+}}, \f{\lim_{a^-}}, \f{\lim{+\infty}} or \f{\lim_{-\infty}}. We will use \f{D^\star(a,\delta)} to refer to any of the intervals \f{(a - \delta,a + \delta)\setminus\{a\}}, \f{(a - \delta,a)}, \f{(a,a + \delta)}, \f{(\delta,+\infty)} or \f{(-\infty,\delta)}. We will use \f{C(l,\epsilon)} to refer to any of the intervals \f{(l - \epsilon,l + \epsilon)} when \f{l} is a number, \f{(\epsilon,+\infty)} when \f{l = +\infty} and \f{\epsilon > 0} or \f{(-\infty,-\epsilon)} when \f{l = -\infty} and \f{\epsilon > 0}.

Assume \f{\lim_{a^\star}f(x)} exists. Then \f{\lim_{a^\star}f(x)} is unique.

First, we will deal with the case where \f{\lim_{a^\star}f(x)} is finite. Suppose, by contradiction, that there are two numbers \f{l} and \f{q} with \f{l \neq q} and \f{\lim_{a^\star}f(x) = l} and \f{\lim_{a^\star}f(x) = q}. We choose \f{\epsilon = (l - q) / 2} and find \f{\delta_l} such that for all \f{x \in D^\star(a,\delta_l)} we have \f{\abs{f(x) - l} < \epsilon} and \f{\delta_q} such that for all \f{x \in D^\star(a,\delta_q)} we have \f{\abs{f(x) - q} < \epsilon}. We choose \f{\delta = \min(\delta_l,\delta_q)} which has the same properties as \f{\delta_l} and \f{\delta_q}. However, \f{(q-\epsilon,q+\epsilon) \perp (l-\epsilon,l+\epsilon)}, but any \f{x \in D^\star(a,\delta)} is mapped to both. Therefore, we have a contradiction, and our desired proof for this case.
Second, we will deal with the case where \f{\lim_{a^\star}f(x)} is positive infinity and the alternative limit is finite. Suppose, by contradiction, that there is a number \f{l} with \f{\lim_{a^\star}f(x) = l}. We choose any \f{\epsilon > 0} and find \f{\delta_{+\infty}} such that for all \f{x \in D^\star(a,\delta_{+\infty})} we have \f{f(x) > \epsilon} and \f{\delta_l} such that for all \f{x \in D^\star(a,\delta_l)} we have \f{\abs{f(x) - l} < \epsilon}. We choose \f{\delta = \min(\delta_{+\infty},\delta_l)} which has the same properties as \f{\delta_{+\infty}} and \f{\delta_l}. However, \f{(l-\epsilon,l+\epsilon) \perp (\epsilon,+\infty)}, but any \f{x \in D^\star(a,\delta)} is mapped to both. Therefore, we have a contradiction, and our desired proof for this case.
Third, we will deal with the case where \f{\lim_{a^\star}f(x)} is positive infinity and the alternative limit is negative infinity. Suppose, by contradiction, that this is the case. We choose any \f{\epsilon} and find \f{\delta_{+\infty}} such that for all \f{x \in D^\star(a,\delta_{+\infty})} we have \f{f(x) > \epsilon} and \f{\delta_{-\infty}} such that for all \f{x \in D^\star(a,\delta_{-\infty})} we have \f{f(x) < \epsilon}. We choose \f{\delta = \min(\delta_{+\infty},\delta_{-\infty})} which has the same properties as \f{\delta_{+\infty}} and \f{\delta_{-\infty}}. However, \f{(-\infty,\epsilon) \perp (\epsilon,+\infty)}, but any \f{x \in D^\star(a,\delta)} is mapped to both. Therefore, we have a contradiction, and our desired proof for this case. The fourth case, where \f{\lim_{a^\star}f(x)} is negative infinity and the alternative limit is finite, and the fifth case, where \f{\lim_{a^\star}f(x)} is negative infinity and the alternative limit is positive infinity, are dealt with in the same manner.

Some very basic relationships, useful for other proofs:
* If \f{\abs{x - x_0} < \epsilon/2} and \f{\abs{y - y_0} < \epsilon/2} then \f{\abs{(x + y) - (x_0 + y_0)} < \epsilon}.
* If \f{\abs{x - x_0} < \epsilon/2} and \f{\abs{y - y_0} < \epsilon/2} then \f{\abs{(x - y) - (x_0 - y_0)} < \epsilon}.
* If \f{\abs{x - x_0} < \min(1,\epsilon/ (2\abs{y_0} + 2))} and \f{\abs{y - y_0} < \epsilon / (2\abs{x_0} + 2)} then \f{\left|xy - x_0y_0\right| < \epsilon}.
* If \f{x_0 \neq 0} and \f{\abs{x - x_0} < \min(\abs{x_0}/2,\epsilon \abs{x_0}^2 / 2)} then \f{x \neq 0} and \f{\abs{1/x - 1/x_0} < \epsilon}.
* If \f{\abs{x - x_0} < \min(1,\epsilon / (\sum_{i=1}^{n-1}(\abs{x_0} + 1)^i \abs{x_0}^{n-i}))} then \f{\abs{x^n - x_0^n} < \epsilon}.
* If \f{x_0 > 0} and \f{\abs{x - x_0} < \min(1,\abs{x_0}/2,\epsilon (\sum_{i=1}^{n-1}(\abs{x_0} + 1)^{i/n}\abs{x_0}^{(n-1)/n}))} then \f{x > 0} and \f{\abs{\sqrt[n]{x} - \sqrt[n]{x_0}} < \epsilon}.

For part \f{1}, we have that \f{\abs{x - x_0} < \epsilon/2} which means \f{-\epsilon/2 < x - x_0 < \epsilon/2} and \f{\abs{y - y_0} < \epsilon/2} which means \f{-\epsilon/2 < y - y_0 < \epsilon/2}. Adding the two inequalities we obtain \f{-\epsilon < (x + y) - (x_0 + y_0) < \epsilon} or \f{\abs{(x + y) - (x_0 + y_0)} < \epsilon}, which is what we wanted.
For part \f{2}, the proof is similar to that for part \f{1}.
For part \f{3}, we have that, since \f{\abs{x - x_0} < 1} then \f{\abs{x} - \abs{x_0} \leq \abs{x - x_0} < 1} or \f{\abs{x} < \abs{x_0} + 1}. We can then rewrite \f{\abs{xy - x_0y_0} = \abs{x(y - y_0) + y_0(x - x_0)} \leq \abs{x}\abs{y - y_0} + \abs{y_0}\abs{x - x_0} < (\abs{x_0} + 1) \epsilon / (2 (\abs{x_0} + 1)) + \abs{y_0} \epsilon / (2(\abs{y_0} + 1)) = \epsilon / 2 + \epsilon / 2 (\abs{y_0} / (\abs{y_0} + 1)) < \epsilon / 2 + \epsilon / 2 = \epsilon}, which means \f{\abs{xy - x_0y_0} < \epsilon}, which is what we wanted.
For part \f{4}, we have that, since \f{\abs{x - x_0} < \abs{x_0}/2} then \f{\abs{x_0} - \abs{x} < x_0 / 2} and \f{\abs{x} >  \abs{x_0}/2} which means \f{x \neq 0} and \f{1/\abs{x} < 2/\abs{x_0}}. We can then rewrite \f{\abs{1/x - 1/x_0} = \abs{(x_0 - x) / x_0} = \abs{x - x_0} / \abs{x}\abs{x_0} < \epsilon\abs{x_0}^2 / 2\abs{x}\abs{x_0} < \epsilon\abs{x_0}^2 / \abs{x_0}\abs{x_0} < \epsilon}, which means \f{\abs{1/x - 1/x_0} < \epsilon}, which is what we wanted.
For part \f{5}, we have that, since \f{\abs{x - x_0} < 1} and \f{\abs{x} - \abs{x_0} \leq \abs{x - x_0}} we have that \f{\abs{x} - \abs{x_0} < 1} or \f{\abs{x} < \abs{x_0} + 1}. We can then rewrite \f{\abs{x^n - x_0^n} = \absX{(x - x_0)(x^{n-1} + x^{n-2}x_0 + \dots + xx_0^{n-2} + x_0^{n-1})} = \abs{x - x_0}\absX{\sum_{i=1}^{n-1}x^{i}x_0^{n-1}} < \abs{x - x_0}(\sum_{i=1}^{n-1}\abs{x}^{i}\abs{x_0}^{n-1}) < (x - x_0)(\sum_{i=1}^{n-1}(\abs{x_0} + 1)^i\abs{x_0}^{n-1}) < \epsilon/(\sum_{i=1}^{n-1}(\abs{x_0} + 1)^i\abs{x_0}^{n-1}) (\sum_{i=1}^{n-1}(\abs{x_0} + 1)^i\abs{x_0}^{n-1}) = \epsilon}, which means \f{\abs{x^n - x_0^n} < \epsilon}, which is what we wanted.
For part \f{6}, we have that, since \f{\abs{x - x_0} < 1} and \f{\abs{x} - \abs{x_0} \leq \abs{x - x_0}} we have that \f{\abs{x} - \abs{x_0} < 1} or \f{\abs{x} < \abs{x_0} + 1} or \f{\sqrt[n]{\abs{x}} < \sqrt[n]{\abs{x_0} + 1}}. We can then rewrite \f{\abs{\sqrt[n]{x} - \sqrt{n}{x_0}} = \abs{x - x_0}/(\sum_{i=1}^{n-1}\abs{x}^{i/n}\abs{x_0}^{(n-1)/n}) < \abs{x - x_0}/(\sum_{i=1}^{n-1}(\abs{x_0} + 1)^{i/n}\abs{x_0}^{(n-1)/n}) < \epsilon (\sum_{i=1}^{n-1}(\abs{x_0} + 1)^{i/n}\abs{x_0}^{(n-1)/n}) / (\sum_{i=1}^{n-1}(\abs{x_0} + 1)^{i/n}\abs{x_0}^{(n-1)/n}) = \epsilon}, which means \f{\abs{\sqrt[n]{x} - \sqrt[n]{x_0}} < \epsilon}, which is what we wanted. Also, since \f{\abs{x - x_0} < \abs{x_0}/2} we have, like in part \f{4}, that \f{\abs{x} > \abs{x_0}/2}.

Considering the basic operations on functions, these is how they interact with limits, assuming the limits exist and are finite:
* \f{\lim_{a^\star}f(x) = \lim_{(a - b)^\star}f(x + b)}.
* \f{\lim_{a^\star}(f + g)(x) = \lim_{a^\star}f(x) + \lim_{a^\star}g(x)}.
* \f{\lim_{a^\star}(f - g)(x) = \lim_{a^\star}f(x) - \lim_{a^\star}g(x)}.
* \f{\lim_{a^\star}(fg)(x) = \lim_{a^\star}f(x)\lim_{a^\star}g(x)}.
* If \f{\lim_{a^\star}g(x) \neq 0} then \f{\lim_{a^\star}(1/g)(x) = 1/\lim_{a^\star}g(x)}.
* If \f{\lim_{a^\star}g(x) \neq 0} then \f{\lim_{a^\star}(f/g)(x) = \lim_{a^\star}f(x)/\lim_{a^\star}g(x)}.
* If \f{f(x) > 0} for all \f{x} and \f{m/n \in \mathbb{Q}} then \f{\lim_{a^\star}f^{\frac{m}{n}}(x) = (\lim_{a^\star} f(x))^{\frac{m}{n}}}.
* \f{\lim_{a^\star}\abs{f}(x) = \abs{\lim_{a^\star}f(x)}}.
* \f{\lim_{a^\star}\max(f,g)(x) = \max(\lim_{a^\star}f(x),\lim_{a^\star}g(x))}.
* \f{\lim_{a^\star}\min(f,g)(x) = \min(\lim_{a^\star}f(x),\lim_{a^\star}g(x))}.
* If \f{\lim_{0}f(x)/x = l} and \f{b \neq 0} then \f{\lim_{0}f(bx)/x = bl}.
* If \f{\lim_af(x)} exists and \f{\lim_b g(x) = a} then \f{\lim_b f(g(x)) = \lim_a f(x)} - If \f{f} approaches a limit at \f{a^\star} then we can replace \f{x} by any quantity dependent on \f{x} which approaches \f{a} as \f{x} does, and \f{f} will approach the same limit.

For part \f{1}, let \f{\epsilon > 0}. We must find a \f{\delta > 0} such that for all \f{x \in D^\star(a-b,\delta)} we have \f{\abs{f(x+b) - \lim_{a^\star}f(x)} < \epsilon}. We choose \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \epsilon}. Let \f{x \in D^\star(a - b,\delta)}. We can write \f{x = \overline{x} - b} with \f{\overline{x} \in D^\star(a,\delta)} and we have \f{\abs{f(x) - \lim_{a^\star}f(x)} = \abs{f(\overline{x} - b + b) - \lim_{a^\star}f(x)} = \abs{f(\overline{x}) - \lim_{a^\star}f(x)}} for which \f{\abs{f(\overline{x} - \lim_{a^\star}f(x))} < \epsilon} holds. We have thus found a \f{\delta} for our \f{\epsilon}, which is what we wanted.
For part \f{2}, let \f{\epsilon > 0}. We must find a \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{(f(x) + g(x)) - (\lim_{a^\star}f(x) + \lim_{a^\star}g(x))} < \epsilon}. We choose \f{\delta_f > 0} such that for all \f{x \in D^\star(a,\delta_f)} we have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \epsilon/2} and \f{\delta_g > 0} such that for all \f{x \in D^\star(a,\delta_g)} we have \f{\abs{g(x) - \lim_{a^\star}g(x)} < \epsilon/2}. We choose \f{\delta = \min(\delta_f,\delta_g)} which has the same properties as \f{\delta_f} and \f{\delta_g}. Let \f{x \in D^\star(a,\delta)}. Then, by \thmref{SomeLimitLikeRelationsLemma}, we have that \f{\abs{f(x) + g(x) - (\lim_{a^\star}f(x) + \lim_{a^\star}g(x))} < \epsilon} or \f{\abs{(f + g)(x) - (\lim_{a^\star}f(x) + \lim_{a^\star}g(x))} < \epsilon} holds. We have thus found a \f{\delta} for our \f{\epsilon}, which is what we wanted.
For parts \f{3}, \f{4}, \f{5}, \f{6} and \f{7}, the proof is similar to that for part \f{2}.
For part \f{8}, let \f{\epsilon > 0}. We must find a \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{\abs{f(x)} - \abs{\lim_{a^\star}f(x)}} < \epsilon} or \f{\abs{\lim_{a^\star}f(x)} - \epsilon < f(x) < \abs{\lim_{a^\star}f(x)} + \epsilon}. First, assume \f{\lim_{a^\star}f(x) > 0}. We build \f{I^+ = (\lim_{a^\star}f(x) - \epsilon,\lim_{a^\star}f(x) + \epsilon) \cap [0,+\infty)}, which is bounded below, and, therefore, admits an infimum. Let \f{\lim_{a^\star}f(x) - \tau = \inf{I^+}} with \f{\tau \leq \epsilon}. We have \f{\lim_{a^\star}f(x) - \tau \geq 0} and \f{\lim_{a^\star}f(x) - \tau \geq \lim_{a^\star}f(x) - \epsilon}. We choose \f{\delta} such that for all \f{x \in D^\star(a,\delta)} we  have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \tau} or \f{\lim_{a^\star}f(x) - \tau < f(x) < \lim_{a^\star}f(x) + \tau}. We then have \f{\lim_{a^\star}f(x) - \epsilon \leq \lim_{a^\star}f(x) - \tau < f(x) < \lim_{a^\star} + \tau \leq \lim_{a^\star} + \epsilon}. Since \f{\lim_{a^\star}f(x) > 0} we have \f{\lim_{a^\star}f(x) = \abs{\lim_{a^\star}f(x)}} and since \f{0 \leq \lim_{a^\star}f(x) - \tau < f(x)} we have \f{f(x) = \abs{f(x)}}. Therefore, we have \f{\abs{\lim_{a^\star}f(x)} - \epsilon < \abs{f(x)} < \abs{\lim_{a^\star}f(x)} + \epsilon}, which is what we wanted. Second, assume \f{\lim_{a^\star}f(x) \leq 0}. We build \f{I^- = (\lim_{a^\star}f(x) - \epsilon,\lim_{a^\star}f(x) + \epsilon) \cap (-\infty,0]}, which is bounded above, and, therefore, admits a supremum. Let \f{\lim_{a^\star}f(x) + \tau = \sup{I^-}} with \f{\tau \leq \epsilon}. We have \f{\lim_{a^\star}f(x) + \tau \leq 0} and \f{\lim_{a^\star}f(x) + \tau \leq \lim_{a^\star}f(x) + \epsilon}. We choose \f{\delta} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \tau} or \f{\lim_{a^\star}f(x) - \tau < f(x) < \lim_{a^\star}f(x) + \tau}. We then have \f{\lim_{a^\star}f(x) - \epsilon \leq \lim_{a^\star}f(x) - \tau < f(x) < \lim_{a^\star} + \tau \leq \lim_{a^\star}f(x) + \epsilon}. Since \f{\lim_{a^\star}f(x) \leq 0} we have \f{-\lim_{a^\star}f(x) = \abs{\lim_{a^\star}f(x)}} and since \f{f(x) < \lim_{a^\star}f(x) - \tau \leq 0} we have \f{-f(x) = \abs{f(x)}}. Therefore, we have, \f{\lim_{a^\star}f(x) - \epsilon < f(x) < 0}, or \f{\abs{\lim_{a^\star}f(x) - \epsilon} > \abs{f(x)} > 0}. Since \f{\abs{\lim_{a^\star}f(x)} + \abs{\epsilon} \geq \abs{\lim_{a^\star}f(x) - \epsilon}} we have \f{\abs{\lim_{a^\star}f(x)} + \abs{\epsilon} > \abs{f(x)}} or, since \f{\epsilon > 0}, \f{\abs{\lim_{a^\star}f(x)} + \epsilon > \abs{f(x)}}. We also have \f{f(x) < \lim_{a^\star}f(x) + \epsilon} or \f{-f(x) > -\lim_{a^\star}f(x) - \epsilon} or \f{\abs{f(x)} > \abs{\lim_{a^\star}f(x)} - \epsilon}. Putting it all together, we have \f{\abs{\lim_{a^\star}f(x)} - \epsilon < \abs{f(x)} < \abs{\lim_{a^\star}f(x)} + \epsilon}, which is what we wanted. We have thus found a \f{\delta} for our \f{\epsilon}, in both cases.
For part \f{9}, we have that \f{\max(f,g) = (f + g + \abs{f - g}) / 2}, therefore, by a simple application of the rules from parts \f{2}, \f{3}, \f{4} and \f{8} we obtain what we want.
For part \f{10}. the proof is similar to that for part \f{9}.
For part \f{11}, let \f{\epsilon > 0}. We must find a \f{\delta > 0} such that for all \f{x} for which \f{0 < \abs{x} < \delta} we have \f{\abs{f(bx)/x - bl} < \epsilon}. Let \f{\overline{\delta} > 0} such that for all \f{x} for which \f{0 < \abs{x} < \overline{\delta}} we have \f{\abs{f(x)/x - l} < \epsilon/\abs{b}}. We choose \f{\delta = \overline{\delta}/\abs{b}}  which has the same properties as \f{\overline{\delta}}. Let \f{x} such that \f{0 < \abs{x} < \delta}. We can write \f{x = \overline{x}/b} and \f{0 < \abs{\overline{x}}/\abs{b} < \overline{\delta}/\abs{b}} or \f{0 < \abs{\overline{x}} < \overline{\delta}} and we have \f{\abs{f(bx)/x - bl} = \abs{f(b\overline{x}/b)/(\overline{x}/b) - bl} = \abs{bf(\overline{x})/\overline{x} - bl} = \abs{b}\abs{f(\overline{x})/\overline{x} - l} < \abs{b}\epsilon/\abs{b} = \epsilon}, which is what we wanted. We have thus found a \f{\delta} for our \f{\epsilon}.
For part \f{12}, let \f{\epsilon > 0} and \f{\lim_a f(x) = l}. We have to choose a \f{\delta > 0} such that for all \f{x} with \f{0 < \abs{x - b} < \delta} we have \f{\abs{f(g(x)) - l} < \delta}. Now, we know that \f{\lim_a f(x) = l}, therefore, for all \f{\epsilon > 0} we can choose a \f{\delta > 0} such that for all \f{x} with \f{0 < \abs{x - a} < \delta} we have \f{\abs{f(x) - l} < \delta}. Now, if \f{x} is in fact another function of \f{x} which is bounded in the same way, we get the same results. But, since \f{\lim_b g(x) = a} we can choose a \f{\delta > 0} such that for all \f{x} with \f{0 < \abs{x - b} < \delta} we have \f{\abs{g(x) - a} < \overline{\delta}}, where \f{\overline{delta} > 0} is such that for all \f{x} with \f{0 < \abs{x - a} < \overline{\delta}} we have \f{\abs{f(x) - l} < \epsilon}. Since \f{g(x)} satisfies for all \f{x} the requirements of a quantity such that \f{\abs{f(x) - l} < \epsilon} (\f{\abs{f(g(x)) - l} < \epsilon}) we have that this is the \f{\delta} we were looking for, and we have \f{\lim_b f(g(x)) = \lim_a f(x)}.

How some global properties of functions influence limits:
* If \f{\lim_{a^\star}f(x)} exists and for all \f{x} we have \f{f(x) \geq 0} then \f{\lim_{a^\star}f(x) \geq 0}.
* If \f{\lim_{a^\star}f(x)} exists, \f{\lim_{a^\star}g(x)} exists and for all \f{x} we have \f{f(x) \leq g(x)} then \f{\lim_{a^\star}f(x) \leq \lim_{a^\star}g(x)}.
* If \f{\lim_{a^\star}f(x)} exists, \f{\lim_{a^\star}h(x)} exists, for all \f{x} we have \f{f(x) \leq g(x) \leq h(x)} and \f{\lim_{a^\star}f(x) = \lim_{a^\star}h(x)} then \f{\lim_{a^\star}g(x) = \lim_{a^\star}f(x) = \lim_{a^\star}h(x)}.
* If \f{\lim_{0^\star} f(x) = 0} and \f{\abs{g}} bounded then \f{\lim_{0^\star}f(x)g(x) = 0}.
* If \f{\lim_{a^\star}\abs{g(x)} = 0} and for all \f{x} we have \f{f(x) > \epsilon > 0} then \f{\lim_{a^\star}f(x) / \abs{g(x)} = +\infty}.
* If \f{\lim_{a^\star}\abs{g(x)} = 0} and for all \f{x} we have \f{f(x) < \epsilon < 0} then \f{\lim_{a^\star}f(x) / \abs{g(x)} = -\infty}.

For part \f{1}, suppose, by contradiction, that \f{\lim_{a^\star}f(x) < 0}. We choose \f{\epsilon = \abs{\lim_{a^\star}f(x)} = -\lim_{a^\star}f(x) > 0} and \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \epsilon} or \f{\lim_{a^\star}f(X) - \epsilon < f(x) < \lim_{a^\star}f(x) + \epsilon} or \f{f(x) < 0} given how \f{\epsilon} was constructed, which is a contradiction, since \f{f(x) \geq 0} for all \f{x}, and which means \f{\lim_{a^\star}f(x) \geq 0}.
For part \f{2}, consider \f{g(x) - f(x)} and, by a simple application of the rule from part \f{1}, we obtain what we want.
For part \f{3}, we have that for all \f{x} we have \f{f(x) \leq g(x)}, therefore, by part \f{2} we have \f{\lim_{a^\star}f(x) \leq \lim_{a^\star}g(x)} and for all \f{x} we have \f{g(x) \leq h(x)}, therefore, by part \f{2} we have \f{\lim_{a^\star}g(x) \leq \lim_{a^\star}h(x)}. This means that \f{\lim_{a^\star}f(x) \leq \lim_{a^\star}g(x)} and \f{\lim_{a^\star}g(x) \leq \lim_{a^\star}f(x)} or \f{\lim_{a^\star}g(x) = \lim_{a^\star}f(x) = \lim_{a^\star}h(x)}, which is what we wanted.
For part \f{4}, since \f{\abs{g}} is bounded we can find \f{M > 0} such that for all \f{x} we have \f{\abs{f(x)} \leq M} or \f{-M \leq \abs{f(x)} \leq M}. Let \f{\epsilon > 0}. We choose \f{\delta > 0} such that for all \f{x \in D^\star(0,\delta)} we have \f{\abs{f(x)} < \epsilon/M}. Let \f{x \in D^\star(0,\delta)}. Then, \f{\abs{f(x)g(x)} = \abs{f(x)}\abs{g(x)} < \epsilon/M \abs{g(x)} < \epsilon/M M = \epsilon}, therefore \f{\abs{f(x)g(x)} < \epsilon}, which is what we wanted. We have thus found an \f{\delta > 0} for our \f{\epsilon}, and we have \f{\lim_{0^\star}f(x)g(x) = 0}.
For part \f{5}, let \f{M > 0}. We choose \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{g(x)} < \epsilon / M} or \f{1/\abs{g(x)} > M / \epsilon}. Let \f{x \in D^\star(a,\delta)}. We have \f{f(x) / \abs{g(x)} > \epsilon / abs{g(x)} > \epsilon / (M / \epsilon) = M} or \f{f(x) > M}, which is what we wanted. We have found \f{\delta > 0} for our \f{M} and we have that \f{\lim_{a^\star}f(x) / \abs{g(x)} = +\infty}.
For part \f{6}, the proof is similar to part \f{5}.

The local behaviour of limits, again assuming all limits are finite:
* If for all \f{x \in D^\star(a,\delta)} we have \f{f(x) = g(x)} then \f{\lim_{a^\star}f(x) = \lim_{a^\star}g(x)}.
* If \f{\lim_{a^\star}f(x) < \lim_{a^\star}g(x)} then there exists \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{f(x) < g(x)}.
* There exists \f{\delta > 0}, \f{m} and \f{M} such that for all \f{x \in D^\star(a,\delta)} we have \f{m < f(x) < M}.
* There exists \f{\delta > 0} and \f{M > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{f(x)} < M}.

For part \f{1}, suppose, by contradiction, that \f{\lim_{a^\star}f(x) \neq \lim_{a^\star}g(x)}. We choose \f{\epsilon = \abs{\lim_{a^\star}f(x) - \lim_{a^\star}g(x)} / 2 > 0} and find \f{\delta_f} such that for all \f{x \in D^\star(a,\delta_f)} we have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \epsilon} and \f{\delta_g} such that for all \f{x \in D^\star(a,\delta_g)} we have \f{\abs{g(x) - \lim_{a^\star}g(x)} < \epsilon}. We choose \f{\delta' = \min(\delta,\delta_f,\delta_g)} which has the same properties as \f{\delta} and \f{\delta_f} and \f{\delta_g}. This means that for \f{x \in D^\star(a,\delta')} we have \f{f(x) = g(x)}, but also, if we assume, without loss of generality, that \f{\lim_{a^\star}f(x) > \lim_{a^\star}g(x)}, \f{\lim_{a^\star}g(x) - \epsilon < g(x) < \lim_{a^\star}g(x) + \epsilon < \lim_{a^\star}f(x) - \epsilon < f(x) < \lim_{a^\star}f(x) + \epsilon}, due to the way we chose \f{\epsilon}, and, therefore, we have \f{f(x) < g(x)}, which is a contradiction, and which means \f{\lim_{a^\star}f(x) = \lim_{a^\star}g(x)}.
For part \f{2}, we must find a \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{f(x) < g(x)}. We choose \f{\epsilon = \abs{\lim_{a^\star}f(x) - \lim_{a^\star}g(x)} / 2 > 0} such that \f{\lim_{a^\star}f(x) +\epsilon < \lim_{a^\star}g(x) - \epsilon}. We choose \f{\delta_f} such that for all \f{x \in D^\star(a,\delta_f)} we have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \epsilon} and \f{\delta_g} such that for all \f{x \in D^\star(a,\delta_g)} we have \f{\abs{g(x) - \lim_{a^\star}g(x)} < \epsilon}. We choose \f{\delta = \min(\delta_f,\delta_g)} which has the same properties as \f{\delta_f} and \f{\delta_g}. Let \f{x \in D^\star(a,\delta)}. We have \f{f(x) < \lim_{a^\star}f(x) + \epsilon < \lim_{a^\star}g(x) - \epsilon < g(x)} or \f{f(x) < g(x)}, which is what we wanted. We have thus found a \f{\delta} which meets our needs.
For part \f{3}, let any \f{\epsilon > 0}. We choose \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{\abs{f(x) - \lim_{a^\star}f(x)} < \epsilon} or \f{\lim_{a^\star}f(X) - \epsilon < f(x) < \lim_{a^\star}f(x) + \epsilon}. We choose \f{m = \inf\{f(x) \given x \in D^\star(a,\delta)\}} and \f{M = \sup\{f(x) \given x \in D^\star(a,\delta)}. We have thus found our \f{\delta > 0}, \f{m} and \f{M}.
For part \f{4}, we have \f{\lim_{a^\star}\abs{f(x)} = \abs{\lim_{a^\star}f(X)}}, therefore, by a simple application of the rules from part \f{3} we obtain what we want.

Again, assuming all limits exist, we have:
* If \f{\lim_{a^+}f(x) = \lim_{a^-}f(x)} then \f{\lim_{a}f(x)} exists and \f{\lim_{a}f(x) = \lim_{a^+}f(x) = \lim_{a^-}f(x)}.
* \f{\lim_{0^+}f(x) = \lim_{0^-}f(-x)}.
* \f{\lim_{0^+}f(x) = \lim_{0}f(\abs{x})}.
* \f{\lim_{0^+}f(x) = \lim_{0}f(x^2)}.
* \f{\lim_{0^+}f(x) = \lim_{+\infty}f(1/x)}.
* \f{\lim_{0^-}f(x) = \lim_{-\infty}f(1/x)}.
* \f{\lim_{+\infty}f(x) = \lim_{-\infty}f(-x)}.

For part \f{1}, assume \f{\lim_{a^+}f(x)} exists and \f{\lim_{a^-}} exists and \f{\lim_{a^+}f(x) = \lim_{a^-}f(x) = l} and let \f{\epsilon > 0}. We must find an \f{\delta > 0} such that for all \f{x \in (a-\delta,a+\delta)} we have \f{f(x) \in C(l,\epsilon)}. We choose \f{\delta^-} such that for all \f{x \in (a-\delta^-,a)} we have \f{f(x) \in C(l,\epsilon)} and \f{\delta^+} such that for all \f{x \in (a,a+\delta^+)} we have \f{f(x) \in C(l,\epsilon)}. We choose \f{\delta = \min(\delta^-,\delta^+)} which has the same properties as \f{\delta^-} and \f{\delta^+}. Therefore, we have that for all \f{x \in (a-\delta,a)\cup(a,a+\delta)} or \f{x \in (a-\delta,a+\delta)\setminus\{a\}} we have \f{f(x) \in C(l,\epsilon)}, therefore \f{\lim_{a}f(x) = l}, which is what we wanted.
For part \f{2}, first, assume \f{\lim_{0^+}f(x)} exists and let \f{\epsilon > 0}. We must find \f{\delta > 0} such that for all \f{x \in (0,\delta)} we have \f{f(-x) \in C(\lim_{0^+}f(x),\epsilon)}. We choose \f{\delta} such that for all \f{x \in (-\delta,0)} we have \f{f(x) \in C(\lim_{0^+}f(x),\epsilon)}. Let \f{x \in (0,\delta)}. We can write \f{x = -\overline{x}} where \f{\overline{x} \in (-\delta,0)} and we have \f{f(-x) = f(-(-\overline{x})) = f(\overline{x}) \in C(\lim_{0^+}f(x),\epsilon)}, therefore \f{f(-x) \in C(\lim_{0^+}f(x),\epsilon)} holds, which means that we have found a \f{\delta} for our \f{\epsilon} and the limit of \f{f(-x)} from above at \f{0} is \f{\lim_{0^+}f(x)}, which is what we wanted. Second, assume \f{\lim_{0^-}f(-x)} exists. The proof is the same as for the first situation.
For part \f{3}, the proof is similar to that for part \f{2}.
For part \f{4}, the proof is similar to that for part \f{2}.
For part \f{5}, first, assume \f{\lim_{0^+}f(x)} exists and let \f{\epsilon > 0}. We must find \f{\delta > 0} such that for all \f{x > \delta} we have \f{f(1/x) \in C(\lim_{0^+}f(x),\epsilon)}. We choose \f{\overline{\delta}} such that for all \f{x \in (0,\overline{\delta})} we have \f{f(x) \in C(\lim_{0^+}f(x),\epsilon)}. We choose \f{\delta = 1/\overline{\delta}}. Let \f{x > \delta}. We can write \f{x = 1/\overline{x}} where \f{1/\overline{x} > 1/\overline{\delta}} or \f{0 < \overline{x} < \overline{\delta}}. We then have \f{f(1/x) = f(1/(1/\overline{x})) = f(\overline{x}) \in C(\lim_{0^+}f(x),\epsilon)}, therefore \f{f(1/x) \in C(\lim_{0^+}f(x),\epsilon)} holds, which means that we have found a \f{\delta} for our \f{\epsilon} and the limit of \f{f(1/x)} at positive infinity is \f{\lim_{0^+}f(x)}, which is what we wanted. Second, assume \f{\lim_{+\infty}f(1/x)} exists. The proof is the same as for the first situation.
For part \f{6}, the proof is similar to that for part \f{5}.
For part \f{7}, first, assume \f{\lim_{+\infty}f(x)} exists and let \f{\epsilon > 0}. We must find \f{N} such that for all \f{x < N} we have \f{f(-x) \in C(\lim_{+\infty}f(x),\epsilon)}. We choose \f{\overline{N} > 0} such that for all \f{x > \overline{N}} we have \f{f(x) \in C(\lim_{+\infty}f(x),\epsilon)}. We choose \f{N = -\overline{N}}. Let \f{x < N}. We can write \f{x = -\overline{x}} where \f{-\overline{x} < N} or \f{-\overline{x} < -\overline{N}} or \f{\overline{x} > \overline{N}}. We then have \f{f(-x) = f(-(-\overline{x})) = f(\overline{x}) \in C(\lim_{+\infty}f(x),\epsilon)}, therefore \f{f(-x) \in C(\lim_{+\infty}f(x),\epsilon)} holds, which means that we have found a \f{N} for our \f{\epsilon} and the limit of \f{f(-x)} at negative infinity is \f{\lim_{+\infty}f(x)}, which is what we wanted. Second, assume \f{\lim_{-\infty}f(-x)} exists. The proof is the same as for the first situation.

= Continuity =

Let \f{f} be a \ref{numeric function}.

\f{f} is \def{continuos} at \f{a} if \f{\lim_{a}f(x) = f(a)}. \f{f} is \def{continuos} on \f{(a,b)} if for all \f{x \in (a,b)} we have that \f{f} is continuos at \f{x}. \f{f} is \def{continuos} on \f{[a,b]} if \f{f} is continuos on \f{(a,b)}, \f{\lim_{a^+}f(x) = f(a)} and \f{\lim_{b^-}f(x) = f(b)}. \f{f} is \def{continuos everywhere} if for all \f{x \in \mathbb{R}} we have that \f{f} is continuos at \f{x}. Intuitively, continuos functions do not contain "jumps", "breaks" or "wild oscilations" and one can draw the graph of a continuos function in one stroke, without lifting the pen off the paper. \f{f} is \def{left-continuos} at \f{a}, or \f{\lim_{a^-} f(x) = f(a)}. \f{f} is \def{left-continuos} on \f{(a,b)} if for all \f{x \in (a,b)} we have that \f{f} is left-continuos at \f{x}. \f{f} is \def{left-continuos} on \f{[a,b]} if \f{f} is left-continuos on \f{(a,b)} and \f{\lim_{b^-}f(x) = f(b)}. \f{f} is \def{left-continuos everywhere} if for all \f{x \in \mathbb{R}} we have that \f{f} is left-continuos at \f{x}. \f{f} is \def{right-continuos} at \f{a} if \f{\lim_{a^-} f(x) = f(a)}. \f{f} is right-continuos on \f{(a,b)} if for all \f{x \in (a,b)} we have that \f{f} is right-continuos at \f{x}. \f{f} is \def{right-continuos} on \f{[a,b]} if \f{f} is right-continuos on \f{(a,b)} and \f{\lim_{a^+}f(x) = f(a)}. \f{f} is \def{right-continuos everywhere} if for all \f{x \in \mathbb{R}} we have that \f{f} is right-continuos at \f{x}. Intuitively, partial continous functions do not contain "breaks" or "wild oscilations", but do contain "jumps" and one can draw the graph of a continuos function in several strokes, without lifting the pen off the paper.

We will use \f{\star}-continuos to refer to any type of continuity for \f{f}.

Let \f{f} and \f{g} be \f{star}-continuous functions, in the manner needed by whatever theorem we have.

Similar to the \ref{limits function theorem}, function operations interact with continuity.
* \f{f + g} is \f{\star}-continuos at \f{a}.
* \f{f - g} is \f{\star}-continuos at \f{a}.
* \f{fg} is \f{\star}-continuos at \f{a}.
* If \f{g(a) \neq 0} then \f{1/g} is \f{\star}-continuos at \f{a}.
* If \f{g(a) \neq 0} then \f{f/g} is \f{\star}-continuos at \f{a}.
* If \f{f(a) > 0} and \f{m/n \in \mathbb{Q}} then \f{f^{\frac{m}{n}}} is \f{\star}-continuos at \f{a}.
* \f{\abs{f}} is \f{\star}-continuos at \f{a}.
* \f{\max(f,g)} is \f{\star}-continuos at \f{a}.
* \f{\min(f,g)} is \f{\star}-continuos at \f{a}.
* If \f{f} is \f{\star}-continuos at \f{g(a)} then \f{f \circ g} is \f{\star}-continuos at \f{a}.

For part \f{1}, we have \f{\lim_{a^\star}f(x) = f(a)} and \f{\lim_{a^\star}g(x) = g(a)} and \f{\lim_{a^\star}(f + g)(x) = f(a) + g(a)}, by \thmref{NumericFunctionTransformationsLimits}, therefore \f{f + g} is \f{\star}-continuous at \f{a}.
For other parts, the proof is similar to that for part \f{1}.
For part \f{10}, proof \f{2}: we have that \f{\lim_{g(a)^\star} f(x) = f(g(a))} and \f{\lim_{a^\star} g(x) = g(a)}, because \f{f} and \f{g} are \f{\star}-continuous. By \thmref{NumericFunctionTransformationsLimits}, we then have that \f{\lim_{a^\star} f(g(x)) = f(g(a))}, which is what we wanted.

If \f{f + g} is continuous at \f{a}, this does not necessarily mean that \f{f} and \f{g} are continous at \f{a} (think of step functions).

Local behaviour of continuous functions:
* If \f{f(a) > 0} then there exists \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{f(x) > 0}.
* If \f{f(a) < 0} then there exists \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{f(x) < 0}.
* There exists \f{\delta > 0} such that \f{f} is bounded above on \f{D^\star(a,\delta)}.
* There exists \f{\delta > 0} such that \f{f} is bounded below on \f{D^\star(a,\delta)}.

For part \f{1}, if \f{f} is \f{\star}-continuos at \f{a}, then \f{\lim_{a^\star}f(x) = f(a)} and \f{\lim_{a^\star}f(x) > 0}, therefore, by \thmref{LocalBehaviourLimits}, we have a \f{\delta > 0} such that for all \f{x \in D^\star(a,\delta)} we have \f{f(x) > 0}, which is what we wanted.
For parts \f{2}, \f{3} and \f{4}, the proof is similar to that for part \f{1}.

A very important theorem, called the \def{intermediate value theorem}. Let \f{f} be a function, \f{a} and \f{b} numbers with \f{a < b} and \f{f} continuos on \f{[a,b]}.
* If \f{f(a) < 0} and \f{f(b) > 0} then there exists \f{x \in [a,b]} such that \f{f(x) = 0}.
* For each \f{y \in f([a,b])} there exists \f{x \in [a,b]} such that \f{f(x) = y}.

For part \f{1}, we have \f{3} proofs. Proof \f{1}: consider the set \f{I = \{x \given f \text{ is negative on }  [a,x]\}}. \f{I} is bounded above (by \f{b}), therefore \f{I} has a supremum. Let \f{\overline{x} = \sup{I}}. We will prove that \f{f(\overline{x}) = 0}. First, assume \f{f(\overline{x}) < 0}. By \thmref{LocalBehaviourContinuosFunctions} there exists \f{\delta_-} such that for all \f{x} such that \f{\overline{x} - \delta_- < x < \overline{x} + \delta_-} we have \f{f(x) < 0}. But, a point \f{x} such that \f{\overline{x} < x < \overline{x} + \delta} has \f{f} negative on \f{[a,x]} because we can find an \f{\overline{\overline{x}}} in \f{(\overline{x} - \delta_-,\overline{x})} with \f{\overline{\overline{x}} \in I} and \f{f} is negative on \f{[\overline{\overline{x}},x]} as well. But, this is a contradiction as \f{x > \overline{x}} and \f{x \in I}, but \f{\overline{x}} is the supremum of \f{I}. Second, assume \f{f(\overline{x}) > 0}. The proof is the same as for the first situation. Therefore, we must have that \f{f(\overline{x}) = 0}. Furthermore, if there are more points \f{x} where \f{f(x) = 0} in \f{[a,b]} then \f{\overline{x}} is the first one of these. Proof \f{2}: consider the set \f{I = \{x \given f(x) < 0\}}. \f{I} is bounded above (by \f{b}), therefore \f{I} has a supremum. Let \f{\overline{x} = \sup{I}}. We will prove that \f{f(\overline{x}) = 0}. First, assume \f{f(\overline{x}) < 0}. Therefore \f{\overline{x} \in I}. But, by \thmref{LocalBehaviourContinuosFunctions}, we have that there exists \f{\delta_- > 0} such that for all \f{x} for which \f{\overline{x} - \delta_- < x < \overline{x} + \delta_-} we have \f{f(x) < 0}. But, this means that for \f{x} such that \f{\overline{x} < x < \overline{x} + \delta_-} we have that \f{f(x) < 0} and \f{f(x) \in I}, which is a contradiction, as \f{x > \overline{x}} but \f{\overline{x}} is the supremum of \f{I}. Second, assume \f{f(\overline{x}) > 0}. The proof is the same as for the first situation. Therefore, we must have that \f{f(\overline{x}) = 0}. Furthermore, if there are more points \f{x} where \f{f(x) = 0} in \f{[a,b]} then \f{\overline{x}} is the last one of these. Proof \f{3}: consider the following sequence of intervals \f{a_0 = a, b_0 = b} and \f{I_n = [a_n = a_{n-1},b_n = (a_{n-1}+b_{n-1})/2]} if \f{f((a_{n-1} + b_{n-1}) / 2) > 0}, \f{I_n = [a_n = (a_{n-1}+b_{n-1})/2,b_n = b_{n-1}]} if \f{f((a_{n-1}+b_{n-1})/2) < 0} and \f{I_n = \{a_n = b_n = (a_{n-1} + b_{n-1})/2\}} if \f{f((a_{n-1} + b_{n-1})/2) = 0}. By the Nested Interval Theorem have that there exists an \f{\overline{x}} which is a member of all \f{I_n}. We will prove that \f{f(\overline{x}) = 0}. First, assume \f{f(\overline{x}) < 0}. Then, by \thmref{LocalBehaviourContinuosFunctions} we can find a \f{\delta_-} such that for all \f{x} for which \f{\overline{x} - \delta_- < x < \overline{x} + \delta_-} we have \f{f(x) < 0}. But, we can find an \f{\overline{n}} such that \f{I_{\overline{n}}} is completely in \f{(\overline{x} - \delta_-,\overline{x} + \delta_-)}, but \f{b_{\overline{n}}} is positive, which is a contradiction, as \f{f(x) < 0} in this interval. Second, assume \f{f(\overline{x}) > 0}. The proof is the same as for the first situation. Therefore, we must have that \f{f(\overline{x}) = 0} and our nested intervals finish with a one-point interval.
For part \f{2}, consider \f{g(x) = f(x) - y} and the cases \f{f(a) \leq y \leq f(b)} and \f{f(b) \leq y \leq f(a)}, and, by a simple application of the rule from part \f{1}, we obtain what we want.

Another important theorem, which establishes that a continuous function on a closed interval is bounded, the \def{bounded theorem}. Let \f{f} be a function, \f{a} and \f{b} numbers with \f{a < b} and \f{f} continuos on \f{[a,b]}.
* \f{f([a,b])} is upper bounded.
* \f{f([a,b])} is lower bounded.

For part \f{1}, consider the set \f{I = \{x \given f \text{ is bounded above on } [a,x]\}}. The proof is then similar to that for the \thmref{IntermediateValueTheorem}, but needs to take into consideration the cases where \f{\overline{x} = a} or \f{\overline{x} = b}.
For part \f{2}, the proof is similar to that for part \f{1}.

A third important theorem, again, on bounded intervals, but regardin \def{maxima and minima}. Let \f{f} be a function, \f{a} and \f{b} numbers with \f{a < b} and \f{f} continuos on \f{[a,b]}. Then:
* \f{\max{f([a,b])}} exists.
* \f{\min{f([a,b])}} exists.

For part \f{1}, we know that \f{f([a,b])} is bounded, by \thmref{BoundedTheorem}, therefore there is \f{\alpha \leq f(x)} for all \f{x \in [a,b]} which is the supremum for \f{f([a,b])}. We need to prove that there is an \f{\overline{x} \in [a,b]} such that \f{f(\overline{x}) = \alpha}. Suppose, by contradiction there is no such \f{\overline{x}}. We can then construct the function \f{g(x) = 1 / (\alpha - f(x))} which is defined and continuous on \f{[a,b]}, and, therefore, \f{g([a,b])} is bounded. However, since \f{\alpha = \sup{f([a,b])}} for all \f{\epsilon > 0} we have that there exists \f{x \in [a,b]} such that \f{f(x) + \epsilon > \alpha} or \f{\alpha - f(x) < \epsilon} or \f{1 / (\alpha - f(x)) > \epsilon}. But this means for every \f{\epsilon > 0} there is an \f{x \in [a,b]} such that \f{g(x) > \epsilon}, which means that \f{g([a,b])} is unbounded, which is a contradiction. We therefore have that there must be an \f{\overline{x} \in [a,b]} such that \f{f(\overline{x}) = \sup{f([a,b])} = \max{f([a,b])}}.
For part \f{2}, the proof is similar to that for part \f{1}.

Interactions between continuity and bijectivity. Let \f{f} be a function, \f{A \subseteq \dom{f}} an interval and \f{f} continous and bijective on \f{A}.
* \f{f} is either increasing or decreasing on \f{A}.
* \f{f^{-1}} is continuous on \f{f(A)}.

For part \f{1}, proof \f{1}: one can select three points \f{x_1, x_2, x_3 \in A} with \f{x_1 < x_2 < x_3}. A case analysis of the posititioning of \f{f(x_1)}, \f{f(x_2)} and \f{f(x_3)} will lead to contradictions unless \f{f(x_1) < f(x_2) < f(x_3)}, because \f{f} is continuous and bijective on \f{A}. Proof \f{2}: let \f{x_1, x_2 \in A} with \f{x_1 < x_2}. We can either have \f{f(x_2) - f(x_1) > 0} or \f{f(x_2) - f(x_1) < 0}. Suppose we're in the first case. Let \f{x_1'} and \f{x_2'} such that \f{x_1 < x_1' < x_2' < x_2}. Let \f{x_1^t = (1 - t)x_1 + tx_1'} and \f{x_2^t = (1 - t)x_2 + tx_2'} with \f{t \in [0,1]} and \f{x_1^0 = x_1}, \f{x_1^1 = x_1'}, \f{x_2^0 = x_2} and \f{x_2^1 = x_2'}. Now, let \f{g(t) = f(x_2^t) - f(x_1^t)}. \f{g} is continuous on \f{[0,1]} (being the difference of composition of continuous functions) and \f{g(t) \neq 0} because \f{x_2^t > x_1^t} and \f{f} is bijective. This means, by \thmref{MeanValueTheorems} that \f{g} is either positive or negative. But, \f{g(0) > 0} therefore \f{g} is positive. This means \f{g(1) > 0} or \f{f(x_2') - f(x_1') > 0} or \f{f(x_2') > f(x_1')}, which is what we wanted.
For part \f{2}, proof \f{1}: we need to prove that \f{\lim_a f^{-1}(x) = f^{-1}(a)}. Assume that \f{f} is increasing and let \f{\epsilon > 0}. We must find a \f{\delta > 0} such that for all \f{x} with \f{0 < \abs{x - a} < \delta} we have \f{\abs{f^{-1}(x) - f^{-1}(a)} < \epsilon}. Consider \f{a \rightarrow f(a)} by \f{f} and \f{f(a) \rightarrow a} by \f{f^{-1}}. We also have that \f{a - \epsilon \rightarrow f(a - \epsilon)} and \f{a + \epsilon \rightarrow f(a + \epsilon)}. Since \f{f} is increasing, we have \f{f(a - \epsilon) < f(a) < f(a + \epsilon)}. We can find \f{\delta_{-}} and \f{\delta_{+}} such that \f{f(a - \epsilon) = f(a) - \delta_{-}} and \f{f(a + \epsilon) = f(a) + \delta_{+}}. Now, if we take \f{\delta < \min(\delta_{-},\delta_{+})} we have that \f{(f(a) - \delta,f(a) + \delta) \subset (f(a - \epsilon),f(a + \epsilon)) \rightarrow Q \subset (a - \epsilon,a + \epsilon)}, that is, because \f{f} is increasing, any point in an interval smaller than the image of \f{(a - \epsilon,a + \epsilon)} will be mapped back to a subset of of \f{(a - \epsilon,a + \epsilon)}. But, since the range of \f{f} is the domain of \f{f^{-1}} we can translate this to finding the \f{\delta} for our \f{\epsilon}, such that if \f{y = f(x)} for \f{x \in A} with \f{0 < \abs{x - f(a)} < \delta} we have that \f{\abs{f^{-1}(x) - a} < \epsilon}. We can write \f{f(a) = \overline{a}} and \f{a = f^{-1}(\overline{a})} and we get a form similar to what we wanted in the first place. Therefore, \f{f^{-1}} is continuous as well. Proof \f{2}, \f{f^{-1}} is continuous and increasing and respects all the conditions of \thmref{IntermediateValueTheorem}, therefore by \thmref{OtherConditionsContinuity} we have that \f{f^{-1}} is continuous as well.

Continuous and bijective functions on an interval \f{A} have a well defined image. If \f{A = [a,b]} for some \f{a < b \in \mathbb{R}} we have that \f{\ran{f} = [f(a),f(b)]} if \f{f} is increasing and \f{\ran{f} = [f(b),f(a)]} if \f{f} is decreasing. If \f{A = (a,b)}, \f{A = (-\infty,b)}, \f{A = (b,+\infty)} or \f{A = \mathbb{R}} then \f{\ran{f} = (\alpha,\beta)}, \f{\ran{f} = (-\infty,\beta)}, \f{\ran{f} = (\alpha,+\infty)} or \f{\ran{f} = \mathbb{R}}, where \f{\alpha} and \f{\beta} are the infimum and supremum of \f{\ran{f}}, respectively.

Let \f{a, b \text{ and } c} be numbers, \f{f} and \f{g} continuos functions on \f{[a,b]} with \f{f(b) = g(b)} and \f{h(x) \colon [a,c] \rightarrow \mathbb{R}} with \f{h(x) = f(x)} for \f{x \in [a,b]} and \f{h(x) = g(x)} for \f{x \in (b,c]}. Then \f{h} is continuos on \f{[a,c]}. Continuous functions on adjacent intervals can be pasted together.

Interactions with the \ref{even-odd} and \ref{positive-negative} decompositions.
* \f{f_E} and \f{f_O} are continuos.
* \f{f^+} and \f{f^-} are continous.

A \def{removable discontinuity} for \f{f} is a point \f{a} for which \f{\lim_{a} f(x)} exists but \f{\lim_{a} f(x) \neq f(a)}.

One can remove such discontinuities and form a continuous everywhere function. Let \f{h(x) = f(x)} if \f{f} is continuos at \f{x} and \f{h(x) = \lim_{y \to x}f(y)} if \f{f} is not continous at \f{x}. Then \f{h} is \ref{everywhere continous}.

== Some Statements About Continuous Functions ==

Let \f{f} be a continuos function with \f{f(x) > 0} for all \f{x} and \f{\lim_{+\infty} f(x) = \lim_{-\infty} f(x) = 0}. Then there exists \f{y} such that \f{f(y) \geq f(x)} for all \f{x}.

Let \f{f} be a continuos function on \f{[a,b]} and \f{f(a) = f(b) = 0} and \f{x_0 \in [a,b]} with \f{f(x_0) > 0}. Then there exists \f{c,d \in [a,b]} with \f{a \leq c < x_0 < d \leq b} such that for all \f{x \in (c,d)} we have \f{f(x) > 0}.

Let \f{f} be a continuos function on \f{[a,b]}. Then:
* If \f{f(a) < f(b)} there exist \f{c,d \in [a,b]} with \f{a \leq c < d \leq b} such that for all \f{x \in (c,d)} we have \f{f(a) < f(x) < f(b)}.
* If \f{f(a) > f(b)} there exist \f{c,d \in [a,b]} with \f{a \leq c < d \leq b} such that for all \f{x \in (c,d)} we have \f{f(a) > f(x) > f(b)}.

Let \f{A} be a dense set and \f{f} and \f{g} be continuos functions.
* If \f{f(x) = 0} for all \f{x \in A} then \f{f(x) = 0} for all \f{x}.
* If \f{f(x) = g(x)} for all \f{x \in A} then \f{f(x) = g(x)} for all \f{x}.
* If \f{f(x) \leq g(x)} for all \f{x \in A} then \f{f(x) \leq g(x)} for all \f{x}.
* If \f{f(x) < g(x)} for all \f{x \in A} then \f{f(x) \leq g(x)} for all \f{x}.

== Uniform Continuity ==

\f{f} is \def{uniformly continuos} on an interval \f{A} if for every \f{\epsilon > 0} there is some \f{\delta > 0} such that for all \f{x,y \in A} with \f{\abs{x - y} < \delta} we have \f{\abs{f(x) - f(y)} < \epsilon}.

There is again interaction between numeric function operations and uniform continuity.\begin{theorem}[NumericFunctionTransformationsUniformContinuity]
* \f{f + g} is uniformly continuos on \f{A}.
* \f{f - g} is uniformly continuos on \f{A}.
* If \f{f} and \f{g} are bounded on \f{A} then \f{fg} is uniformly continuos on \f{A}.
* If \f{f(x) \in B} for all \f{x \in A} then \f{g \circ f} is uniformly continuos on \f{A}.

If \f{f} be a continuos function on \f{[a,b]} then \f{f} is uniformly continous on \f{[a,b]}. This means that continuity implies uniform continuity. Uniform continuity is a weaker form.

A theorem similar to the \ref{pasting theorem} of continuous functions. let \f{a, b, \text{ and } c} be numbers, \f{f} and \f{g} functions with \f{f} uniformly continuos on \f{[a,b]} and \f{g} uniformly continuos on \f{[b,c]} and \f{f(b) = g(b)}. Then \f{h(x) \colon [a,c] \rightarrow \mathbb{R}} with \f{h(x) = f(x)} for \f{x \in [a,b]} and \f{h(x) = g(x)} for \f{x \in (b,c]} is uniformly continuos on \f{[a,c]}.

= Derivatives =

Let \f{f} be a numeric function and \f{a \text{ and } b } numbers.

The \def{derivative} of \f{f} at \f{a}, or \f{f'(a)} is the limit \f{\lim^h_0 (f(a + h) - f(a)) / h}, or the limit \f{\lim^h_a (f(h) - f(a)) / (h - a)}, or the limit \f{\lim^h_0 (f(x + h) - f(x - h)) / 2h}, or the limit \f{\lim^{h,k}_{0^+} (f(x + h) - f(x - k)) / (h + k)}. \f{f} \def{differentiable} at \f{a} if the derivative of \f{f} at \f{a} exists. \f{f} \def{differentiable} on \f{(a,b)} if for all \f{x \in (a,b)} we have that \f{f} is differentiable at \f{x}. \f{f} \def{differentiable} on \f{[a,b]} if for all \f{x \in [a,b]} we have that \f{f} is differentiable at \f{x}. \f{f} is \def{differentiable everywhere} if for all \f{x \in \mathbb{R}} we have that \f{f} is differentiable at \f{x}. The tangent line for \f{f} at \f{a} is defined only when \f{f} is differentiable at \f{a} and is given by \f{y(x) = f'(x)(x - a) + f(a)}. The \def{left-derivative} of \f{f} at \f{a}, or \f{f_-'(a)} is the limit \f{\lim^h_{0^-} (f(a + h) - f(a)) / h} \f{\given} The limit \f{\lim^h_{a^-} (f(h) - f(a)) / (h - a)}. \f{f} \def{left-differentiable} at \f{a} if the left-derivative of \f{f} at \f{a} exists. \f{f} is \left{left-differentiable on \f{(a,b)} if for all \f{x \in (a,b)} we have that \f{f} is left-differentiable at \f{x}. \f{f} is \def{left-differentiable} on \f{[a,b]} if for all \f{x \in [a,b]} we have that \f{f} is left-differentiable at \f{x \in (a,b]}. \f{f} is \def{left-differentiable everywhere} if for all \f{x \in \mathbb{R}} we have that \f{f} is differentiable at \f{x}. The \def{right-derivative} of \f{f} at \f{a}, or \f{f_+'(a)} is the limit \f{\lim^h_{0^+} (f(a + h) - f(a)) / h}, or the limit \f{\lim^h_{a^+} (f(h) - f(a)) / (h - a)}. \f{f} is \def{right-differentiable} at \f{a} if the right-derivative of \f{f} at \f{a} exists. \f{f} is \def{right-differentiable} on \f{(a,b)} if for all \f{x \in (a,b)} we have that \f{f} is right-differentiable at \f{x}. \f{f} is \def{right-differentiable} on \f{[a,b]} if for all \f{x \in [a,b]} we have that \f{f} is right-differentiable at \f{x \in [a,b)}. \f{f} is \def{right-differentiable everywhere} if for all \f{x \in \mathbb{R}} we have that \f{f} is differentiable at \f{x}.

All forms of the definitions (with regards to step size) are equivalent.

We will use \f{\star}-differentiable to refer to any type of differentiability for \f{f}.

The \def{derivative} of \f{f}, or \f{f'} or \f{df/dx} is the function \f{f' \colon A \rightarrow \mathbb{R}} with \f{A = \{ x \in \dom{f} \given f \text{ is differentiable at } x \} \subseteq \dom{f}} and \f{(f')(x) = f'(x)}. The \def{second derivative} of \f{f}, or \f{f"}, or \f{d^2f/dx^2} is the function \f{f" \colon A \rightarrow \mathbb{R}} with \f{A = \{ x \in \dom{f} \given f \text{ is differentiable at } x \text{ and } f' \text{ is differentiable at } x \} \subseteq \dom{f'} \subseteq \dom{f}} and \f{(f")(x) = (f')'(x)}. A \def{higher order derivative} of \f{f}, or \f{f^{(n)}}, or \f{d^nf/dx^n} is the function \f{f^{(n)} \colon A \rightarrow \mathbb{R}} with \f{A = \{ x \in \dom{f} \given f^{n-1} \text{ exists and is differentiable at } x \} \subseteq \dom{f^{(n-1)}} \subseteq \dots \subseteq \dom{f^{(1)}} \subseteq \dom{f}} and \f{(f^{(n)})(x) = (f^{(n-1)})'(x)}. This is the result of differentiating \f{f} \f{n} times. We have, by convention \f{f^{(0)} = f}. We also have \f{f^{(1)} = f'} and \f{f^{(2)} = f"}.

Intuitively, if \f{\dom{f} = \dom{f'}}, we can understand \f{f'} as providing information on the rate of change of \f{f}. Consider a point \f{x_0 \in \dom{f}} and \f{dx > 0} a sufficiently small offset. Then, if we know \f{f(x_0)} we can obtain \f{f(x_0 + dx) = f(x_0) + f'(x_0)}. In this sense, \f{f'} codes for each point, if, going to the ``next'' point in the domain, \f{f} will increase (\f{f'} is positive) or decrease (\f{f'} is negative) and by how much (\f{\abs{f'}}). Thus, if \f{\dom{f} = [a,b]} and we know \f{f(a)} and \f{f'} we can reconstruct \f{f}.

Let \f{f} be a continuous function in whatever way we want to.

Differentiability implies \ref{continuity}. \f{f} is \f{\star}-continuous at \f{a}.

First, assume \f{f} is differentiable at \f{a}. This means \f{\lim^h_0 (f(a+h) - f(a)) / h} exists and is finite. Also \f{\lim^h_0 h = 0} exists and is finite. Multiplying the two, by \thmref{NumericFunctionTransformationsLimits}, we obtain \f{(\lim^h_0 (f(a + h) - f(a)) / h)(\lim^h_0 h) = 0} or \f{\lim^h_0 h (f(a+h) - f(a)) / h = 0} or \f{\lim^h_0 f(a + h) - f(a) = 0} or \f{\lim^h_0 f(a + h) = f(a)}. By \thmref{NumericFunctionTransformationsLimits} we have \f{\lim^h_0 f(a + h) = \lim^h_a f(a + h - a) = \lim^h_a f(h)}. Therefore, we have \f{\lim^h_a f(h) = f(a)}, which means \f{f} is continuos.
For left-differentiability and right-differentiability, the proof is similar.

\ref{Monotonous} functions have a special relationship with derivatives:
* If \f{f} is right-differentiable at \f{a} and there exists \f{\epsilon > 0} such that \f{f} is nondecreasing on \f{(a,a + \epsilon)} then \f{f_+'(a) = \inf\{ (f(h) - f(a)) / (h - a) \given h \in (a,a + \epsilon) \}}.
* If \f{f} is right-differentiable at \f{a} and there exists \f{\epsilon > 0} such that \f{f} is nonincreasing on \f{(a,a + \epsilon)} then \f{f_+'(a) = \sup\{ (f(h) - f(a)) / (h - a) \given h \in (a,a + \epsilon) \}}.
* If \f{f} is left-differentiable at \f{a} and there exists \f{\epsilon > 0} such that \f{f} is nondecreasing on \f{(a - \epsilon,a)} then \f{f_-'(a) = \sup\{ (f(h) - f(a)) / (h - a) \given h \in (a - \epsilon,a) \}}.
* If \f{f} is left-differentiable at \f{a} and there exists \f{\epsilon > 0} such that \f{f} is nonincreasing on \f{(a - \epsilon,a)} then \f{f_-'(a) = \inf\{ (f(h) - f(a)) / (h - a) \given h \in (a - \epsilon,a) \}}.

For part \f{1}, we have that \f{f_+'(a) = \lim_{a^+} (f(h) - f(a)) / (h - a)}. The set \f{A^+ = \{ (f(h) - f(a)) / (h - a) \given h \in (a,a + \epsilon) \}} is bounded below by \f{0} (\f{h - a > 0} and \f{f(h) - f(a) \geq 0} for \f{h \in (a,a+\epsilon)}). It therefore admits an infimum. Suppose now, by contradiction, that \f{f_+'(a) < \inf{A^+}}. We can then find \f{\overline{\epsilon} = \inf{A^+} - f_+'(a)} such that for all \f{\delta > 0} we will have in fact for all \f{x} with \f{0 < x - a < \delta} and \f{x \in (a,a + \epsilon)} we have \f{\abs{(f(h) - f(a)) / (h - a) - f_+'(a)} \geq \overline{\epsilon}} or \f{(f(h) - f(a)) / (h - a) - f_+'(a) \geq \overline{\epsilon}} (since \f{(f(h) - f(a)) / (h - a)} is positive and greater, by construction than \f{f_+'(a)}) or \f{(f(h) - f(a)) / (h - a) - f_+'(a) \geq \inf{A^+} - f_+'(a)} or \f{(f(h) - f(a)) / (h - a) \geq \inf{A^+}}, which, for our chosen interval for \f{h} is true. But, this means the derivative does not exist, and this is a contradiction. Therefore \f{f_+'(a) = \inf\{ (f(h) - f(a)) / (h - a) \given h \in (a,a + \epsilon) \}}.
For parts \f{2}, \f{3} and \f{4} the proofs are similar to that for part \f{1}.

There is an interaction between basic function properties and differentiability as well.
* \f{f + g} is \f{\star}-differentiable at \f{a} and \f{(f + g)'(a) = f'(a) + g'(a)} (or \f{(f + g)' = f' + g'} or \f{d(f + g)/dx = df/dx + dg/dx}).
* \f{f - g} is \f{\star}-differentiable at \f{a} and \f{(f - g)'(a) = f'(a) - g'(a)} (or \f{(f - g)' = f' + g'} or \f{d(f - g)/dx = df/dx - dg/dx}).
* \f{fg} is \f{\star}-differentiable at \f{a} and \f{(fg)'(a) = f'(a)g(a) + f(a)g'(a)} (or \f{(fg)' = f'g + fg'} or \f{dfg/dx = df/dx g + fg/dx f}.
* If \f{g(a) \neq 0} then \f{1/g} is \f{\star}-differentiable at \f{a} and \f{(1/g)'(a) = -g'(a) / g^2(a)}.
* If \f{g(a) \neq 0} then \f{f/g} is \f{\star}-differentiable at \f{a} and \f{(f/g)'(a) = (f'(a)g(a) - f(a)g'(a)) / g^2(a)}.
* If \f{f} is \f{\star}-differentiable at \f{g(a)} then \f{(f \circ g)} is differentiable at \f{a} and \f{(f \circ g)'(a) = f'(g(a))g'(a)}.
* \f{f_1 \cdots f_n} is \f{\star}-differentiable at \f{a} and \f{(f_1 \cdots f_n)'(a) = \sum_{i=1}^nf_1(a)\cdots f_{i-1}(a)f_i'(a)f_{i+1}(a)\cdots f_{n}(a)}.
* If for all \f{i} we have \f{f_i} is \f{\star}-differentiable at \f{(f_1 \circ \dots \circ f_{i-1})(a)} then \f{(f_1 \circ \dots \circ f_n)(a)} is \f{\star}-differentiable at \f{a} and \f{(f_1 \circ \dots \circ f_n)'(a) = \Pi_{i=1}^n f_i'((f_1 \circ \dots \circ f_{i-1})(a))}.
* If \f{f(a) \neq 0} then \f{\abs{f}} is \f{\star}-differentiable at \f{a} and \f{\abs{f}'(a) = \abs{f'(a)}}.
* If \f{f(a) \neq g(a)} then \f{\max(f,g)} is \f{\star}-differentiable at \f{a} and \f{(\max(f,g))'(a) = \max(f'(a),g'(a))}.
* If \f{f(a) \neq g(a)} then \f{\min(f,g)} is \f{\star}-differentiable at \f{a} and \f{(\min(f,g))'(a) = \min(f'(a),g'(a))}.
* If \f{f^{(n)}(a)} and \f{g^{(n)}(a)} exist then \f{(fg)^{(n)}(a) = \sum_{k=0}^n {\binom{n}{k}} f^{(k)}(a)g^{(n-k)}(a)} - Leibniz's Formula.
* If \f{f^{(n)}(g(a))} and \f{g^{(n)}(a)} exist then \f{(f \circ g)^{(n)}(a) = \sum_{i=1}^n f^{(i)}(g(a))[\sum_{j=1}^{k_i} A_{ij} g^{(p_1^{ij})}(a) \cdots g^{(p_{l_i}^{ij})}(a)]}, where \f{k_i \geq 1}, \f{A_{ij} \in \mathbb{R}} and \f{\sum_{k=1}^{l_i}p_k^{ij} = n}.

For part \f{1}, we have that \f{(f + g)'(a) = \lim^h_{0^\star} ((f + g)(a + h) - (f + g)(a)) / h = \lim^h_{0^\star} (f(a + h) + g(a + h) - f(a) - g(a)) / h = \lim^h_{0^\star} [(f(a + h) - f(a)) / h + (g(a + h) - g(a)) / h]}. Since both these limits exist and are finite, we have by \thmref{NumericFunctionTransformationsLimits} that \f{\lim^h_{0^\star} ((f + g)(a + h) - (f + g)(a)) / h = \lim^h_{0^\star} (f(a + h) - f(a)) / h - \lim^h_{0^\star} (g(a + h) - g(a)) / h = f'(a) + g'(a)}. Therefore, we have \f{(f + g)'(a) = f'(a) + g'(a)}. 
For part \f{2}, the proof is similar to that for part \f{1}.
For part \f{3}, we have that \f{\lim^h_{0^\star} ((fg)(a + h) - (fg)(a)) / h = \lim^h_{0^\star} (f(a+h)g(a+h) - f(a)g(a))/h = \lim^h_{0^\star} (f(a + h)(g(a + h) + g(a) - g(a)) - f(a)g(a)) / h = \lim^h_{0^\star} (f(a + h)g(a + h) + f(a + h)g(a) - f(a + h)g(a) - f(a)g(a)) / h = \lim^h_{0^\star} (f(a + h)g(a) - f(a)g(a)) / h + \lim^h_{0^\star} (f(a + h)g(a + h) - f(a + h)g(a)) / h = \lim^h_{0^\star} g(a) (f(a + h) - f(a)) / h + \lim^h_{0^\star} f(a + h) (g(a + h) - g(a))/ h = g(a) \lim^h_{0^\star} (f(a + h) - f(a)) / h + \lim^h_{0^\star} f(a + h) \lim^h_{0^\star} (g(a + h) - g(a)) / h = g(a) f'(a) + f(a) g'(a) = f'(a) g(a) + f(a)g'(a)}, which is what we wanted. Several times in the previous expansions we have used the fact that the limits involved existed and used \thmref{NumericFunctionTransformationsLimits}.
For part \f{4}, we must first make sure that \f{\lim^h_{0^\star} ((1/g)(a + h) - (1/g)(a))/h} makes sense around \f{0}. Since \f{g} is differentiable at \f{a}, we have that, by \thmref{DifferentiabilityImpliesContinuity}, that \f{g} is continuous at \f{a}. Since \f{\lim_a g(x) = g(a) \neq 0}, this means that \f{\lim_a g(x)} is either positive or negative. By \thmref{LocalBehaviourLimits} we have that there exists a \f{\delta > 0} such that for all \f{x} with \f{0 < \abs{x - a} < \delta} then \f{f(x)} is also positive or negative as \f{g(a)}. Therefore, for sufficiently small \f{h} we have that \f{g(a + h) \neq 0}, and the limit makes sense. We then have that \f{(1/g)'(a) = \lim^h_{0^\star} ((1/g)(a + h) - (1/g)(a)) / h = \lim^h_{0^\star} (1/g(a+h) - 1/g(a)) / h = \lim^h_{0^\star} - (g(a+h) - g(a))/(g(a)g(a+h)h) = - 1/g(a) \lim^h_{0^\star} 1/g(a+h) \lim^h_{0^\star} (g(a + h) - g(a)) = -g'(a) / g^2(a)}, which is what we wanted. Alternatively, we could use the product rule and the chain rule with \f{1/x} and \f{f(x)} to get what we wanted.
For part \f{5}, we just have to apply the product rule to \f{(f (1/g))'} and we obtain what we want.
For part \f{6}, we define \f{\Phi(h) = (f(g(a + h)) - f(g(a))) / (g(a + h) - g(a))} if \f{g(a + h) - g(a) \neq 0} and \f{\Phi(h) = f'(g(a))} if \f{g(a + h) - g(a) = 0}. We first prove that \f{\Phi} is continuous at \f{0}, that is \f{\lim^h_0 \Phi(h) = \Phi(0)}. Let \f{\epsilon > 0}. We must find \f{\delta > 0} such that for all \f{h \in (-\delta,\delta)\setminus\{0\}} we have that \f{\abs{\Phi(h) - \Phi(0)} < \epsilon}. Now, whatever \f{\delta} we choose, for a certain \f{h} for which \f{g(a + h) - g(a) = 0} we have that \f{\Phi(h) = f'(g(a)) = \Phi(0)}, therefore we have \f{\abs{\Phi(h) - \Phi(0)} = \abs{\Phi(0) - \Phi(0)} = 0 < \epsilon}. We will, therefore, focus on the case where \f{g(a + h) - g(a)} is defined. We can rewrite \f{\Phi(h) = (f(g(a + h) - g(a) + g(a)) - f(g(a))) / (g(a + h) - g(a))}. Let \f{k(h) = g(a + h) - g(a)} and rewrite \f{\Phi(h) = (f(g(a) + k(h)) - f(g(a))) / k(h)}, which looks very close to a derivative-like limit. Now, \f{\lim_0 k(h) = \lim_0 g(a + h) - g(a) = 0}. Also, we know \f{f} is differentiable at \f{g(a)}, therefore, for our \f{\epsilon} we can find a \f{\overline{\delta} > 0} such that, for all \f{k} with \f{0 < \abs{k} < \overline{\delta}} we have \f{\abs{(f(g(a) + k) - f(g(a))) / k - f'(g(a))} < \epsilon}. We will choose our \f{\delta} such that for all \f{h} with \f{0 < \abs{h} < \delta} we have \f{\abs{k(h)} < \overline{\delta}} (we can do this since \f{\lim_0 k(h) = 0}). Therefore, for a chosen \f{h}, we have \f{k(h) < \overline{\delta}} which means that it can act as \f{k} in \f{\abs{(f(g(a) + k(h)) - f(g(a))) / k(h) - f'(g(a))} < \epsilon}, which is what we wanted. We have, therefore, proven that \f{\lim^h_0 \Phi(h) = \Phi(0)}. We can now write \f{(f \circ g)'(a) = \lim^h_0 (f(g(a + h)) - f(g(a))) / h = \lim^h_0 ((f(g(a + h)) - f(g(a))) / (g(a + h) - g(a))) (g(a + h) - g(a)) / h}. Since both limits are defined, we have, by \thmref{NumericFunctionTransformationsLimits}, that \f{(f \circ g)'(a) = \lim^h_0 \Phi(h) \lim^h_0 (g(a + h) - g(a)) / h = \Phi(0) g'(a) = f'(g(a)) g'(a)}, which is what we wanted.
For parts \f{7} and \f{8}, a simple induction argument suffices.
For part \f{9}, assume \f{f(a) > 0} and consider only the case where \f{f} is simply differentiable. Since \f{f} is differentiable at \f{a}, we have, by \thmref{DifferentiabilityImpliesContinuity}, that \f{f} is continuous at \f{a} and \f{\lim_a f(x) > 0}. By, \thmref{LocalBehaviourLimits}, there is a \f{\delta > 0} such that for all \f{x \in (a-\delta,a+\delta)} we have that \f{f(x) > 0}. Now, we will prove that \f{\abs{f}'(a) = f'(a)} in this case. We have \f{\lim^h_0 (\abs{f}(a + h) - \abs{f}(a)) / h = f'(a)}. Let \f{\epsilon > 0}. We must find a \f{\delta_{\abs{\star}} > 0} such that for all \f{h} with \f{0 < \abs{h} < \delta_{\abs{\star}}} we have \f{\abs{ (\abs{f}(a + h) - \abs{f}(a)) / h - f'(a) } < \epsilon}. If we select \f{\delta_{\abs{\star}} = \min(\delta',\delta")}, where \f{\delta'} is such that for \f{h} with \f{0 < \abs{h} < \delta'} we have \f{\abs{a + h} < \delta} and \f{\delta"} is such that for \f{h} with \f{0 < \abs{h} < \delta"} we have \f{\abs{ (f(a + h) - f(a)) / h - f'(a) } < \epsilon}. \f{\delta_{\abs{\star}}} will have all the properties of \f{\delta'} and \f{\delta"}. Therefore, for all \f{h} with \f{0 < \abs{h} < \delta_{\abs{\star}}} we have that \f{f(a + h) > 0} and \f{f(a) > 0}, therefore, we have \f{\abs{ (\abs{f}(a + h) - \abs{f}(a)) / h - f'(a)} = \abs{(f(a + h) - f(a)) / h - f'(a)} < \epsilon}. Therefore, we've proven that \f{\abs{f}'(x) = f'(x)} when \f{f(a) > 0}. By a similar approach we can prove that \f{\abs{f}'(x) = -f'(x)} when \f{f(a) < 0}. The proof is similar for left- and right-differentiabilty.
For part \f{10}, assume \f{f(a) > g(a)} and consider only the case where \f{f} is simply differentiable. Since \f{f} and \f{g} are differentiable at \f{a}, we have, by \thmref{DifferentiabilityImpliesContinuity}, that \f{f} is continuous at \f{a} and \f{g} is continuous at \f{a} and \f{\lim_a f(x) > \lim_a g(x)}. By, \thmref{LocalBehaviourLimits}, there is a \f{\delta > 0} such that for all \f{x \in (a-\delta,a+\delta)} we have that \f{f(x) > g(x)}. Now, similar to part \f{9}, we select a \f{\delta_{\max}} small enough to have the local behaviour and small enough so that for all \f{h} with \f{0 < \abs{h} < \delta_{\max}} we have \f{\abs{(f(a + h) - f(a))/h - f'(a)} < \epsilon}. We then have for \f{h} with \f{0 < \abs{h} < \delta_{\max}} that \f{\abs{(\max(f,g)(a + h) - \max(f,g)(a)) / h - f'(a)} = \abs{(f(a + h) - f(a)) / h - f'(a)} < \epsilon}, which is true. By a similar approach we can prove that \f{\max(f,g)'(x) = g'(x)} when \f{f(a) < g(a)}. The proof is similar for left- and right-differentiability.
For part \f{11}, the proof is similar to that for part \f{10}.
For part \f{12}, a simple induction argument suffices, as well as the application of the theorem which states that \f{\binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}}.
For part \f{13}, a simple induction argument suffices.

If \f{f + g} is differentiable at \f{a}, this does not necessarily mean that \f{f} and \f{g} are differentiable at \f{a} (think of step functions).

Some common domain modifications and their interaction with differentiability.
* If \f{f} is \f{\star}-differentiable at \f{a} then \f{f(x) + c} is \f{\star}-differentiable at \f{a} and \f{(f(x) + c)'(a) = f'(a)} - codomain shift.
* If \f{f} is \f{\star}-differentiable at \f{c + a} then \f{f(x + c)} is \f{\star}-differentiable at \f{a} and \f{(f(x + c))'(a) = f'(a + c)} - domain shift.
* If \f{f} is \f{\star}-differentiable at \f{a} then \f{cf(x)} is \f{\star}-differentiable at \f{a} and \f{(cf(x))'(a) = cf'(a)} - codomain shift.
* If \f{f} is \f{\star}-differentiable at \f{ca} then \f{f(cx)} is \f{\star}-differentiable at \f{a} and \f{(f(cx)')(a) = cf'(ca)} - domain scale.

For part \f{1}, proof \f{1}: simply apply \thmref{NumericFunctionTransformationsDifferentiability} (and see SpecialNumericFunctions). Proof \f{2}: we have \f{(f + c)'(a) = \lim^h_0 ((f + c)(a + h) - (f + c)(a)) / h = \lim^h_0 (f(a + h) - f(a)) / h = f'(a)}, which is what we wanted.
For part \f{2}, proof \f{1}: simply apply \thmref{NumericFunctionTransformationsDifferentiability} (and see SpecialNumericFunctions). Proof \f{2}: we have \f{g(x) = f(x + c)} and \f{g'(a) = \lim^h_0 (g(a + h) - g(a)) / h = \lim^h_0 (f(a + h + c) - f(a + c)) / h = \lim^h_0 (f(t + h) - f(t)) / h = f'(t) = f'(t + c)}, where \f{t = c + a}.
For part \f{3}, proof \f{1}: we can use \thmref{NumericFunctionTransformationsDifferentiability} and the fact that if \f{f(x) = c} then \f{f'(x) = 0} (see SpecialNumericFunctions) to get our answer. Proof \f{2}, we have that \f{g(x) = f(x) + c}. Then, we have \f{g'(a) = \lim^h_0 (g(a + h) - g(a)) / h = \lim^h_0 (cf(a + h) - cf(a)) / h = \lim^h_0 c (f(a + h) - f(a)) / h}. Since \f{\lim^h_0 c = c} (see SpecialNumericFunctions) and \f{\lim^h_0 (f(a + h) - f(a)) / h = f'(a)} exists, we have that, by \thmref{NumericFunctionTransformationsLimits}, we get \f{\lim^h_0 c (f(a + h) - f(a)) / h = \lim^h_0 c \lim^h_0 (f(a + h) - f(a)) / h = c f'(a)} or \f{g'(a) = cf'(a)}, which is what we wanted.
For part \f{5}, proof \f{1}: simply apply \thmref{NumericFunctionTransformationsDifferentiability} (and see SpecialNumericFunctions). Proof \f{2}: we have \f{g(x) = f(cx)} and \f{g'(a) = \lim^h_0 (g(a + h) - g(a)) / h = \lim^h_0 (f(ca + ch) - f(ca)) / h = \lim^h_0 c (f(ca + ch) - f(ca)) / ch = c \lim^h_0 (f(t + ch) - f(t)) / ch} with \f{t = ca}. Now, we have \f{\lim^h_0 ch = h}, and, \f{ch} behaves like \f{h} when \f{h \rightarrow 0}, therefore, by \thmref{NumericFunctionTransformationsLimits}, we have \f{\lim^h_0 (f(t + ch) - f(t)) / ch = \lim^h_0 (f(t + h) - f(t)) / h = f'(t) = f'(ca)}. Therefore, we have \f{g'(a) = cf'(ca)}, which is what we wanted.

At \ref{maxima} or \ref{minima} of a function, the derivative is \f{0}. More precisely, let \f{f} be a function on \f{(a,b)}, \f{x} a (local) maxima (or minima) of \f{f} on \f{(a,b)} and \f{f} is differentiable at \f{x}. Then \f{f'(x) = 0}.

Proof \f{1}: we have that \f{f'(x) = \lim^h_0 (f(x + h) - f(x)) / h} and \f{x} is a maximum of \f{f}. Therefore, for a tight enough \f{\delta} we will have that for \f{h \in (0,\delta)} we have \f{(f(x + h) - f(x)) / h) \leq 0} since \f{f(x + h) \leq f(x)}. Similarly we have that for \f{h \in (-\delta,0)} we have \f{(f(x + h) - f(x)) / h \geq 0} since \f{f(x + h) \geq f(x)}. Assume that \f{f'(x) > 0}. This means that, by \thmref{LocalBehaviourLimits}, there is an \f{\overline{delta}} such that for all \f{h \in (-\delta,\delta) \setminus \{0\}} we have \f{(f(x + h) - f(x)) / h > 0}. This contradicts our previous result that on one side of \f{0} we have positive values and on the other we have negative values. A similar contradiction follows if we assume \f{f'(x) < 0}. Therefore, we have to have \f{f'(x) = 0}. If \f{x} is a minima point the argument is almost the same.
Proof \f{2}: If we look at the left and right limits, one is positive, while the other is negative. Since \f{f'(x)} exists, we have by \thmref{PropertiesPartialLimitsLimitsInfinity}, the left and right limits must be equal, therefore \f{0}.

\f{x} is a \def{critical point} of \f{f} if \f{f'(x) = 0}. The critical points of \f{f}, the end points of intervals in the domain and the points where \f{f} is not differentiable are the candidates for minima and maxima for \f{f}. If the problem is "tractable", enumerating these points is a strategy for finding the extreme points of a function. This sort of procedure, again, if "tractable", will always locate the maximum and minimum value of a continuous function on a closed interval. If the function is not continuous or if we are seeking the maximum or minimum on an open interval or the whol eline, then we cannot even be sure beforehand that the maximum and minimum values exist, so all the information obtained by this procedure may say nothing.

There is a suite of theorems, the \def{mean value theorems}, which have very wide applications: Let \f{f \text{ and } g} be functions which are continuous on \f{[a,b]} and differentiable on \f{(a,b)}.
* If \f{f(a) = f(b)} then there is a number \f{x \in (a,b)} such that \f{f'(x) = 0} - \red{Rolle's Theorem}.
* There exists \f{x \in (a,b)} such that \f{f'(x) = (f(b) - f(a)) / (b-a)} - \def{Mean Value Theorem}.
* There exists \f{x \in (a,b)} such that \f{(f(b) - f(a))g'(x) = (g(b) - g(a))f'(x)} - \def{Cauchy Mean Value Theorem}.

Let \f{f} be a function which is continuous and differentiable on \f{(a,b)}, \f{\lim^y_{a^+}f(y)} exists and \f{\lim^y_{b^-}f(y)} exists.
* There exists \f{x \in (a,b)} such that \f{f'(x) = (\lim^y_{b^-}f(y) - \lim^y_{a^+}f(y)) / (b - a)}.

Let \f{f} and \f{g} be functions which are continuous on \f{[a,b]} and differentiable on \f{(a,b)} and \f{g(b) \neq g(a)} and \f{f'(x)} and \f{g'(x)} are never simultaneously \f{0}.
* There exists \f{x \in (a,b)} such that \f{(f(b) - f(a)) / (g(b) - g(a)) = f'(x) / g'(x)}.

For part \f{1}, since \f{f} is continuous on \f{[a,b]} we have that, by \thmref{MaximumMinimumTheorem}, that \f{f} has a maximum point on \f{[a,b]}. If \f{f} is constant, then \f{f(a) = f(b) = f(x)} for all \f{x \in (a,b)} means that any \f{x \in (a,b)} is a maximum point and, by \thmref{DerivativesAtMaximaMinima} we have \f{f'(x) = 0}, which is what we wanted. If \f{f} is not constant, then there is a point \f{x \in (a,b)} which has \f{f(x)} different from \f{f(a) = f(b)}. If \f{f(x) > f(a)} then, by \thmref{MaximumMinimumTheorem}, we have a maximum point \f{\overline{x}}, but this maximum must be \f{f(\overline{x}) > f(x)}, therfore, \f{\overline{x} \neq a} and \f{\overline{x} \neq b}, therefore \f{\overline{x} \in (a,b)}. Furthermore, by \thmref{DerivativesAtMaximaMinima}, we have \f{f'(\overline{x}) = 0}. If \f{f(x) < f(a)} then, by a same argument, we'll have \f{f'(\overline{x}) = 0}. We have thus proved our point.
For part \f{2}, we build the function \f{h(x) = f(x) - (f(b) - f(a)) / (b - a) (x - a)}, which is the distance from \f{f(x)} to the secant between \f{(a,f(a))} and \f{(b,f(b))}. Now, we have that \f{h} is continuous on \f{[a,b]}, by \thmref{NumericFunctionTransformationsContinuity} and being the difference of two continous functions on \f{[a,b]}, and \f{h} is differentiable on \f{(a,b)}, by \thmref{NumericFunctionTransformationsDifferentiability} and being the difference of two differentiable functions on \f{(a,b)}, and \f{h(a) = f(a)} and \f{h(b) = f(b)}. Therefore, by part \f{1}, we have that there exists an \f{x \in (a,b)} such that \f{h'(x) = 0}. This means \f{f'(x) - (f(b) - f(a)) / (b - a) = 0} or \f{f'(x) = (f(b) - f(a)) / (b - a)}, which is what we wanted.
For part \f{3}, let \f{h(x) = f(x)(g(b) - g(a)) - g(x)(f(b) - f(a))} with \f{h} continuous on \f{[a,b]}, differentiable on \f{(a,b)}, \f{h(a) = f(a)g(b) - g(a)f(b)} and \f{h(b) = f(a)g(b) - g(a)f(b)}. By part \f{1}, we then have that there exists \f{x \in (a,b)} such that \f{h'(x) = 0}, which means \f{f'(x)(g(b) - g(a)) - g'(x)(f(b) - f(a)) = 0}, from which we get what we wanted.
For part \f{4}, we can build \f{\overline{f}} such that \f{\overline{f}(x) = f(x)} for \f{x \in (a,b)} and \f{\overline{f}(a) = \lim^y_{a^+} f(y)} and \f{\overline{f}(b) = \lim^y_{b^-} f(y)}. \f{\overline{f}} is continous then on \f{[a,b]} and differentiable on \f{(a,b)}, therefore, by part \f{2} we have that there exists \f{x \in (a,b)} such that \f{\overline{f}'(x) = (\overline{f}(b) - \overline{f}(a)) / (b - a)}. Now, \f{\overline{f}' = f'}, and we get what we wanted.

Another \def{pasting theorem}. Let \f{a, b \text{ and } c} be numbers, \f{f} differentiable on \f{[a,b)} and \f{g} differentiable on \f{(b,c]}, \f{f(b) = g(b)}, \f{h(x) \colon [a,c] \rightarrow \mathbb{R}} with \f{h(x) = f(x)} for \f{x \in [a,b]} and \f{h(x) = g(x)} for \f{x \in [b,c]} and the left-derivative of \f{f} at \f{a} is equal to the right derivative of \f{g} at \f{a}. \f{h} is differentiable on \f{[a,c]}.

Since the left and right derivatives are equal, it means \f{\lim^h_{0^-} (f(a + h) - f(a)) / h = \lim^h_{0^-} (h(a + h) - h(a)) / h = \lim^h_{0^+} (h(a + h) - h(a)) / h = \lim^h_{0^+} (g(a + h) - g(a)) / h}, since for values smaller than \f{a}, which \f{a + h} is, in the \f{0^-} case, \f{f} is equal to \f{h} and for values larger than \f{a}, which \f{a + h} is, in the \f{0^+} case, \f{g} is equal to \f{h}. By \thmref{PropertiesPartialLimitsLimitsInfinity}, the limit \f{\lim^h_0 (h(a + h) - h(a)) / h} exists, therefore \f{h'(a)} exists and is equal to the different partial limits. Furthermore, \f{h} is differentiable on \f{[a,b)}, because \f{f} is differentiable on \f{[a,b)} and \f{h} is differentiable on \f{(b,c]} because \f{g} is differentiable on \f{(b,c]}. Therefore, \f{h} is differentiable on \f{[a,c]}.

Let \f{f} and \f{g} be functions, \f{a} a number, \f{\delta > 0}, for all \f{x \in (a-\delta,a+\delta)} we have \f{f(x) = g(x)}, \f{f} differentiable at \f{a} and \f{g} differentiable at \f{a}. Then \f{f'(a) = g'(a)}. The \ref{influence of local behavior}.

We will prove \f{\lim^h_a (f(h) - f(a)) / (h - a) = g'(a)}. Let \f{\epsilon > 0}, we will choose a \f{\delta_f = \min(\delta,\delta_g)}, where \f{\delta_g} is such that for all \f{h} with \f{\abs{h - a} < \delta_g} we have \f{\abs{(g(h) - g(a)) / (h - a) - g'(a)} < \epsilon}. Now, for \f{h} such that \f{\abs{h - a} < \delta_f} we have that \f{f(h) = g(h)} or \f{f(h) - f(a) = g(h) - f(a)} or \f{f(h) - f(a) = g(h) - g(a)} (since \f{f(a) = g(a)}) or \f{(f(h) - f(a)) / (h - a) = (g(h) - g(a)) / (h - a)}. Since for our \f{h} we also have \f{\abs{(g(h) - g(a)) / (h - a) - g'(a)} < \epsilon} we also have  \f{\abs{(f(h) - f(a)) / (h - a) - g'(a)} < \epsilon}, which is what we wanted. We have therefore found an \f{\delta_f} for our \f{\epsilon} and we have the limit we wanted, thus \f{f'(a) = g'(a)}.

Interaction between even and oddity. Let \f{f} be a function which is \f{k}-differentiable everywhere.
* If \f{f} is even then \f{f'} is odd.
* If \f{f} is odd then \f{f'} is even.
* If \f{f} is even then \f{f^{(k)}} is even if \f{k} is even and odd if \f{k} is odd.
* If \f{f} is odd then \f{f^{(k)}} is odd if \f{k} is even and even if \f{k} is odd.

For part \f{1}, we have \f{f'(x) = \lim^h_0 (f(x + h) - f(x)) / h = \lim^h_0 (f(-x - h) - f(-x)) / h = - \lim^h_0 (f(-x - h) - f(-x)) / (-h) = - \lim^h_0 (f(-x + h) - f(-x)) / h = -f'(-x)}, which is what we wanted. We used the fact that \f{-h} tends to \f{0} as \f{h} tends to \f{0} and \thmref{NumericFunctionTransformationsLimits}.
For parts \f{2}, \f{3} and \f{4}, the proof is similar to that for part \f{1}.

Some \def{behavior at infinity}. Let \f{f} is a differentiable function and \f{\lim_{+\infty}f(x)} exists and \f{\lim_{+\infty}f'(x)} exists.
\f{\lim_{+\infty}f'(x) = 0}.
This is not true if only \f{\lim_{+\infty}f(x)} exists, as \f{\lim_{+\infty}f'(x)} might not exist - see \f{x^{-1}\sin{x^3}} for a counterexample.

Assume, by contradiction, that \f{\lim_{+\infty}f'(x) \neq 0}. Assume that \f{\lim_{+\infty}f'(x) > 0}. Then, there exists an \f{N' > 0} such that for all \f{x > N'} we have \f{f'(x) > M > 0}, where \f{M < \lim_{+\infty}f'(x)}. But this means that on \f{(N',y)} for all \f{y > N'}, \f{f} is increasing, by \thmref{UnderstandingFunctionsFromDerivatives}, therefore \f{f} is increasing on \f{(N',+\infty)}. Similarly, for \f{\epsilon > 0} there exists an \f{N > 0} such that for all \f{x > N} we have \f{\abs{f(x) - \lim_{+\infty}f(x)} < \epsilon}. Now, select \f{x > \max(N,N')}. If we let \f{\epsilon = M/2} we then get, \f{\abs{f(x + 1) - \lim^{+\infty}f(x)} < M/2} and \f{\abs{f(x) - \lim^{+\infty}f(x)} < M/2} or \f{\abs{f(x + 1) - f(x)} < M}. But, by \thmref{BoundedGrowth}, we have that \f{f(x + 1) - f(x) \geq M}, which is a contradiction. If we assume \f{\lim_{+\infty}f'(x) < 0}, we have a very similar proof.

== Undestanding Functions From Their Derivatives ==

Let \f{f \text{ and } g} be functions which are continuous on \f{[a,b]} and differentiable on \f{(a,b)}.
* If for all \f{x \in [a,b]} we have \f{f'(x) = 0} then there exists a number \f{c} such that \f{f = c}.
* If for all \f{x \in [a,b]} we have \f{f'(x) = g'(x)} then there exists a number \f{c} such that \f{f = g + c}.
* If for all \f{x \in [a,b]} we have \f{f'(x) > 0} then \f{f} is increasing on \f{[a,b]}.
* If for all \f{x \in [a,b]} we have \f{f'(x) < 0} then \f{f} is decreasing on \f{[a,b]}.
* For all \f{x \in [a,b]} we have \f{f'(x) \geq 0} if and only if \f{f} is nondecreasing on \f{[a,b]}.
* For all \f{x \in [a,b]} we have \f{f'(x) \leq 0} if and only if \f{f} is nonincreasing on \f{[a,b]}.
* If \f{f'} is continuous and \f{c_1, c_2 \in [a,b]} are two adjacent critical points of \f{f} then the sign of \f{f'} on \f{[c_1,c_2]} can be determined by a single inspection of \f{f'(y)} for \f{y \in (c_1,c_2)}.

The converse of points \f{1} and \f{2} are true, but the converse of point \f{3} is not true. In general, if \f{f} is increasing then \f{f'(x) \geq 0}, while if \f{f} is decreasing then \f{f'(x) \leq 0} (think of \f{x^3}).

Let \f{c \in (a,b)} be critical point of \f{f}, \f{c_1, c_2 \in [a,b]} with \f{c_1 < c < c_2} be two adjacent critical points to \f{c} of \f{f}, or \f{c_1 = a} and \f{c_2 = b} if such points don't exist and \f{y_1 \in (c_1,c)} and \f{y_2 \in (c,c_2)}.
* If \f{f'(y_1) > 0} and \f{f'(y_2) < 0} then \f{c} is a local maximum of \f{f}.
* If \f{f'(y_1) < 0} and \f{f'(y_2) > 0} then \f{c} is a local minimum of \f{f}.
* If \f{f'(y_1) < 0} and \f{f'(y_2) < 0} or \f{f'(y_1) > 0} and \f{f'(y_2) > 0} then \f{c} is neither a local minima nor a local maxima of \f{f}.

Let \f{f} be twice-differentiable on \f{(a,b)} and \f{c} a critical point of \f{f}.
* If \f{f"(c) > 0} then \f{c} is a local minimum for \f{f}.
* If \f{f"(c) < 0} then \f{c} is a local maximum for \f{f}.

If \f{f"(c) = 0} then we can't say anything new about \f{c}, besides the fact that it is a critical point. The converse of points \f{9} and \f{10} is not true. In general, if \f{c} is a maximum then \f{f"(x) \leq 0}, while if \f{c} is a minimum then \f{f"(x) \geq 0}.

For part \f{1}, let \f{x < y < z \in [a,b]} be three numbers. By \thmref{MeanValueTheorems} we have that \f{f'(y) = (f(z) - f(x)) / (z - x) = 0} or \f{f(z) = f(x)}. Since this holds for any arbitrary triple of numbers, we have that \f{f} is constant.
For part \f{2}, consider the function \f{f - g}, then \f{(f - g)' = 0}, therefore, by part \f{1}, we have \f{f - g = c} or \f{f = g + c}, which is what we wanted.
For part \f{3}, let \f{x < y < z \in [a,b]} be three numbers. By \thmref{MeanValueTheorems} we have that \f{f'(y) = (f(z) - f(x)) / (z - x) > 0}. Since \f{z - x > 0} we have \f{f(z) - f(x) > 0} or \f{f(z) > f(x)}. Since this holds for any arbitrary triple of numbers, we have that \f{f} is increasing.
For part \f{4}, the proof is similar to that for part \f{3}.
For part \f{5}, we first prove ``\f{\Rightarrow}''. Let \f{x,y \in [a,b]} with \f{x < y}. By \thmref{MeanValueTheorem} we have that there exists \f{z \in [x,y]} such that \f{f'(z) = (f(y) - f(x)) / (y - x)}. Since \f{f'(z) \geq 0} and \f{y - x > 0} we have that \f{f(y) - f(x) \geq 0} or \f{f(y) \geq f(x)}. Since this holds for any arbitrary \f{x} and \f{y} we have that \f{f} is nondecreasing. We now prove ``\f{\Leftarrow}''. Let \f{x \in [a,b]}. We will only need to prove that \f{\lim^h_{x^+} (f(h) - f(x)) / (h - x) \geq 0}. By \thmref{PropertiesPartialLimitsLimitsInfinity} we then have that \f{f'(x) = \lim^h_x (f(h) - f(x)) / (h - x) = \lim^h_{x^+} (f(h) - f(x)) / (h - x) \geq 0} or \f{f'(x) \geq 0}, which is what we want. Furthermore, by \thmref{LeftRightDerivativesMonotonousFunctions}, we have that \f{f_+'(x) = \inf\{ (f(h) - f(x)) / (h - x) \given h > x \}}. Since \f{f(h) - f(x) \geq 0} and \f{h - x > 0}, we have that \f{(f(h) - f(x)) / (h - x) \geq 0}, for \f{h > x}, therefore \f{f_+'(x) \geq 0} at least. Therefore, \f{f'(x) \geq 0}.
For part \f{6}, the proof is similar to that for part \f{5}.
For part \f{7}, if \f{c_1} and \f{c_2} are adjacent, then there is no other \f{c_3 \in (c_1,c_2)} such that \f{f'(c_3) = 0}. Therefore, there cannot be two points \f{y_1, y_2 \in (c_1,c_2)} with \f{f'(y_1) < 0} and \f{f'(y_2) > 0} or \f{f'(y_1) > 0} and \f{f'(y_2) < 0}, because, by \thmref{IntermediateValueTheorem}, we would have to have a \f{0} crossing.
For part \f{8}, by part \f{3}, we have that on \f{[c_1,c]} \f{f} is increasing, while, by part \f{4}, on \f{[c,c_2]} \f{f} is decreasing, therefore, all the points to the left of \f{c} have smaller images than \f{c}, and all the points to the right of \f{c} have smaller images than \f{c}, therefore \f{c} is a local maximum point.
For parts \f{9} and \f{10}, the proof is similar to that for part \f{8}.
For part \f{11}, we have that, since \f{c} is a critical point, \f{f'(c) = 0}. Therefore, we have that \f{f"(c) = \lim^h_0 (f'(a + h) - f'(a)) / h = \lim^h_0 f'(a + h) / h > 0}. Since this has to hold for \f{h} positive or negative in an interval around \f{0}, \f{f'(a + h)} must be positive or negative according to \f{h} as well, therefore, by part \f{9}, \f{c} is a local minimum.
For part \f{12}, the proof is similar to that for part \f{11}.

== L'Hopitals Rules ==

\def{L'Hopital's Rule} is a important connection between differentiability and limits. Let \f{f,g} be functions with \f{\lim_a f(x) = \lim_a g(x) = 0} and \f{\lim_a f'(x) / g'(x)} exists. Then \f{\lim_a f(x) / g(x)} exists and \f{\lim_a f(x)/g(x) = \lim_a f'(x)/g'(x)}. \ref{DifferentiableFunctionsCantBeTooNasty} is a special case with \f{f(x) = \overline{f}(x) - \overline{f}(a)}, \f{g(x) = x - a} and using one alternative definition of the derivative.

Because \f{\lim_a f'(x)/g'(x)} exists we have that there exists \f{\delta > 0} such that for all \f{x \in (a - \delta,a + \delta) \setminus \{0\}} we have \f{f'(x)} and \f{g'(x)} exist and \f{g'(x) \neq 0}. On the other hand, \f{f} and \f{g} are not even assumed to be defined at \f{a}. We can define \f{f(a) = g(a) = 0} (changing the previous values of \f{f(a)} and \f{g(a)}, if necessary). Thus, \f{f} and \f{g} are continous at \f{a}. If we select an \f{x \in [a,a+\delta)} we have that \f{g} is continuous on \f{[a,x]} and differentiable on \f{(a,x)}, therefore, by \thmref{MeanValueTheorems} we have that \f{g(x) \neq 0}. Otherwise, we would have an \f{\overline{x} \in (a,x)} with \f{g'(x) = (g(x) - g(0)) / (x - 0) = 0}, which contradicts our first results. Since this is valid for all \f{x \in [a,a+\delta)} we have that \f{g(x) \neq 0} for all \f{[a,a+\delta)}. A similar proof is used to show that \f{g(x) \neq 0} for all \f{x \in (a-\delta,a+\delta)}. If we again select an \f{x \in [a,a+\delta)} we have that \f{f} and \f{g} are continous on \f{[a,x]} and differentiable on \f{(a,x)}, therefore, by \thmref{MeanValueTheorems} we have that there is a number \f{\overline{x} \in (a,x)} such that \f{(f(x) - 0)g'(\overline{x}) = (g(x) - 0)f'(\overline{x})} or \f{f(x) / g(x) = f'(\overline{x})/g'(\overline{x})} since both \f{g} and \f{g'} are non-null at the points we're considering. Now, \f{\overline{x}} approaches \f{a} as \f{x} approaches \f{a}, because \f{\overline{x}} is in \f{(a,x)}. Since \f{\lim^y_a f'(y)/g'(y)} exists, we have that \f{\lim_a f(x)/g(x) = \lim_a f'(\overline{x}) / g'(\overline{x}) = \lim^y_a f'(y) / g'(y)}. The reason for this is as follows: we know that for all \f{\epsilon' > 0} we can find a \f{\delta' > 0} such that for all \f{y} with \f{0 < \abs{y - a} < \delta'} we have \f{\abs{f'(y) / g'(y) - \lim^y_a f'(y)/g'(y)} < \epsilon'}. We also know that \f{f(x)/g(x) = f'(\overline{x}) / g'(\overline{x})} in our interval and, therefore, \f{\lim_a f(x)/g(x) = \lim_a f'(\overline{x})/g'(\overline{x})}. Let \f{\epsilon > 0}. We have to proove that \f{\lim_a f'(\overline{x})/g'(\overline{x}) = \lim^y_a f'(y) / g'(y)}. Let us choose \f{\delta" = \min(\delta,\delta',\overline{\delta})}, where \f{\delta} is the limit of the inteval from the start of the proof, \f{\delta'} is such that for all \f{y} with \f{0 < \abs{y - a} < \delta'} we have \f{\abs{f'(y) / g'(y) - \lim^y_a f'(y)/g'(y)} < \epsilon} and \f{\overline{\delta}} is such that for all \f{x} with \f{0 < \abs{x - a} < \overline{delta}} we have \f{\abs{\overline{x} - a} < \delta'} (since \f{\lim_a \overline{x} = a} and \f{\overline{x} = \overline{x}(x)}, more formally). Thus \f{\delta"} has all the properties of \f{\delta}, \f{\delta'} and \f{\overline{\delta}}. More importantly, an \f{x} with \f{0 < \abs{x - a} < \delta"} has an associated \f{\overline{x}} which behaves like the \f{y} for our target limit, therefore \f{\abs {f'(\overline{x})/ g'(\overline{x}) - \lim^y_a f'(y)/g'(y)} < \epsilon}, holds, and we have what we wanted.

== Convexity ==

Let \f{f} be a convex function, \f{a \text{ and } b} numbers with \f{a < b} and \f{f} differentiable at \f{a}.}
* \f{f} lies above the tangent line through \f{(a,f(a))}, except at \f{(a,f(a))} itself.
* If \f{f} differentiable at \f{b} then \f{f'(a) < f'(b)}.

Let \f{f} is a function, \f{A \subseteq \dom{f}} an interval and \f{f} is differentiable on \f{A}.
* If \f{a, b \in A} are numbers with \f{a < b} and \f{f(a) = f(b)} and \f{f'}  is increasing then \f{f(x) < f(a) = f(b)} for all \f{x \in (a,b)}.
* \f{f'} is increasing on \f{A} if and only if \f{f} is convex on \f{A}.
* If the graph of \f{f} lies above each tangent line except at the point of contact, then \f{f} is convex on \f{A}.
* If \f{f} intersects each of its tangents on \f{A} just once, then \f{f} is either convex or concave on \f{A}.

Let \f{f} is a convex function, \f{A \subseteq \dom{f}} an interval and \f{f} is differentiable on \f{A}.
* \f{f} is either increasing or decreasing or there exists \f{c \in A} such that \f{f} is decreasing on \f{A \cap (-\infty,c)} and increasing on \f{A \cap (c,+\infty)}.
* If \f{f} is increasing then \f{f \circ g} is convex - If \f{f(x) = x^2} and \f{g(x) = x^2 - 1}, \f{f \circ g} is no longer weakly convex, so we need the assumption that \f{f} is increasing.

These are alternative versions of basic facts about convex functions, using differentiability for simpler proofs.

Legt \f{f} be a twice-differentiable function with for all \f{x \geq 0} we have \f{f(x) > 0}, \f{f} increasing and \f{f'(0) = 0}.
* There exists \f{x > 0} such that \f{f"(x) = 0}.}

For a "reasonable" function \f{f} we always find an \ref{inflection point}.

For part \f{1}, we have that \f{f'(a)} is the infimum for \f{(f(h_x) - f(a)) / (h_x - a)} where \f{h_x} is sufficiently close to, but larger than, \f{a} (we're using the alternative definition of the derivative). Now, since \f{f} is convex we have that any other point \f{b > h_x > a} will have \f{f'(a) < (f(b) - f(a)) / (b - a)}. But, with a little bit of algebraic manipulation, we have \f{f'(a) (b - a) + f(a) < f(b)}. But \f{f'(a) (b - a) + f(a)} is the value of the tangent through \f{(a,f(a))} at point \f{b}. Therefore, we've proven what we wanted for \f{b > a}. The proof for \f{b < a} is similar.
For part \f{2}, if \f{f} is differentiable at \f{b}, we have that \f{f'(b)} is the supremum for \f{(f(h_y) - f(b)) / (h_y - b)} where \f{h_y} is sufficiently close to, but smaller than, \f{b}. We then have \f{f'(a) < (f(h_x) - f(a)) / (h_x - a) < (f(h_y) - f(b)) / (h_y - b) < f'(b)}, from where \f{f'(a) < f'(b)}, which is what we wanted.
For part \f{3}, proof \f{1}: suppose, by contradiction, that there is an \f{\overline{x} \in (a,b)} such that \f{f(\overline{x}) \geq f(a) = f(b)}. By \thmref{DifferentiabilityImpliesContinuity} we have that \f{f} is continuous on \f{[a,b]} and by \thmref{MaximumMinimumTheorem} we have that \f{f} takes on a maximum on \f{[a,b]}. This maximum will be actually \f{\overline{\overline{x}} \in (a,b)} because we know \f{\overline{x} \geq f(a) = f(b)}, therefore \f{a} and \f{b} cannot be maxima. By \thmref{MeanValueTheorems} we have that there is an \f{x' \in (a,b)} such that \f{f'(x') = 0}. Since \f{f'} is increasing, for all \f{x \in (a,x')} we have \f{f'(x) < 0} and for all \f{x \in (x',b)} we have \f{f'(x) > 0}. If \f{\overline{\overline{x}} \in (a,x')} we have that \f{f'(\overline{\overline{x}}) < 0}. Therefore, by \thmref{LocalBehaviourLimits} there is a \f{\delta > 0} such that for all \f{x \in (\overline{\overline{x}}-\delta,\overline{\overline{x}}+\delta)} we have \f{(f(x) - f(\overline{\overline{x}})) / (x - \overline{\overline{x}}) < 0}. But, since \f{f(x) - f(\overline{\overline{x}})} is always negative, since \f{\overline{\overline{x}}} is the maximum, and \f{x - \overline{\overline{x}}} is positive or negative depending on \f{x}, we have a contradiction, as the fraction cannot be always negative. Therefore, all \f{x \in A} must be \f{f(x) < f(a) = f(b)}. Proof \f{2}: suppose, by contradiction, that there is \f{\overline{x} \in (a,b)} with \f{f(\overline{x}) > f(a) = f(b)}. Then, the maximum of \f{f} on \f{[a,b]} occurs at some point \f{\overline{\overline{x}} \in (a,b)} with \f{f(\overline{\overline{x}}) > f(a)} and \f{f(\overline{\overline{x}}) = 0}, by \thmref{DerivativesAtMaximaMinima}. By \thmref{MeanValueTheorems} we can find \f{x' \in [a,\overline{\overline{x}}]} such that \f{f'(x') = (f(\overline{\overline{x}}) - f(a)) / (\overline{\overline{x}} - a) > 0}, which contradicts the fact that \f{f'} is increasing. Suppose, by contradiction, that \f{f(\overline{x}) = f(a)} for some \f{\overline{x} \in (a,b)}. We know that \f{f} is not constant on \f{[a,\overline{x}]}, since \f{f'} would be \f{0}. There is some \f{\overline{\overline{x}} \in (a,\overline{x})} with \f{f(\overline{\overline{x}}) < f(a)}. By \thmref{MeanValueTheorems} we have that there is an \f{x' \in (\overline{\overline{x}},\overline{x})} such that \f{f'(x') = (f(\overline{x}) - f(\overline{\overline{x}})) / (\overline{x} - \overline{\overline{x}}) > 0}. On the other hand, since \f{\overline{x}} is a local maximum, we have by, \thmref{DerivativesAtMaximaMinima} that \f{f'(\overline{x}) = 0}. We have reached a contradiction, since \f{f'} was supposed to be increasing. We have proven what we wanted, therefore.
For part \f{4}, we first prove ``\f{\Rightarrow}''. Define \f{h(x) = f(x) - (f(b) - f(a))/(b-a) (x - a)}. \f{h'} is also increasing and \f{h(a) = h(b) = f(a)}. Applying part \f{3}, we have that \f{h(x) < f(a)} for all \f{x \in (a,b)}, therefore \f{f(x) - (f(b) - f(a)) / (b - a)(x - a) < f(a)} or \f{(f(x) - f(a))/(x - a) < (f(b) - f(a)) / (b - a)}, which means \f{f} is convex. We now have to prove ``\f{\Leftarrow}''. But, part \f{2} already took care of this.
For part \f{5}, let \f{a,b,x \in A} be such that \f{a < x < b}. The tangent through \f{a} is \f{t_a(x) = f'(a) (x - a) + f(a)} while the tangent through \f{b} is \f{t_b(x) = f'(b)(x - b) + f(b)}. Now, \f{t_a(a) = f(a)}, \f{t_a(b) < f(b)}, \f{t_b(b) = f(b)} and \f{t_b(a) < f(a)}. If we substract the first from the second two we obtain \f{t_a(b) - t_a(a) < f(b) - f(a)} or \f{f'(a) < (f(b) - f(a)) / (b-a)}. Similarly, if we substract the fourth from the third, we obtain \f{t_b(b) - t_b(a) > f(b) - f(a)} or \f{f'(b) > (f(b) - f(a)) / (b-a)}. We thus have \f{f'(a) < f'(b)}, which is what we wanted, and which means \f{f'} is increasing. Therefore, by part \f{4}, \f{f} is convex.
For part \f{7}, we have that \f{f'} is increasing, by part \f{4}. If \f{f'(x) > 0} for all \f{x \in A} then \f{f} will be increasing on \f{A} by \thmref{UnderstandingFunctionsFromDerivatives}. If \f{f'(x) < 0} for all \f{x \in A} then \f{f} will be decreasing on \f{A} by \thmref{UnderstandingFunctionsFromDerivatives}. If \f{f'(x)} is neither positive or negative, then there is a single point \f{c \in A} such that \f{f'(c) = 0}, because \f{f'} is increasing. For all points in \f{x \in A \cap (-\infty,c)} we have that \f{f'(x) < 0} and \f{f} is decreasing. For all points \f{x \in A \cap (c,+\infty)} we have that \f{f'(x) > 0} and \f{f} is increasing. Therefore, we have the three cases we want, and these are the only possible cases.
For part \f{8}, if we only work with \f{f} differentiable, we can provide a proof based on case analysis that \f{(f \circ g)'} is increasing, and \f{(f \circ g)} is convex by part \f{4}. We know \f{f} is increasing and \f{f'} increasing and \f{g'} increasing, and we take it from there. If we assume that \f{f} is twice differentiable, we can provide another proof, by showing that \f{(f \circ g)" > 0} for all \f{x \in A}, which means that \f{(f \circ g)'} is increasing and \f{(f \circ g)} is convex by part \f{4}.
For part \f{9}, choose \f{x_0} with \f{f'(x_0) < 0}. Since \f{f} is decreasing on \f{[0,+\infty)} we have that \f{f'} is nonincreasing on \f{[0,+\infty)} by \thmref{UnderstandingFunctionsFromDerivatives}. Next, we find \f{x_1} with \f{f'(x_1) > f'(x_0)} (such a point must exist, because otherwise we'd have that \f{f'} is unbounded, and it [intuitively] is a contradiction). Therefore, on \f{[0,x_1]} we have that \f{f'} is continuous and \f{f'(0) > f'(x_0)} and \f{f'(x_1) > f'(x_0)} therefore the minimum is somewhere in \f{(0,x_1)}. Let \f{\overline{x}} be this minimum. By \thmref{DerivativesAtMaximaMinima} we have that \f{f"(\overline{x}) = 0}, and this is the point we were looking for.

Let \f{f} be a weakly convex function, \f{a \text{ and } b} numbers with \f{a < b} and \f{f} differentiable at \f{a}.
* \f{f} lies above the tangent line through \f{(a,f(a))} or is equal to the tangent line through \f{(a,f(a))}.
* If \f{f} differentiable at \f{b} then \f{f'(a) \leq f'(b)}.

Let \f{f} is a function, \f{A \subseteq \dom{f}} an interval and \f{f} is differentiable on \f{A}.
* If \f{a, b \in A} are numbers with \f{a < b} and \f{f(a) = f(b)} and \f{f'}  is increasing then \f{f(x) \leq f(a) = f(b)} for all \f{x \in (a,b)}.
* \f{f'} is nondecreasing on \f{A} if and only if \f{f} is weakly convex on \f{A}.
* If the graph of \f{f} lies above each tangent line or is equal to it, then \f{f} is weakly convex on \f{A}.

Let \f{f} is a weakly convex function, \f{A \subseteq \dom{f}} an interval and \f{f} is differentiable on \f{A}.
* \f{f} is either nondecreasing or nonincreasing or there exists \f{c \in A} such that \f{f} is nonincreasing on \f{A \cap (-\infty,c)} and nondecreasing on \f{A \cap (c,+\infty)}.
* If \f{f} is nondecreasing then \f{f \circ g} is weakly convex - If \f{f(x) = \abs{x}} and \f{g(x) = x^2 - 1}, \f{f \circ g} is no longer convex, so we need the assumption that \f{f} is increasing.

These are alternative versions of basic facts about weakly convex functions, using differentiability for simpler proofs.

Let \f{f} be a twice-differentiable function with for all \f{x \geq 0} we have \f{f(x) > 0}, \f{f} increasing and \f{f'(0) = 0}.
* There exists \f{x > 0} such that \f{f"(x) = 0}.

For a "reasonable" function \f{f} we always find an \ref{inflection point}.

\f{x} is an \ref{inflection point} of \f{f} if the tangent line at \f{(x,f(x))} lies on one side of the graph to the left of \f{x} and on the other to the right. Intuitively, inflection points are points where a function changes from convex to concave, or vice-versa. Inflection points have \f{f"(x) = 0} and \f{\sigma(f"(y < x)) \neq \sigma(f"(y > x))}, but this is not necessary.

Some nice properties of convex functions:
* \f{f_+'} and \f{f_-'} exist.
* \f{f_+'} and \f{f_-'} are increasing.
* For all \f{x \in \dom{f}} we have \f{f_-'(x) \leq f_+'(x)}.
* If \f{\dom{f} = \mathbb{R}} or \f{\dom{f}} is an open interval then \f{f} is continous.
* If \f{\dom{f}} is a closed interval with \f{f} left-continous at \f{\min\dom{f}} and right-continuous at \f{\max\dom{f}} then \f{f} is continous.

For the last two points, we could have problems at interval end points - points where convexity is mantained but not continuity. If these are all-right, then \f{f} is convex even for closed intervals.


What we get out of weak convexity:
* \f{f_+'} and \f{f_-'} exist.
* \f{f_+'} and \f{f_-'} are increasing.

For part \f{1}, let \f{x \in \dom{f}} and consider \f{f_+'(x) = \lim^h_{x^+} (f(h) - f(x)) / (h - x)}. By \thmref{DifferentiabilityConvexity} we have that we can find a region around \f{x} in which \f{f} is either increasing or decreasing. Assume \f{f} is increasing. Then the set \f{X^+ = \{(f(h) - f(x)) / (h - x) \given h > x\}} is lower bounded (by any \f{h < x}) and it has an infimum. One can then prove that this infimum is the right-derivative. If we assume \f{f} is decreasing, a similar proof follows. Therefore, the two exist.
For part \f{2}, we can derive a contradiction if we assume the infimum of the set associated with one point is lower than that of a previous point.
For part \f{3}, the proof is similar to that for part \f{2}.
For part \f{4}, we want to prove that \f{\lim_a f(x) = f(a)}. It is enough to prove \f{\lim_{a^+} f(x) = f(a)}. Let \f{\epsilon > 0}. We must pick \f{\delta > 0} such that for all \f{x} with \f{0 < x - a < \delta} we have \f{\abs{f(x) - f(a)} < \epsilon}. We can divide both by \f{\abs{x - a}} and get \f{\abs{(f(x) - f(a)) / (x - a)} < \epsilon/\abs{x - a}}. But \f{\abs{x - a} < A - B} where \f{A = \max\dom{f}} and \f{B = \min\dom{f}}. Therefore \f{\epsilon/\abs{a - b} > \epsilon/(A - B)}. [don't know where this was headed in the notes.]
For part \f{5}, if \f{f} is all-right at interval end points, then, with a proof similar to part \f{4}, we can prove our result.

== The Graph Drawing Algorithm ==

The graph drawing algorithm: Consider a function \f{f \colon \mathbb{R} \rightarrow \mathbb{R}} which has a continuous derivative \f{f'}. To sketch a graph of this function we first ask wether it is even or odd, or if it is a domain shifted version of an even or odd function, in order that we may avoid doing half the work. On the interval that's left, we must first find the critical points of \f{f} and the value of \f{f} at these points, as well as the signs of \f{f'} on the regions between the critical points. We must also optionally find the points \f{x} where \f{f(x) = 0}. Finally, we must take care of the behaviour of \f{f(x)} as \f{x} becomes large or large negative (again, if possible). Using this we might plot the minima and maxima of \f{f}, the behaviour to the left and right of critical points as well as any \f{0} crossing. The zeros of \f{f"} cand be computed and the sign of \f{f"} is determined on the intervals between consecutive zeros. Where \f{f" > 0} the function is convex, where \f{f" < 0} the function is concave.

== Bijectivity And Differentiability ==

Let \f{f \colon A \rightarrow \mathbb{R}} be a continuous and bijective function on the interval \f{A}, \f{a \in \ran{f}} a number and \f{f} differentiable at \f{f^{-1}(a)}.
* If \f{f'(f^{-1}(a)) = 0} then \f{f^{-1}} is not differentiable at \f{a}.
* If \f{f'(f^{-1}(a)) \neq 0} then \f{f^{-1}} is differentiable at \f{a} and \f{(f^{-1})'(a) = 1 / f'(f^{-1}(a))}.
* If \f{f^{(k)}(f^{-1}(a))} exists and is nonzero then \f{(f^{-1})^{(k)}(a)} exists.

For part \f{1}, proof \f{1}: we have that \f{\lim^h_{f^{-1}(a)} (f(h) - f(f^{-1}(a))) / (h - f^{-1}(a)) = \lim^{f^{-1}(\overline{h})}_{f^{-1}(a)} = (\overline{h} - a) / (f^{-1}(\overline{h}) - f^{-1}(a)) = 0}, making use of the alternative definition of a derivative and the fact that \f{f} is bijective, therefore, to every \f{h \in A} a unique \f{\overline{h} \in \ran{f}} corresponds with \f{f(h) = \overline{h}}. Now, for every \f{\epsilon > 0}, we can find a \f{\delta > 0} such that for all \f{f^{-1}(\overline{h})} with \f{0 < \absX{f^{-1}(\overline{h}) - a} < \delta} means that \f{0 < \absX{(\overline{h} - a) / (f^{-1}(\overline{h}) - f^{-1}(a))} < \epsilon} or \f{\absX{(f^{-1}(\overline{h}) - f^{-1}(a)) / (\overline{h} - a)} > 1/\epsilon}. Consider now \f{(f^{-1})'(a) = \lim^{\overline{h}}_a (f^{-1}(\overline{h}) - f^{-1}(a)) / (\overline{h} - a)}. We just proved that whatever \f{\overline{\epsilon}} we found, we can find a \f{\delta > 0} such that, if we use \f{\epsilon = 1/\overline{\epsilon}} we can make the derivative expression larger than \f{\overline{\epsilon}}, therefore, the limit is \f{+\infty}, which means \f{f'(a)} does not exist. Proof \f{2}: if \f{f^{-1}} were differentiable at \f{a}, then we would have, by \thmref{NumericFunctionTransformationsDifferentiability} that \f{(f \circ f^{-1})(a) = f'(f^{-1}(a))(f^{-1})'(a) = 1} hence \f{0(f^{-1})'(a) = 1}, which is a contradiction to theorems from BasicObjects.
For part \f{2}, proof \f{1}: assume \f{f'(f^{-1}(a)) > 0}. Let \f{\epsilon > 0}. We have to choose a \f{\delta > 0} such that for all \f{h} with \f{0 < \abs{h - a} < \delta} we have \f{\abs{(f^{-1}(h) - f^{-1}(a)) / (h - a) - f'(f^{-1}(a))} < \epsilon} or \f{1/f'(f^{-1}(a)) - \epsilon < (f^{-1}(h) - f^{-1}(a)) / (h - a) < 1/f'(f^{-1}(a)) + \epsilon} or \f{1 - \epsilon f'(f^{-1}(a)) < f'(f^{-1})(a) (f^{-1}(h) - f^{-1}(a)) / (h - a) < 1 + \epsilon f'(f^{-1}(a))} (because \f{f'(f^{-1}(a)) > 0} or \f{f'(f^{-1}(a)) / (1 + \epsilon f'(f^{-1}(a))) < (h - a) / (f^{-1}(h) - f^{-1}(a)) < f'(f^{-1}(a)) / (1 - \epsilon f'(f^{-1}(a)))} or \f{f'(f^{-1}(a)) - \epsilon (f'(f^{-1}(a)))^2 / (1  + \epsilon f'(f^{-1}(a))) < (h - a) / (f^{-1}(h) - f^{-1}(a)) < f'(f^{-1}(a)) + \epsilon (f'(f^{-1}(a)))^2 / (1 - \epsilon f'(f^{-1}(a)))} or \f{f'(f^{-1}(a)) - \epsilon_1 < (h - a) / (f^{-1}(h) - f^{-1}(a)) < f'(f^{-1}(a)) + \epsilon_2}. Let \f{\overline{\epsilon} = \min(\epsilon_1,\epsilon_2)}. The existance of \f{f'(f^{-1}(a))} means that we can select a \f{\overline{\delta} > 0} such that for all \f{f^{-1}(h)} with \f{0 < \abs{f^{-1}(h) - f^{-1}(a)} < \overline{\delta}} we have \f{\abs{(h - a) / (f^{-1}(h) - f^{-1}(a))} < \overline{\epsilon}}. We can do the last step, because \f{f} is bijective and we can select for each point in \f{\ran{f}} a unique argument to \f{f} which yields it, and we can work in the domain in inverses of \f{f}. We choose \f{\delta = \overline{\delta}} for our \f{\epsilon}, and obtain what we want. If \f{f'(f^{-1}(a)) < 0} the proof is very similar, and since there are two multiplications by \f{f'(f^{-1}(a))} in our argumentation, the ineuality directions work out. Proof \f{2}: let \f{f(b) = a}. We have \f{(f^{-1})'(a) = \lim^h_0 (f^{-1}(a + h) - f^{-1}(a)) / h = \lim^h_0 (f^{-1}(a + h) - b) / h}. Now, \f{f(b + k(h)) = a + h}, where \f{k(h)} is unique, because \f{f} is bijective, and each point is mapped to a different point. We can then write \f{\lim^h_0 (f^{-1}(f(b + k(h))) - b) / (f(b + k(h)) - a) = \lim^h_0 k(h) / (f(b + k(h)) - f(b))}. Now, we have that \f{f(b + k(h)) = a + h} or \f{b + k(h) = f^{-1}(a + h)} (since \f{f} is bijective) and \f{k(h) = f^{-1}(a + h) - f^{-1}(a)}. Since \f{f} is continuous on \f{A}, this means \f{f^{-1}} is continuous on \f{\ran{f}}, therefore \f{\lim^h_0 k(h) = 0}. Now, since \f{k(h) \rightarrow 0}, this means the limit \f{\lim^{k(h)}_0 (f(b + k(h)) - f(b)) / k(h) = f'(b) = f'(f^{-1}(a)) \neq 0}. By \thmref{NumericFunctionTransformationsLimits} we have that, since \f{(f^{-1})'(a) = \lim^{k(h)}_0 1 / ((f(b + k(h)) - f(b)) / k(h)) = 1 / \lim^{k(h)}_0 (f(b + k(h)) - f(b)) / k(h) = 1 / f'(f^{-1}(a))}, which is what we wanted.

